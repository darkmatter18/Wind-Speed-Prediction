{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>air_temperature_mean</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19950101</td>\n",
       "      <td>2.7</td>\n",
       "      <td>990.6667</td>\n",
       "      <td>263</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>19950102</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1000.7083</td>\n",
       "      <td>301</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19950103</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>1027.3750</td>\n",
       "      <td>355</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19950104</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>1035.9584</td>\n",
       "      <td>86</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>19950105</td>\n",
       "      <td>-5.1</td>\n",
       "      <td>1034.5416</td>\n",
       "      <td>124</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  air_temperature_mean   pressure  wind_direction  wind_speed\n",
       "0  19950101                   2.7   990.6667             263         6.7\n",
       "1  19950102                   0.6  1000.7083             301         4.1\n",
       "2  19950103                  -0.3  1027.3750             355         3.2\n",
       "3  19950104                  -2.8  1035.9584              86         1.6\n",
       "4  19950105                  -5.1  1034.5416             124         3.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./dataset-daily.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(dataset, test_size = 0.1)\n",
    "trainset, valset = train_test_split(trainset, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                    2.004012e+07\n",
       "air_temperature_mean   -6.400000e+00\n",
       "pressure                1.023375e+03\n",
       "wind_direction          1.410000e+02\n",
       "wind_speed              2.800000e+00\n",
       "Name: 3309, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindSpeedDataset(Dataset):\n",
    "    def __init__(self, dataframe, suffle=False, transform=None):\n",
    "        self.dataframe = dataframe.copy()\n",
    "        dataframe1 = dataframe.copy()\n",
    "        self.suffle = suffle\n",
    "        self.transform = transform\n",
    "        \n",
    "        if 'time' in dataframe1:\n",
    "            dataframe1.pop('time')\n",
    "        if 'wind_speed' in dataframe1:\n",
    "            self.labelset = dataframe1.pop('wind_speed')\n",
    "            \n",
    "        self.featureset = dataframe1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.featureset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # print(idx)\n",
    "        \n",
    "        label = np.array([self.labelset.iloc[idx]])\n",
    "        features = self.featureset.iloc[idx].to_numpy()\n",
    "        \n",
    "        sample =(features, label)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample\n",
    "        return (torch.from_numpy(data),torch.from_numpy(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeTransform(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "\n",
    "    Example:\n",
    "        >>> ComposeTransform([\n",
    "        >>>     ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for t in self.transforms:\n",
    "            img = t(data)\n",
    "        return img\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindSpeedDataset(trainset,transform=ComposeTransform([ToTensor()]))\n",
    "test_dataset = WindSpeedDataset(testset, transform=ComposeTransform([ToTensor()]))\n",
    "val_dataset = WindSpeedDataset(valset, transform=ComposeTransform([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  -6.4000, 1023.3750,  141.0000], dtype=torch.float64),\n",
       " tensor([2.8000], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "trainloader = dataloader.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valloader = dataloader.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "testloader = dataloader.DataLoader(test_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7500e+01,  1.0093e+03,  2.7200e+02],\n",
       "        [ 1.4300e+01,  1.0203e+03,  3.0700e+02],\n",
       "        [ 1.6500e+01,  1.0132e+03,  2.5900e+02],\n",
       "        [ 2.7000e+00,  9.9842e+02,  1.4400e+02],\n",
       "        [-6.0000e-01,  1.0070e+03,  2.8800e+02]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 30)\n",
    "        self.fc2 = nn.Linear(30, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = SGD(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 out of 592Training Loss: 39.43460990287162 Test Loss: 1.7769277552002376e+18\n",
      "Epoch: 0 Batch: 20 out of 592Training Loss: 2914969709586819.5 Test Loss: 318577857511920.5\n",
      "Epoch: 0 Batch: 40 out of 592Training Loss: 12456507950623.791 Test Loss: 141990303319505.47\n",
      "Epoch: 0 Batch: 60 out of 592Training Loss: 3378312826246.108 Test Loss: 63285166540737.94\n",
      "Epoch: 0 Batch: 80 out of 592Training Loss: 1502044077061.8481 Test Loss: 28206211165339.152\n",
      "Epoch: 0 Batch: 100 out of 592Training Loss: 669455247053.8478 Test Loss: 12571523892689.455\n",
      "Epoch: 0 Batch: 120 out of 592Training Loss: 298376540757.96936 Test Loss: 5603132678888.728\n",
      "Epoch: 0 Batch: 140 out of 592Training Loss: 132986518743.49657 Test Loss: 2497318366859.636\n",
      "Epoch: 0 Batch: 160 out of 592Training Loss: 59272144092.03969 Test Loss: 1113055525919.0303\n",
      "Epoch: 0 Batch: 180 out of 592Training Loss: 26417611432.15547 Test Loss: 496089242903.2727\n",
      "Epoch: 0 Batch: 200 out of 592Training Loss: 11774335392.284046 Test Loss: 221107112618.66666\n",
      "Epoch: 0 Batch: 220 out of 592Training Loss: 5247823910.7572365 Test Loss: 98547532024.24243\n",
      "Epoch: 0 Batch: 240 out of 592Training Loss: 2338956274.2276306 Test Loss: 43922721947.15151\n",
      "Epoch: 0 Batch: 260 out of 592Training Loss: 1042474182.8956548 Test Loss: 19576391090.424244\n",
      "Epoch: 0 Batch: 280 out of 592Training Loss: 464632033.2008372 Test Loss: 8725210701.575758\n",
      "Epoch: 0 Batch: 300 out of 592Training Loss: 207087323.62365007 Test Loss: 3888826647.2727275\n",
      "Epoch: 0 Batch: 320 out of 592Training Loss: 92298801.6682832 Test Loss: 1733248494.5454545\n",
      "Epoch: 0 Batch: 340 out of 592Training Loss: 41138058.4622775 Test Loss: 772504072.7272727\n",
      "Epoch: 0 Batch: 360 out of 592Training Loss: 18334702.828483574 Test Loss: 344306816.0\n",
      "Epoch: 0 Batch: 380 out of 592Training Loss: 8171731.2142373035 Test Loss: 153459122.42424244\n",
      "Epoch: 0 Batch: 400 out of 592Training Loss: 3642263.100024049 Test Loss: 68396559.51515152\n",
      "Epoch: 0 Batch: 420 out of 592Training Loss: 1623454.0255067972 Test Loss: 30483297.060606062\n",
      "Epoch: 0 Batch: 440 out of 592Training Loss: 723477.1824755182 Test Loss: 13586691.181818182\n",
      "Epoch: 0 Batch: 460 out of 592Training Loss: 322460.61095012754 Test Loss: 6055690.015151516\n",
      "Epoch: 0 Batch: 480 out of 592Training Loss: 143723.13785633468 Test Loss: 2699065.018939394\n",
      "Epoch: 0 Batch: 500 out of 592Training Loss: 64062.38431225732 Test Loss: 1202933.7613636365\n",
      "Epoch: 0 Batch: 520 out of 592Training Loss: 28559.27503684503 Test Loss: 536069.2774621212\n",
      "Epoch: 0 Batch: 540 out of 592Training Loss: 12718.34922936967 Test Loss: 238969.78977272726\n",
      "Epoch: 0 Batch: 560 out of 592Training Loss: 5671.455628449105 Test Loss: 106505.90743371213\n",
      "Epoch: 0 Batch: 580 out of 592Training Loss: 2526.585565620269 Test Loss: 47480.61292613636\n",
      "Epoch: 1 Batch: 0 out of 592Training Loss: 50.83848448057432 Test Loss: 29271.193507339016\n",
      "Epoch: 1 Batch: 20 out of 592Training Loss: 690.9861658178514 Test Loss: 13061.282640861742\n",
      "Epoch: 1 Batch: 40 out of 592Training Loss: 309.82651966618306 Test Loss: 5825.139278527462\n",
      "Epoch: 1 Batch: 60 out of 592Training Loss: 138.73142486760017 Test Loss: 2593.0568588719225\n",
      "Epoch: 1 Batch: 80 out of 592Training Loss: 61.959610981815516 Test Loss: 1153.5637133049242\n",
      "Epoch: 1 Batch: 100 out of 592Training Loss: 27.42023996573021 Test Loss: 515.5091164328835\n",
      "Epoch: 1 Batch: 120 out of 592Training Loss: 12.110996938303202 Test Loss: 232.1112051299124\n",
      "Epoch: 1 Batch: 140 out of 592Training Loss: 5.4485905826503815 Test Loss: 105.43538041548295\n",
      "Epoch: 1 Batch: 160 out of 592Training Loss: 2.5323506260543147 Test Loss: 48.16421323834044\n",
      "Epoch: 1 Batch: 180 out of 592Training Loss: 1.1554126450984308 Test Loss: 22.72739621364709\n",
      "Epoch: 1 Batch: 200 out of 592Training Loss: 0.5694285211848998 Test Loss: 11.323993733434966\n",
      "Epoch: 1 Batch: 220 out of 592Training Loss: 0.2969616654854046 Test Loss: 6.180997491785974\n",
      "Epoch: 1 Batch: 240 out of 592Training Loss: 0.16823381326544612 Test Loss: 3.8678524616089733\n",
      "Epoch: 1 Batch: 260 out of 592Training Loss: 0.13129556385686048 Test Loss: 2.795829549206026\n",
      "Epoch: 1 Batch: 280 out of 592Training Loss: 0.1084071461246896 Test Loss: 2.307078905177839\n",
      "Epoch: 1 Batch: 300 out of 592Training Loss: 0.08394453167175094 Test Loss: 2.1772011080474565\n",
      "Epoch: 1 Batch: 320 out of 592Training Loss: 0.07170960292420232 Test Loss: 2.1333152374083344\n",
      "Epoch: 1 Batch: 340 out of 592Training Loss: 0.058858187566892275 Test Loss: 2.1335342309691687\n",
      "Epoch: 1 Batch: 360 out of 592Training Loss: 0.06368564059466307 Test Loss: 2.141121080653234\n",
      "Epoch: 1 Batch: 380 out of 592Training Loss: 0.07483106159361985 Test Loss: 2.135430917595372\n",
      "Epoch: 1 Batch: 400 out of 592Training Loss: 0.06765253991732982 Test Loss: 2.1324241281007277\n",
      "Epoch: 1 Batch: 420 out of 592Training Loss: 0.0754104398367314 Test Loss: 2.1262634802948344\n",
      "Epoch: 1 Batch: 440 out of 592Training Loss: 0.06879053040712627 Test Loss: 2.131057400143508\n",
      "Epoch: 1 Batch: 460 out of 592Training Loss: 0.08022203397986404 Test Loss: 2.1282158216292206\n",
      "Epoch: 1 Batch: 480 out of 592Training Loss: 0.07703935932791309 Test Loss: 2.1364941735836593\n",
      "Epoch: 1 Batch: 500 out of 592Training Loss: 0.05584751361245501 Test Loss: 2.148819991697868\n",
      "Epoch: 1 Batch: 520 out of 592Training Loss: 0.060302103975227866 Test Loss: 2.1645819088726332\n",
      "Epoch: 1 Batch: 540 out of 592Training Loss: 0.09110704124010564 Test Loss: 2.138567780906504\n",
      "Epoch: 1 Batch: 560 out of 592Training Loss: 0.06018364122489882 Test Loss: 2.1502457194481837\n",
      "Epoch: 1 Batch: 580 out of 592Training Loss: 0.08533976916208627 Test Loss: 2.1255295032804664\n",
      "Epoch: 2 Batch: 0 out of 592Training Loss: 0.002979767685000961 Test Loss: 2.1286934774481887\n",
      "Epoch: 2 Batch: 20 out of 592Training Loss: 0.09417748589230562 Test Loss: 2.1262755793603985\n",
      "Epoch: 2 Batch: 40 out of 592Training Loss: 0.07000952392948964 Test Loss: 2.135868283609549\n",
      "Epoch: 2 Batch: 60 out of 592Training Loss: 0.05832257191323571 Test Loss: 2.126317421821031\n",
      "Epoch: 2 Batch: 80 out of 592Training Loss: 0.08987108078223063 Test Loss: 2.1257678286144226\n",
      "Epoch: 2 Batch: 100 out of 592Training Loss: 0.06780368241047909 Test Loss: 2.1256961967005874\n",
      "Epoch: 2 Batch: 120 out of 592Training Loss: 0.07585150663165989 Test Loss: 2.1254510068983743\n",
      "Epoch: 2 Batch: 140 out of 592Training Loss: 0.06716377722156659 Test Loss: 2.1289130812793067\n",
      "Epoch: 2 Batch: 160 out of 592Training Loss: 0.07359334247643098 Test Loss: 2.1310776128913416\n",
      "Epoch: 2 Batch: 180 out of 592Training Loss: 0.0502290476040954 Test Loss: 2.146438888302355\n",
      "Epoch: 2 Batch: 200 out of 592Training Loss: 0.0669788961455959 Test Loss: 2.1316314879240412\n",
      "Epoch: 2 Batch: 220 out of 592Training Loss: 0.05502010095643799 Test Loss: 2.1331693995179553\n",
      "Epoch: 2 Batch: 240 out of 592Training Loss: 0.08521079801809224 Test Loss: 2.127157367088578\n",
      "Epoch: 2 Batch: 260 out of 592Training Loss: 0.09921919312467868 Test Loss: 2.1273225274952976\n",
      "Epoch: 2 Batch: 280 out of 592Training Loss: 0.07679818755211042 Test Loss: 2.1295749793450036\n",
      "Epoch: 2 Batch: 300 out of 592Training Loss: 0.06668035633469054 Test Loss: 2.1262916552297995\n",
      "Epoch: 2 Batch: 320 out of 592Training Loss: 0.0853464485363767 Test Loss: 2.1258472484169584\n",
      "Epoch: 2 Batch: 340 out of 592Training Loss: 0.05705013592318986 Test Loss: 2.125653466266213\n",
      "Epoch: 2 Batch: 360 out of 592Training Loss: 0.0715055292475478 Test Loss: 2.1264855784900263\n",
      "Epoch: 2 Batch: 380 out of 592Training Loss: 0.07248329187057513 Test Loss: 2.1269077974738497\n",
      "Epoch: 2 Batch: 400 out of 592Training Loss: 0.06775531865899061 Test Loss: 2.128387681462548\n",
      "Epoch: 2 Batch: 420 out of 592Training Loss: 0.05382159912266952 Test Loss: 2.1292009841312063\n",
      "Epoch: 2 Batch: 440 out of 592Training Loss: 0.07371690764559548 Test Loss: 2.1256537692564907\n",
      "Epoch: 2 Batch: 460 out of 592Training Loss: 0.06348792926508251 Test Loss: 2.1278065634947834\n",
      "Epoch: 2 Batch: 480 out of 592Training Loss: 0.09543234398867159 Test Loss: 2.125902693154234\n",
      "Epoch: 2 Batch: 500 out of 592Training Loss: 0.08786341412531407 Test Loss: 2.133726378056136\n",
      "Epoch: 2 Batch: 520 out of 592Training Loss: 0.05447256469215874 Test Loss: 2.1264236655199165\n",
      "Epoch: 2 Batch: 540 out of 592Training Loss: 0.07626916175999295 Test Loss: 2.1258623597748354\n",
      "Epoch: 2 Batch: 560 out of 592Training Loss: 0.07118942454668581 Test Loss: 2.1284802278334443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 580 out of 592Training Loss: 0.06460335542146171 Test Loss: 2.12873207664851\n",
      "Epoch: 3 Batch: 0 out of 592Training Loss: 0.002331554688311912 Test Loss: 2.134601447279706\n",
      "Epoch: 3 Batch: 20 out of 592Training Loss: 0.08737311230283952 Test Loss: 2.128901807873538\n",
      "Epoch: 3 Batch: 40 out of 592Training Loss: 0.07689909995183117 Test Loss: 2.1270535441510603\n",
      "Epoch: 3 Batch: 60 out of 592Training Loss: 0.07865896768768267 Test Loss: 2.1262907148762182\n",
      "Epoch: 3 Batch: 80 out of 592Training Loss: 0.0681860285324053 Test Loss: 2.125666715656266\n",
      "Epoch: 3 Batch: 100 out of 592Training Loss: 0.06043144806100613 Test Loss: 2.1272471070741164\n",
      "Epoch: 3 Batch: 120 out of 592Training Loss: 0.0886548966931846 Test Loss: 2.129651915620674\n",
      "Epoch: 3 Batch: 140 out of 592Training Loss: 0.05449598034403606 Test Loss: 2.1254517898866623\n",
      "Epoch: 3 Batch: 160 out of 592Training Loss: 0.06601130457276465 Test Loss: 2.1259774924679236\n",
      "Epoch: 3 Batch: 180 out of 592Training Loss: 0.07598963776706634 Test Loss: 2.1258371219490515\n",
      "Epoch: 3 Batch: 200 out of 592Training Loss: 0.10062411533885911 Test Loss: 2.1289738671797696\n",
      "Epoch: 3 Batch: 220 out of 592Training Loss: 0.06865291509050865 Test Loss: 2.1256168191180085\n",
      "Epoch: 3 Batch: 240 out of 592Training Loss: 0.06151692879215116 Test Loss: 2.132655364771684\n",
      "Epoch: 3 Batch: 260 out of 592Training Loss: 0.05925521876783815 Test Loss: 2.1283209445801647\n",
      "Epoch: 3 Batch: 280 out of 592Training Loss: 0.060532827796605314 Test Loss: 2.1363262621064982\n",
      "Epoch: 3 Batch: 300 out of 592Training Loss: 0.06798154889198518 Test Loss: 2.1277903214548575\n",
      "Epoch: 3 Batch: 320 out of 592Training Loss: 0.07129348418094171 Test Loss: 2.126627214704499\n",
      "Epoch: 3 Batch: 340 out of 592Training Loss: 0.07279736777047614 Test Loss: 2.1257216966513433\n",
      "Epoch: 3 Batch: 360 out of 592Training Loss: 0.07807267861625197 Test Loss: 2.125564919276671\n",
      "Epoch: 3 Batch: 380 out of 592Training Loss: 0.0678016686553129 Test Loss: 2.125460358280124\n",
      "Epoch: 3 Batch: 400 out of 592Training Loss: 0.07737674324901025 Test Loss: 2.1265933789087064\n",
      "Epoch: 3 Batch: 420 out of 592Training Loss: 0.07452538660700914 Test Loss: 2.1258657026019963\n",
      "Epoch: 3 Batch: 440 out of 592Training Loss: 0.05428219936572511 Test Loss: 2.1256211691282014\n",
      "Epoch: 3 Batch: 460 out of 592Training Loss: 0.07748332686961037 Test Loss: 2.125711530672781\n",
      "Epoch: 3 Batch: 480 out of 592Training Loss: 0.059405333741219674 Test Loss: 2.1262157940954873\n",
      "Epoch: 3 Batch: 500 out of 592Training Loss: 0.07700451379045124 Test Loss: 2.1271636887933267\n",
      "Epoch: 3 Batch: 520 out of 592Training Loss: 0.06069420880772348 Test Loss: 2.1292763564622765\n",
      "Epoch: 3 Batch: 540 out of 592Training Loss: 0.07778688377737156 Test Loss: 2.1366430067203264\n",
      "Epoch: 3 Batch: 560 out of 592Training Loss: 0.07696746298299245 Test Loss: 2.138567984781482\n",
      "Epoch: 3 Batch: 580 out of 592Training Loss: 0.076457124052563 Test Loss: 2.128623907087427\n",
      "Epoch: 4 Batch: 0 out of 592Training Loss: 0.0006707552317026499 Test Loss: 2.1259357606371245\n",
      "Epoch: 4 Batch: 20 out of 592Training Loss: 0.06066899045953531 Test Loss: 2.136763295328075\n",
      "Epoch: 4 Batch: 40 out of 592Training Loss: 0.06658460227595808 Test Loss: 2.136979245891174\n",
      "Epoch: 4 Batch: 60 out of 592Training Loss: 0.056293156829099585 Test Loss: 2.1403881274163723\n",
      "Epoch: 4 Batch: 80 out of 592Training Loss: 0.07312390691036345 Test Loss: 2.1397436643865975\n",
      "Epoch: 4 Batch: 100 out of 592Training Loss: 0.0588965415532725 Test Loss: 2.15509884860931\n",
      "Epoch: 4 Batch: 120 out of 592Training Loss: 0.06763801582754587 Test Loss: 2.1321857099731765\n",
      "Epoch: 4 Batch: 140 out of 592Training Loss: 0.05517685477308226 Test Loss: 2.1300139492659858\n",
      "Epoch: 4 Batch: 160 out of 592Training Loss: 0.07433591601924402 Test Loss: 2.125668283439044\n",
      "Epoch: 4 Batch: 180 out of 592Training Loss: 0.0765087218119096 Test Loss: 2.1290953904390335\n",
      "Epoch: 4 Batch: 200 out of 592Training Loss: 0.053667853747642755 Test Loss: 2.1261987710992494\n",
      "Epoch: 4 Batch: 220 out of 592Training Loss: 0.05762287588483785 Test Loss: 2.1259967838272904\n",
      "Epoch: 4 Batch: 240 out of 592Training Loss: 0.0551962076898021 Test Loss: 2.1266007791414405\n",
      "Epoch: 4 Batch: 260 out of 592Training Loss: 0.09371673257354876 Test Loss: 2.1295474483208223\n",
      "Epoch: 4 Batch: 280 out of 592Training Loss: 0.0953225089554449 Test Loss: 2.1344966601693267\n",
      "Epoch: 4 Batch: 300 out of 592Training Loss: 0.0838700126476392 Test Loss: 2.128688371316953\n",
      "Epoch: 4 Batch: 320 out of 592Training Loss: 0.05936035411363036 Test Loss: 2.125714447687973\n",
      "Epoch: 4 Batch: 340 out of 592Training Loss: 0.09061474599576488 Test Loss: 2.1254667899373807\n",
      "Epoch: 4 Batch: 360 out of 592Training Loss: 0.09582084016283984 Test Loss: 2.143460705199025\n",
      "Epoch: 4 Batch: 380 out of 592Training Loss: 0.08367201292562258 Test Loss: 2.1351167690573316\n",
      "Epoch: 4 Batch: 400 out of 592Training Loss: 0.09922732835021453 Test Loss: 2.136916067338351\n",
      "Epoch: 4 Batch: 420 out of 592Training Loss: 0.07502821322639593 Test Loss: 2.1282584707845342\n",
      "Epoch: 4 Batch: 440 out of 592Training Loss: 0.08181925495929344 Test Loss: 2.125481482482318\n",
      "Epoch: 4 Batch: 460 out of 592Training Loss: 0.06527247806592276 Test Loss: 2.127285676923665\n",
      "Epoch: 4 Batch: 480 out of 592Training Loss: 0.061344554241380904 Test Loss: 2.126536719952569\n",
      "Epoch: 4 Batch: 500 out of 592Training Loss: 0.062489654059540115 Test Loss: 2.1255073321588114\n",
      "Epoch: 4 Batch: 520 out of 592Training Loss: 0.07715018752297056 Test Loss: 2.1293555705836327\n",
      "Epoch: 4 Batch: 540 out of 592Training Loss: 0.06133924805841267 Test Loss: 2.1292578636696846\n",
      "Epoch: 4 Batch: 560 out of 592Training Loss: 0.07904301390595066 Test Loss: 2.126069641926072\n",
      "Epoch: 4 Batch: 580 out of 592Training Loss: 0.0703951974895346 Test Loss: 2.1266404541604444\n"
     ]
    }
   ],
   "source": [
    "trainlosses = []\n",
    "testlosses = []\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trainloss = 0\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(trainloader):\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        target = target.type(torch.FloatTensor)\n",
    "#         if cuda:\n",
    "#             image, target = image.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        trainloss += loss.item()\n",
    "        \n",
    "        if batch % 20 == 0:\n",
    "            testloss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in valloader:\n",
    "                    data = data.type(torch.FloatTensor)\n",
    "                    target = target.type(torch.FloatTensor)\n",
    "#                     if cuda:\n",
    "#                         image, target = image.cuda(), target.cuda()\n",
    "                    ps = model(data)\n",
    "                    testloss += criterion(ps, target).item()\n",
    "                testloss = testloss / len(valloader)        \n",
    "            trainloss = trainloss / len(trainloader)\n",
    "        \n",
    "            trainlosses.append(trainloss)\n",
    "            testlosses.append(testloss)\n",
    "        \n",
    "            print(f'Epoch: {epoch}',\n",
    "                  f'Batch: {batch} out of {len(trainloader)}',\n",
    "                  f'Training Loss: {trainloss}',\n",
    "                  f'Test Loss: {testloss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16985152948>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(testlosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
