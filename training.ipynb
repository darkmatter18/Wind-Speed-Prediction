{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import Normalize_df, WindSpeedDataset, ComposeTransform, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>air_temperature_mean</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370203</td>\n",
       "      <td>0.103164</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.322799</td>\n",
       "      <td>0.268912</td>\n",
       "      <td>0.838440</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.302483</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.988858</td>\n",
       "      <td>0.260417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.246050</td>\n",
       "      <td>0.850758</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.827372</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  air_temperature_mean  pressure  wind_direction  wind_speed\n",
       "0  0.000000              0.370203  0.103164        0.732591    0.625000\n",
       "1  0.000011              0.322799  0.268912        0.838440    0.354167\n",
       "2  0.000022              0.302483  0.709078        0.988858    0.260417\n",
       "3  0.000033              0.246050  0.850758        0.239554    0.093750\n",
       "4  0.000044              0.194131  0.827372        0.345404    0.291667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Normalize_df(pd.read_csv('./dataset-daily.csv'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(dataset, test_size = 0.1)\n",
    "trainset, valset = train_test_split(trainset, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                    0.559717\n",
       "air_temperature_mean    0.525959\n",
       "pressure                0.198761\n",
       "wind_direction          0.479109\n",
       "wind_speed              0.354167\n",
       "Name: 2138, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindSpeedDataset(trainset,transform=ComposeTransform([ToTensor()]))\n",
    "test_dataset = WindSpeedDataset(testset, transform=ComposeTransform([ToTensor()]))\n",
    "val_dataset = WindSpeedDataset(valset, transform=ComposeTransform([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5260, 0.1988, 0.4791], dtype=torch.float64),\n",
       " tensor([0.3542], dtype=torch.float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "trainloader = dataloader.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valloader = dataloader.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "testloader = dataloader.DataLoader(test_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3183, 0.4030, 0.6407],\n",
       "        [0.4018, 0.5420, 0.3872],\n",
       "        [0.6953, 0.4560, 0.6880],\n",
       "        [0.6862, 0.4684, 0.6964],\n",
       "        [0.1670, 0.6671, 0.3398]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 30)\n",
    "        self.fc2 = nn.Linear(30, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = SGD(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 out of 592 Training Loss: 3.357122438280163e-05 Test Loss: 0.036849915219301525\n",
      "Epoch: 0 Batch: 20 out of 592 Training Loss: 0.0009286321600158176 Test Loss: 0.0241932076610145\n",
      "Epoch: 0 Batch: 40 out of 592 Training Loss: 0.0006795604045671494 Test Loss: 0.023977849147437762\n",
      "Epoch: 0 Batch: 60 out of 592 Training Loss: 0.0008818594143051323 Test Loss: 0.023406696215688222\n",
      "Epoch: 0 Batch: 80 out of 592 Training Loss: 0.0008571129203145197 Test Loss: 0.023168733641575796\n",
      "Epoch: 0 Batch: 100 out of 592 Training Loss: 0.0008598594183606179 Test Loss: 0.02306930905659542\n",
      "Epoch: 0 Batch: 120 out of 592 Training Loss: 0.0007085874739950242 Test Loss: 0.022656964818121527\n",
      "Epoch: 0 Batch: 140 out of 592 Training Loss: 0.0008194323195050002 Test Loss: 0.022549687746284304\n",
      "Epoch: 0 Batch: 160 out of 592 Training Loss: 0.0008527859742649218 Test Loss: 0.022445912703970505\n",
      "Epoch: 0 Batch: 180 out of 592 Training Loss: 0.0008271154986396949 Test Loss: 0.0227327468014802\n",
      "Epoch: 0 Batch: 200 out of 592 Training Loss: 0.0006540911606534485 Test Loss: 0.022094824541427166\n",
      "Epoch: 0 Batch: 220 out of 592 Training Loss: 0.0008314090995431121 Test Loss: 0.022563872915325744\n",
      "Epoch: 0 Batch: 240 out of 592 Training Loss: 0.0007795770396526198 Test Loss: 0.02180899642090398\n",
      "Epoch: 0 Batch: 260 out of 592 Training Loss: 0.0007687777719292178 Test Loss: 0.02150759131076153\n",
      "Epoch: 0 Batch: 280 out of 592 Training Loss: 0.0006897097570631003 Test Loss: 0.0213657500924345\n",
      "Epoch: 0 Batch: 300 out of 592 Training Loss: 0.0006577281667765638 Test Loss: 0.021241282496096876\n",
      "Epoch: 0 Batch: 320 out of 592 Training Loss: 0.0006521084897142905 Test Loss: 0.021117678279000702\n",
      "Epoch: 0 Batch: 340 out of 592 Training Loss: 0.0009793049738591256 Test Loss: 0.02243819885717874\n",
      "Epoch: 0 Batch: 360 out of 592 Training Loss: 0.0008518750644177005 Test Loss: 0.020870923098162606\n",
      "Epoch: 0 Batch: 380 out of 592 Training Loss: 0.0005843257156091968 Test Loss: 0.020690298695681675\n",
      "Epoch: 0 Batch: 400 out of 592 Training Loss: 0.0008429421954531655 Test Loss: 0.021993963555856186\n",
      "Epoch: 0 Batch: 420 out of 592 Training Loss: 0.000575088473128873 Test Loss: 0.020752904633285874\n",
      "Epoch: 0 Batch: 440 out of 592 Training Loss: 0.0007606070073310431 Test Loss: 0.020365222018550743\n",
      "Epoch: 0 Batch: 460 out of 592 Training Loss: 0.0009673357909831603 Test Loss: 0.020683852184888427\n",
      "Epoch: 0 Batch: 480 out of 592 Training Loss: 0.0006694404416148582 Test Loss: 0.02050010345269446\n",
      "Epoch: 0 Batch: 500 out of 592 Training Loss: 0.0006911272938986846 Test Loss: 0.020104204166052637\n",
      "Epoch: 0 Batch: 520 out of 592 Training Loss: 0.0008366248579525397 Test Loss: 0.019896635031645103\n",
      "Epoch: 0 Batch: 540 out of 592 Training Loss: 0.0005480880367685753 Test Loss: 0.01978889037943163\n",
      "Epoch: 0 Batch: 560 out of 592 Training Loss: 0.0006424036779181561 Test Loss: 0.019675439828654016\n",
      "Epoch: 0 Batch: 580 out of 592 Training Loss: 0.000811865184658849 Test Loss: 0.019937424253757028\n",
      "Epoch: 1 Batch: 0 out of 592 Training Loss: 3.1583080370281196e-05 Test Loss: 0.019857004111117654\n",
      "Epoch: 1 Batch: 20 out of 592 Training Loss: 0.0006709257421164363 Test Loss: 0.020147367571057243\n",
      "Epoch: 1 Batch: 40 out of 592 Training Loss: 0.0007650096227913741 Test Loss: 0.01944658321074464\n",
      "Epoch: 1 Batch: 60 out of 592 Training Loss: 0.0008203325667729298 Test Loss: 0.019329837586018115\n",
      "Epoch: 1 Batch: 80 out of 592 Training Loss: 0.0008728568384532927 Test Loss: 0.01956368568635574\n",
      "Epoch: 1 Batch: 100 out of 592 Training Loss: 0.0005254478780518319 Test Loss: 0.019683922609229656\n",
      "Epoch: 1 Batch: 120 out of 592 Training Loss: 0.0007845403213805864 Test Loss: 0.01994815793869114\n",
      "Epoch: 1 Batch: 140 out of 592 Training Loss: 0.0006896428224355044 Test Loss: 0.01918843604102166\n",
      "Epoch: 1 Batch: 160 out of 592 Training Loss: 0.0007273533239024163 Test Loss: 0.019226047680466294\n",
      "Epoch: 1 Batch: 180 out of 592 Training Loss: 0.0008889848693360477 Test Loss: 0.019116177616083543\n",
      "Epoch: 1 Batch: 200 out of 592 Training Loss: 0.000600507699961732 Test Loss: 0.01985840532385434\n",
      "Epoch: 1 Batch: 220 out of 592 Training Loss: 0.0006344231518625479 Test Loss: 0.018982270269506527\n",
      "Epoch: 1 Batch: 240 out of 592 Training Loss: 0.0007679871682623048 Test Loss: 0.01899889298400729\n",
      "Epoch: 1 Batch: 260 out of 592 Training Loss: 0.000742699082448189 Test Loss: 0.019437727430186263\n",
      "Epoch: 1 Batch: 280 out of 592 Training Loss: 0.0005877562288337598 Test Loss: 0.018752756196067134\n",
      "Epoch: 1 Batch: 300 out of 592 Training Loss: 0.0007314080791629941 Test Loss: 0.019053092169942276\n",
      "Epoch: 1 Batch: 320 out of 592 Training Loss: 0.000733432885349075 Test Loss: 0.01881376001983881\n",
      "Epoch: 1 Batch: 340 out of 592 Training Loss: 0.0005369863447929813 Test Loss: 0.01873533929888667\n",
      "Epoch: 1 Batch: 360 out of 592 Training Loss: 0.0006731990397538429 Test Loss: 0.019057190444348664\n",
      "Epoch: 1 Batch: 380 out of 592 Training Loss: 0.0006408010372131552 Test Loss: 0.018585088087782038\n",
      "Epoch: 1 Batch: 400 out of 592 Training Loss: 0.0006704632304145022 Test Loss: 0.0187570236880812\n",
      "Epoch: 1 Batch: 420 out of 592 Training Loss: 0.0006573530313010627 Test Loss: 0.018945656639213364\n",
      "Epoch: 1 Batch: 440 out of 592 Training Loss: 0.0005926482061541507 Test Loss: 0.01869683030985691\n",
      "Epoch: 1 Batch: 460 out of 592 Training Loss: 0.0006461032225238573 Test Loss: 0.019293145509436727\n",
      "Epoch: 1 Batch: 480 out of 592 Training Loss: 0.0005329067605696194 Test Loss: 0.018448947198604317\n",
      "Epoch: 1 Batch: 500 out of 592 Training Loss: 0.0005105011982835664 Test Loss: 0.018421530991679792\n",
      "Epoch: 1 Batch: 520 out of 592 Training Loss: 0.0007071652035729166 Test Loss: 0.01840376052441019\n",
      "Epoch: 1 Batch: 540 out of 592 Training Loss: 0.0006441306818058817 Test Loss: 0.01864612258081748\n",
      "Epoch: 1 Batch: 560 out of 592 Training Loss: 0.0008054397548001043 Test Loss: 0.01835402235155925\n",
      "Epoch: 1 Batch: 580 out of 592 Training Loss: 0.0006101155773966868 Test Loss: 0.01831299679840661\n",
      "Epoch: 2 Batch: 0 out of 592 Training Loss: 1.0368761109389567e-05 Test Loss: 0.018262310662645508\n",
      "Epoch: 2 Batch: 20 out of 592 Training Loss: 0.0006235760439547015 Test Loss: 0.018314968824570038\n",
      "Epoch: 2 Batch: 40 out of 592 Training Loss: 0.0005978662558857053 Test Loss: 0.018517396356346028\n",
      "Epoch: 2 Batch: 60 out of 592 Training Loss: 0.0008307851149412734 Test Loss: 0.018969169967793718\n",
      "Epoch: 2 Batch: 80 out of 592 Training Loss: 0.0005931288846747502 Test Loss: 0.01840220406919605\n",
      "Epoch: 2 Batch: 100 out of 592 Training Loss: 0.0005494765481743234 Test Loss: 0.0182561822038031\n",
      "Epoch: 2 Batch: 120 out of 592 Training Loss: 0.000641778219733642 Test Loss: 0.01821047900893697\n",
      "Epoch: 2 Batch: 140 out of 592 Training Loss: 0.000697465835682312 Test Loss: 0.018144332694425953\n",
      "Epoch: 2 Batch: 160 out of 592 Training Loss: 0.0005726862688388134 Test Loss: 0.01811857718558079\n",
      "Epoch: 2 Batch: 180 out of 592 Training Loss: 0.0006837845390575755 Test Loss: 0.018160576942715456\n",
      "Epoch: 2 Batch: 200 out of 592 Training Loss: 0.0008096416116776931 Test Loss: 0.01834134189905881\n",
      "Epoch: 2 Batch: 220 out of 592 Training Loss: 0.0006313025226714422 Test Loss: 0.018057898988693276\n",
      "Epoch: 2 Batch: 240 out of 592 Training Loss: 0.0005328192959795847 Test Loss: 0.01802705435936029\n",
      "Epoch: 2 Batch: 260 out of 592 Training Loss: 0.0006041318094211975 Test Loss: 0.018093909297463004\n",
      "Epoch: 2 Batch: 280 out of 592 Training Loss: 0.0006325595031942095 Test Loss: 0.01811774078206244\n",
      "Epoch: 2 Batch: 300 out of 592 Training Loss: 0.00048459073735480993 Test Loss: 0.01810052641314652\n",
      "Epoch: 2 Batch: 320 out of 592 Training Loss: 0.0006819987260831244 Test Loss: 0.01905738289475046\n",
      "Epoch: 2 Batch: 340 out of 592 Training Loss: 0.0007661015867521656 Test Loss: 0.01869114343373274\n",
      "Epoch: 2 Batch: 360 out of 592 Training Loss: 0.0007008977720714183 Test Loss: 0.018629149637876475\n",
      "Epoch: 2 Batch: 380 out of 592 Training Loss: 0.0008293379316883928 Test Loss: 0.019750318863908902\n",
      "Epoch: 2 Batch: 400 out of 592 Training Loss: 0.000574469785526136 Test Loss: 0.017881497370333156\n",
      "Epoch: 2 Batch: 420 out of 592 Training Loss: 0.0007104190546832722 Test Loss: 0.017867604614531792\n",
      "Epoch: 2 Batch: 440 out of 592 Training Loss: 0.0004494532727271104 Test Loss: 0.017889927266603052\n",
      "Epoch: 2 Batch: 460 out of 592 Training Loss: 0.0005550476454614636 Test Loss: 0.01812558087950685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 480 out of 592 Training Loss: 0.0009353106711205243 Test Loss: 0.018220635355012775\n",
      "Epoch: 2 Batch: 500 out of 592 Training Loss: 0.0005929857606282452 Test Loss: 0.017820934761513137\n",
      "Epoch: 2 Batch: 520 out of 592 Training Loss: 0.0007770770078566212 Test Loss: 0.017837836382430836\n",
      "Epoch: 2 Batch: 540 out of 592 Training Loss: 0.000644089535838063 Test Loss: 0.018733670199501583\n",
      "Epoch: 2 Batch: 560 out of 592 Training Loss: 0.0007339283960409356 Test Loss: 0.01789578387979418\n",
      "Epoch: 2 Batch: 580 out of 592 Training Loss: 0.0008111443880521478 Test Loss: 0.018289549932391805\n",
      "Epoch: 3 Batch: 0 out of 592 Training Loss: 1.6128153515023155e-05 Test Loss: 0.0177792228518449\n",
      "Epoch: 3 Batch: 20 out of 592 Training Loss: 0.0006620145706833872 Test Loss: 0.017848059173433507\n",
      "Epoch: 3 Batch: 40 out of 592 Training Loss: 0.0007080287736019978 Test Loss: 0.017773602417119862\n",
      "Epoch: 3 Batch: 60 out of 592 Training Loss: 0.0008015244757640362 Test Loss: 0.01979110015302219\n",
      "Epoch: 3 Batch: 80 out of 592 Training Loss: 0.0006400201273839599 Test Loss: 0.017751706674144687\n",
      "Epoch: 3 Batch: 100 out of 592 Training Loss: 0.0006781026197587535 Test Loss: 0.017793673356980875\n",
      "Epoch: 3 Batch: 120 out of 592 Training Loss: 0.0005669053465288393 Test Loss: 0.01773641954324293\n",
      "Epoch: 3 Batch: 140 out of 592 Training Loss: 0.0007124546568104764 Test Loss: 0.01818312593968585\n",
      "Epoch: 3 Batch: 160 out of 592 Training Loss: 0.0005913536507377064 Test Loss: 0.017830144023085297\n",
      "Epoch: 3 Batch: 180 out of 592 Training Loss: 0.0007582044788138704 Test Loss: 0.01774167599488106\n",
      "Epoch: 3 Batch: 200 out of 592 Training Loss: 0.0006721376897799382 Test Loss: 0.01886543944640311\n",
      "Epoch: 3 Batch: 220 out of 592 Training Loss: 0.000667064974312613 Test Loss: 0.01795197633504303\n",
      "Epoch: 3 Batch: 240 out of 592 Training Loss: 0.0007407656479253184 Test Loss: 0.019641728214496237\n",
      "Epoch: 3 Batch: 260 out of 592 Training Loss: 0.0005292123671419086 Test Loss: 0.017793490313289858\n",
      "Epoch: 3 Batch: 280 out of 592 Training Loss: 0.0006432963736594087 Test Loss: 0.017782004403698287\n",
      "Epoch: 3 Batch: 300 out of 592 Training Loss: 0.000584976898612671 Test Loss: 0.017923524633233406\n",
      "Epoch: 3 Batch: 320 out of 592 Training Loss: 0.0006630087067009916 Test Loss: 0.018082201364450157\n",
      "Epoch: 3 Batch: 340 out of 592 Training Loss: 0.0008057201295003399 Test Loss: 0.01778936303735739\n",
      "Epoch: 3 Batch: 360 out of 592 Training Loss: 0.0005802240896036874 Test Loss: 0.01765877232978572\n",
      "Epoch: 3 Batch: 380 out of 592 Training Loss: 0.0007769193339380243 Test Loss: 0.017795322048407274\n",
      "Epoch: 3 Batch: 400 out of 592 Training Loss: 0.0005941482847740903 Test Loss: 0.017755596314655675\n",
      "Epoch: 3 Batch: 420 out of 592 Training Loss: 0.0004811510571319846 Test Loss: 0.01762440165144984\n",
      "Epoch: 3 Batch: 440 out of 592 Training Loss: 0.0005771865237820523 Test Loss: 0.01800593303050846\n",
      "Epoch: 3 Batch: 460 out of 592 Training Loss: 0.0007162226551395393 Test Loss: 0.017604158960184024\n",
      "Epoch: 3 Batch: 480 out of 592 Training Loss: 0.0007663245577480311 Test Loss: 0.017981603180735627\n",
      "Epoch: 3 Batch: 500 out of 592 Training Loss: 0.0004751983065309349 Test Loss: 0.017875702825911118\n",
      "Epoch: 3 Batch: 520 out of 592 Training Loss: 0.0005922625274687153 Test Loss: 0.017616169916987983\n",
      "Epoch: 3 Batch: 540 out of 592 Training Loss: 0.000661618012230974 Test Loss: 0.017657217481455795\n",
      "Epoch: 3 Batch: 560 out of 592 Training Loss: 0.0006557054980992203 Test Loss: 0.01776664288516975\n",
      "Epoch: 3 Batch: 580 out of 592 Training Loss: 0.0006033524871934959 Test Loss: 0.017648853363370465\n",
      "Epoch: 4 Batch: 0 out of 592 Training Loss: 3.709355243355841e-05 Test Loss: 0.01854038819367292\n",
      "Epoch: 4 Batch: 20 out of 592 Training Loss: 0.0005096532619140574 Test Loss: 0.01759035642096547\n",
      "Epoch: 4 Batch: 40 out of 592 Training Loss: 0.0006461434180320579 Test Loss: 0.01795462978241796\n",
      "Epoch: 4 Batch: 60 out of 592 Training Loss: 0.0007445581978052353 Test Loss: 0.017557810852656876\n",
      "Epoch: 4 Batch: 80 out of 592 Training Loss: 0.0006408680016857445 Test Loss: 0.01794407551818894\n",
      "Epoch: 4 Batch: 100 out of 592 Training Loss: 0.0006063484868013848 Test Loss: 0.017846396940788538\n",
      "Epoch: 4 Batch: 120 out of 592 Training Loss: 0.0006175149846164399 Test Loss: 0.017578601422649102\n",
      "Epoch: 4 Batch: 140 out of 592 Training Loss: 0.0005704220005473572 Test Loss: 0.017502988446997762\n",
      "Epoch: 4 Batch: 160 out of 592 Training Loss: 0.0008309686794155237 Test Loss: 0.01775726102906364\n",
      "Epoch: 4 Batch: 180 out of 592 Training Loss: 0.0005258427414110573 Test Loss: 0.017656980731496304\n",
      "Epoch: 4 Batch: 200 out of 592 Training Loss: 0.0005829096612009231 Test Loss: 0.017748046865700886\n",
      "Epoch: 4 Batch: 220 out of 592 Training Loss: 0.0005861179552060582 Test Loss: 0.017516964700806773\n",
      "Epoch: 4 Batch: 240 out of 592 Training Loss: 0.0008195566406157341 Test Loss: 0.0176203374055948\n",
      "Epoch: 4 Batch: 260 out of 592 Training Loss: 0.000706091524338032 Test Loss: 0.018127482017558632\n",
      "Epoch: 4 Batch: 280 out of 592 Training Loss: 0.00046133512444047496 Test Loss: 0.017578186391443578\n",
      "Epoch: 4 Batch: 300 out of 592 Training Loss: 0.0005426284070787321 Test Loss: 0.0180006794204625\n",
      "Epoch: 4 Batch: 320 out of 592 Training Loss: 0.0008393331597854454 Test Loss: 0.018581065784956358\n",
      "Epoch: 4 Batch: 340 out of 592 Training Loss: 0.0006131241453867087 Test Loss: 0.017566960977009414\n",
      "Epoch: 4 Batch: 360 out of 592 Training Loss: 0.0008047166669624951 Test Loss: 0.0177809821678128\n",
      "Epoch: 4 Batch: 380 out of 592 Training Loss: 0.0008277862246893778 Test Loss: 0.01787771881826109\n",
      "Epoch: 4 Batch: 400 out of 592 Training Loss: 0.000629971120366166 Test Loss: 0.01831268632756264\n",
      "Epoch: 4 Batch: 420 out of 592 Training Loss: 0.0004503005020462777 Test Loss: 0.017619132845676646\n",
      "Epoch: 4 Batch: 440 out of 592 Training Loss: 0.0007083448722707097 Test Loss: 0.017525825979343306\n",
      "Epoch: 4 Batch: 460 out of 592 Training Loss: 0.000823668374961854 Test Loss: 0.018332189912061123\n",
      "Epoch: 4 Batch: 480 out of 592 Training Loss: 0.0005561434997083673 Test Loss: 0.01750092397472172\n",
      "Epoch: 4 Batch: 500 out of 592 Training Loss: 0.0005369178103405482 Test Loss: 0.017581162487410686\n",
      "Epoch: 4 Batch: 520 out of 592 Training Loss: 0.0007212511163246018 Test Loss: 0.017451388842539127\n",
      "Epoch: 4 Batch: 540 out of 592 Training Loss: 0.0007900240818321825 Test Loss: 0.01751248511506922\n",
      "Epoch: 4 Batch: 560 out of 592 Training Loss: 0.0006110313601732555 Test Loss: 0.017542003064811455\n",
      "Epoch: 4 Batch: 580 out of 592 Training Loss: 0.000514972436117829 Test Loss: 0.01779651089402085\n"
     ]
    }
   ],
   "source": [
    "trainlosses = []\n",
    "testlosses = []\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trainloss = 0\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(trainloader):\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        target = target.type(torch.FloatTensor)\n",
    "#         if cuda:\n",
    "#             image, target = image.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        trainloss += loss.item()\n",
    "        \n",
    "        if batch % 20 == 0:\n",
    "            testloss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in valloader:\n",
    "                    data = data.type(torch.FloatTensor)\n",
    "                    target = target.type(torch.FloatTensor)\n",
    "#                     if cuda:\n",
    "#                         image, target = image.cuda(), target.cuda()\n",
    "                    ps = model(data)\n",
    "                    testloss += criterion(ps, target).item()\n",
    "                testloss = testloss / len(valloader)        \n",
    "            trainloss = trainloss / len(trainloader)\n",
    "        \n",
    "            trainlosses.append(trainloss)\n",
    "            testlosses.append(testloss)\n",
    "        \n",
    "            print(f'Epoch: {epoch}',\n",
    "                  f'Batch: {batch} out of {len(trainloader)}',\n",
    "                  f'Training Loss: {trainloss}',\n",
    "                  f'Test Loss: {testloss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c6628d4dc8>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.plot(trainlosses)\n",
    "plt.plot(testlosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "expected = []\n",
    "for f, l in testloader:\n",
    "    for i in model(f.type(torch.FloatTensor)).tolist():\n",
    "        result.append(i[0])\n",
    "    for j in l.tolist():\n",
    "        expected.append(j[0])\n",
    "    #result.append(.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c6612d0508>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.plot(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
