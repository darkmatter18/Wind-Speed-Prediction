{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Copyright 2020 Arkadip Bhattacharya\n",
    "\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import Normalize_df, WindSpeedDataset, ComposeTransform, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>air_temperature_mean</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370203</td>\n",
       "      <td>0.103164</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.322799</td>\n",
       "      <td>0.268912</td>\n",
       "      <td>0.838440</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.302483</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.988858</td>\n",
       "      <td>0.260417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.246050</td>\n",
       "      <td>0.850758</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.827372</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  air_temperature_mean  pressure  wind_direction  wind_speed\n",
       "0  0.000000              0.370203  0.103164        0.732591    0.625000\n",
       "1  0.000011              0.322799  0.268912        0.838440    0.354167\n",
       "2  0.000022              0.302483  0.709078        0.988858    0.260417\n",
       "3  0.000033              0.246050  0.850758        0.239554    0.093750\n",
       "4  0.000044              0.194131  0.827372        0.345404    0.291667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Normalize_df(pd.read_csv('./dataset-daily.csv'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(dataset, test_size = 0.1)\n",
    "trainset, valset = train_test_split(trainset, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                    0.119730\n",
       "air_temperature_mean    0.562077\n",
       "pressure                0.550206\n",
       "wind_direction          0.462396\n",
       "wind_speed              0.166667\n",
       "Name: 650, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindSpeedDataset(trainset,transform=ComposeTransform([ToTensor()]))\n",
    "test_dataset = WindSpeedDataset(testset, transform=ComposeTransform([ToTensor()]))\n",
    "val_dataset = WindSpeedDataset(valset, transform=ComposeTransform([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5621, 0.5502, 0.4624], dtype=torch.float64),\n",
       " tensor([0.1667], dtype=torch.float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainloader = dataloader.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valloader = dataloader.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "testloader = dataloader.DataLoader(test_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Count: 1\n",
      "Device: GeForce 940MX\n",
      "Device Capability: (5, 0)\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if cuda else torch.device('cpu')\n",
    "if cuda:\n",
    "    print(\"Device Count:\", torch.cuda.device_count())\n",
    "    print(\"Device:\", torch.cuda.get_device_name())\n",
    "    print(\"Device Capability:\", torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight 10 1 3, but got 2-dimensional input of size [32, 3] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-be5085baf749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 202\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight 10 1 3, but got 2-dimensional input of size [32, 3] instead"
     ]
    }
   ],
   "source": [
    "conv1(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\in2ar\\.conda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm1): LSTM(3, 100, batch_first=True, dropout=0.2)\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "model = Model(3, 100, 1, cuda=cuda)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 0 out of 93 Training Loss: 0.0005129686686941372 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 1 out of 93 Training Loss: 0.0304908919278332 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 2 out of 93 Training Loss: 0.09470503210460626 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 3 out of 93 Training Loss: 0.14049793958103143 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 4 out of 93 Training Loss: 0.19984701826607668 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 5 out of 93 Training Loss: 0.2295725363436886 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 6 out of 93 Training Loss: 0.27246117770111045 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 7 out of 93 Training Loss: 0.29663638576304396 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 8 out of 93 Training Loss: 0.3289236481431671 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 9 out of 93 Training Loss: 0.3524751941803642 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 10 out of 93 Training Loss: 0.3798984344247528 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 11 out of 93 Training Loss: 0.4032600185964056 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 12 out of 93 Training Loss: 0.4311247429940649 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 13 out of 93 Training Loss: 0.46086420483326396 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 14 out of 93 Training Loss: 0.49860285587048014 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 15 out of 93 Training Loss: 0.5259099415633627 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 16 out of 93 Training Loss: 0.5479508991483398 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 17 out of 93 Training Loss: 0.5617018583360859 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 18 out of 93 Training Loss: 0.5747565887067266 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 19 out of 93 Training Loss: 0.6098025030586668 Test Loss: 0.06099853833967989\n",
      "Epoch: 1 Batch: 20 out of 93 Training Loss: 0.006774645257801083 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 21 out of 93 Training Loss: 0.024453099776119258 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 22 out of 93 Training Loss: 0.04156273226818469 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 23 out of 93 Training Loss: 0.05863322127243903 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 24 out of 93 Training Loss: 0.06681118342658904 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 25 out of 93 Training Loss: 0.07221391606709388 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 26 out of 93 Training Loss: 0.0796934659183624 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 27 out of 93 Training Loss: 0.08984563635295656 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 28 out of 93 Training Loss: 0.10006074929243353 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 29 out of 93 Training Loss: 0.11569158123439577 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 30 out of 93 Training Loss: 0.1289784232676628 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 31 out of 93 Training Loss: 0.1416748980582359 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 32 out of 93 Training Loss: 0.1590631487370136 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 33 out of 93 Training Loss: 0.16988290050631788 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 34 out of 93 Training Loss: 0.18768801862007406 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 35 out of 93 Training Loss: 0.2009762613624933 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 36 out of 93 Training Loss: 0.21318490752583769 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 37 out of 93 Training Loss: 0.22389108264452245 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 38 out of 93 Training Loss: 0.23407444493478563 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 39 out of 93 Training Loss: 0.25270781056588915 Test Loss: 0.01664561837572943\n",
      "Epoch: 1 Batch: 40 out of 93 Training Loss: 0.0028906628154625204 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 41 out of 93 Training Loss: 0.013902579959522656 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 42 out of 93 Training Loss: 0.028931276466738154 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 43 out of 93 Training Loss: 0.040806293081532885 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 44 out of 93 Training Loss: 0.050463159162591864 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 45 out of 93 Training Loss: 0.06012708280612128 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 46 out of 93 Training Loss: 0.07324156683314936 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 47 out of 93 Training Loss: 0.0876277895309438 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 48 out of 93 Training Loss: 0.10950717196245091 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 49 out of 93 Training Loss: 0.1181617049165477 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 50 out of 93 Training Loss: 0.13173573702950375 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 51 out of 93 Training Loss: 0.1436011293568124 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 52 out of 93 Training Loss: 0.15601288766045468 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 53 out of 93 Training Loss: 0.17648483492810624 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 54 out of 93 Training Loss: 0.1933708643205871 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 55 out of 93 Training Loss: 0.21093917318065064 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 56 out of 93 Training Loss: 0.2205341426320781 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 57 out of 93 Training Loss: 0.22942513652522462 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 58 out of 93 Training Loss: 0.23753236379851 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 59 out of 93 Training Loss: 0.24952046905089514 Test Loss: 0.012944478456946936\n",
      "Epoch: 1 Batch: 60 out of 93 Training Loss: 0.00287430861187762 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 61 out of 93 Training Loss: 0.01357561291349715 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 62 out of 93 Training Loss: 0.02342923951489514 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 63 out of 93 Training Loss: 0.03744652622533626 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 64 out of 93 Training Loss: 0.048822649169393245 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 65 out of 93 Training Loss: 0.0622120968733508 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 66 out of 93 Training Loss: 0.07137741835010833 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 67 out of 93 Training Loss: 0.08746401534570045 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 68 out of 93 Training Loss: 0.09950152353537864 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 69 out of 93 Training Loss: 0.10943466180158443 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 70 out of 93 Training Loss: 0.12181480729175395 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 71 out of 93 Training Loss: 0.1365964779366333 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 72 out of 93 Training Loss: 0.1475037423629839 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 73 out of 93 Training Loss: 0.16143978794259614 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 74 out of 93 Training Loss: 0.17318059911174363 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 75 out of 93 Training Loss: 0.17849640256879515 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 76 out of 93 Training Loss: 0.18961446906594223 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 77 out of 93 Training Loss: 0.20872330467370934 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 78 out of 93 Training Loss: 0.2262017260041613 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 79 out of 93 Training Loss: 0.2361802951168794 Test Loss: 0.012529002079232172\n",
      "Epoch: 1 Batch: 80 out of 93 Training Loss: 0.002620965932081868 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 81 out of 93 Training Loss: 0.011726737712095906 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 82 out of 93 Training Loss: 0.0212063274723994 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 83 out of 93 Training Loss: 0.034704004172753024 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 84 out of 93 Training Loss: 0.05449529992468231 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 85 out of 93 Training Loss: 0.07146253319508902 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 86 out of 93 Training Loss: 0.08760366471536032 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 87 out of 93 Training Loss: 0.09910353714651934 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 88 out of 93 Training Loss: 0.10939143667049281 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 89 out of 93 Training Loss: 0.11824774438567034 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 90 out of 93 Training Loss: 0.13262872280604712 Test Loss: 0.011963869953020052\n",
      "Epoch: 1 Batch: 91 out of 93 Training Loss: 0.14087865011818282 Test Loss: 0.011963869953020052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 92 out of 93 Training Loss: 0.14750332919932477 Test Loss: 0.011963869953020052\n",
      "Epoch: 2 Batch: 0 out of 93 Training Loss: 0.00012529593321584885 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 1 out of 93 Training Loss: 0.010771549757449858 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 2 out of 93 Training Loss: 0.024484823908536665 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 3 out of 93 Training Loss: 0.04220760193082594 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 4 out of 93 Training Loss: 0.057126326005785696 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 5 out of 93 Training Loss: 0.06466167558345103 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 6 out of 93 Training Loss: 0.08193839665862822 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 7 out of 93 Training Loss: 0.0931827309391191 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 8 out of 93 Training Loss: 0.1030607218703916 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 9 out of 93 Training Loss: 0.11637581091734671 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 10 out of 93 Training Loss: 0.12694132466229702 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 11 out of 93 Training Loss: 0.14443956639978195 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 12 out of 93 Training Loss: 0.15754353393229747 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 13 out of 93 Training Loss: 0.1713523440844109 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 14 out of 93 Training Loss: 0.18004744544985796 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 15 out of 93 Training Loss: 0.1913709179348042 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 16 out of 93 Training Loss: 0.20706410192313696 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 17 out of 93 Training Loss: 0.2136931842974117 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 18 out of 93 Training Loss: 0.22018542744579817 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 19 out of 93 Training Loss: 0.23407245099905039 Test Loss: 0.012146893931044773\n",
      "Epoch: 2 Batch: 20 out of 93 Training Loss: 0.0026310758653574633 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 21 out of 93 Training Loss: 0.015695184654324595 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 22 out of 93 Training Loss: 0.02605022322270209 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 23 out of 93 Training Loss: 0.039062460138230864 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 24 out of 93 Training Loss: 0.05284561328473622 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 25 out of 93 Training Loss: 0.06122374366852576 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 26 out of 93 Training Loss: 0.07406626846525008 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 27 out of 93 Training Loss: 0.09242496285471255 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 28 out of 93 Training Loss: 0.10033121351036364 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 29 out of 93 Training Loss: 0.10954172089579636 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 30 out of 93 Training Loss: 0.12580538466456467 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 31 out of 93 Training Loss: 0.1391915392118984 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 32 out of 93 Training Loss: 0.15058584157261187 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 33 out of 93 Training Loss: 0.16027891654404933 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 34 out of 93 Training Loss: 0.17337379157426888 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 35 out of 93 Training Loss: 0.1823088891792589 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 36 out of 93 Training Loss: 0.19082353375735814 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 37 out of 93 Training Loss: 0.19934206753852898 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 38 out of 93 Training Loss: 0.2167082272458368 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 39 out of 93 Training Loss: 0.23193665519240433 Test Loss: 0.0116333132609725\n",
      "Epoch: 2 Batch: 40 out of 93 Training Loss: 0.00263967690687698 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 41 out of 93 Training Loss: 0.026995647229962765 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 42 out of 93 Training Loss: 0.03284503891762418 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 43 out of 93 Training Loss: 0.041326439005099955 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 44 out of 93 Training Loss: 0.05974461510475797 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 45 out of 93 Training Loss: 0.08181653916534108 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 46 out of 93 Training Loss: 0.09702489711042565 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 47 out of 93 Training Loss: 0.10980546753551883 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 48 out of 93 Training Loss: 0.12282537016715926 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 49 out of 93 Training Loss: 0.13477698247578068 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 50 out of 93 Training Loss: 0.14585456344928666 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 51 out of 93 Training Loss: 0.1544376732751362 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 52 out of 93 Training Loss: 0.1625026967346303 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 53 out of 93 Training Loss: 0.17716288286890908 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 54 out of 93 Training Loss: 0.18270359933074637 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 55 out of 93 Training Loss: 0.19602471124079152 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 56 out of 93 Training Loss: 0.20856713410165234 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 57 out of 93 Training Loss: 0.2241012724086277 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 58 out of 93 Training Loss: 0.2351677799567215 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 59 out of 93 Training Loss: 0.24444040469523592 Test Loss: 0.011454574425112118\n",
      "Epoch: 2 Batch: 60 out of 93 Training Loss: 0.0027431947409141365 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 61 out of 93 Training Loss: 0.011817542765938074 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 62 out of 93 Training Loss: 0.02273389410664919 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 63 out of 93 Training Loss: 0.03446204487671736 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 64 out of 93 Training Loss: 0.04235632431199435 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 65 out of 93 Training Loss: 0.05625987790962342 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 66 out of 93 Training Loss: 0.0770922973050702 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 67 out of 93 Training Loss: 0.0829505165337551 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 68 out of 93 Training Loss: 0.09092801895429972 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 69 out of 93 Training Loss: 0.10122506779124621 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 70 out of 93 Training Loss: 0.10846121928160671 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 71 out of 93 Training Loss: 0.12249056289409879 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 72 out of 93 Training Loss: 0.1378721999234486 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 73 out of 93 Training Loss: 0.14471268553724173 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 74 out of 93 Training Loss: 0.15428925112327221 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 75 out of 93 Training Loss: 0.161879044887327 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 76 out of 93 Training Loss: 0.18110685695400122 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 77 out of 93 Training Loss: 0.1995871373682249 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 78 out of 93 Training Loss: 0.21173446339061144 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 79 out of 93 Training Loss: 0.22024834987213496 Test Loss: 0.011459914290092209\n",
      "Epoch: 2 Batch: 80 out of 93 Training Loss: 0.0024431310586540806 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 81 out of 93 Training Loss: 0.01144678890629698 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 82 out of 93 Training Loss: 0.021941993384799255 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 83 out of 93 Training Loss: 0.037913080304106964 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 84 out of 93 Training Loss: 0.04925639927311827 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 85 out of 93 Training Loss: 0.062088220379492534 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 86 out of 93 Training Loss: 0.06996339559479643 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 87 out of 93 Training Loss: 0.08024146035238196 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 88 out of 93 Training Loss: 0.0919242687515824 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 89 out of 93 Training Loss: 0.10335821472032954 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 90 out of 93 Training Loss: 0.11065901070757796 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 91 out of 93 Training Loss: 0.12040731683298041 Test Loss: 0.010892384092916142\n",
      "Epoch: 2 Batch: 92 out of 93 Training Loss: 0.1284335060037582 Test Loss: 0.010892384092916142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 0 out of 93 Training Loss: 0.00014478305695197916 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 1 out of 93 Training Loss: 0.006845092981733302 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 2 out of 93 Training Loss: 0.02221768686888359 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 3 out of 93 Training Loss: 0.03692612855104349 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 4 out of 93 Training Loss: 0.047229262649692516 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 5 out of 93 Training Loss: 0.06522575637666128 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 6 out of 93 Training Loss: 0.08367399556902788 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 7 out of 93 Training Loss: 0.09479539154437921 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 8 out of 93 Training Loss: 0.10267327783969782 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 9 out of 93 Training Loss: 0.11651619674978397 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 10 out of 93 Training Loss: 0.13019151671198748 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 11 out of 93 Training Loss: 0.1434889357757344 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 12 out of 93 Training Loss: 0.1525835819762721 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 13 out of 93 Training Loss: 0.1606204729747548 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 14 out of 93 Training Loss: 0.17060274968264244 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 15 out of 93 Training Loss: 0.18229828926143787 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 16 out of 93 Training Loss: 0.19352990726349495 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 17 out of 93 Training Loss: 0.2037813352641239 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 18 out of 93 Training Loss: 0.21154351590541742 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 19 out of 93 Training Loss: 0.22459316684273622 Test Loss: 0.011317790274254301\n",
      "Epoch: 3 Batch: 20 out of 93 Training Loss: 0.0025789053391991274 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 21 out of 93 Training Loss: 0.015245300273257316 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 22 out of 93 Training Loss: 0.0291536045473395 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 23 out of 93 Training Loss: 0.0445655831348477 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 24 out of 93 Training Loss: 0.0536115025382815 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 25 out of 93 Training Loss: 0.06866801113154275 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 26 out of 93 Training Loss: 0.08289449990059716 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 27 out of 93 Training Loss: 0.09303346418108327 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 28 out of 93 Training Loss: 0.09676251702274663 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 29 out of 93 Training Loss: 0.11063176535333974 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 30 out of 93 Training Loss: 0.13240246244038922 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 31 out of 93 Training Loss: 0.14491078771914823 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 32 out of 93 Training Loss: 0.15805553600456101 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 33 out of 93 Training Loss: 0.16527870811786038 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 34 out of 93 Training Loss: 0.17367955811585767 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 35 out of 93 Training Loss: 0.17869732391353232 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 36 out of 93 Training Loss: 0.184336175697702 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 37 out of 93 Training Loss: 0.19729487367238385 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 38 out of 93 Training Loss: 0.20561981436129195 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 39 out of 93 Training Loss: 0.21682492506023032 Test Loss: 0.010524669408120892\n",
      "Epoch: 3 Batch: 40 out of 93 Training Loss: 0.002417887496159472 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 41 out of 93 Training Loss: 0.008542448217378058 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 42 out of 93 Training Loss: 0.020718533480868735 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 43 out of 93 Training Loss: 0.026942180404768385 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 44 out of 93 Training Loss: 0.037341471696601305 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 45 out of 93 Training Loss: 0.04571347118388215 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 46 out of 93 Training Loss: 0.056041398020790015 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 47 out of 93 Training Loss: 0.06538197391699353 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 48 out of 93 Training Loss: 0.07163814541916648 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 49 out of 93 Training Loss: 0.08383049343745509 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 50 out of 93 Training Loss: 0.09575208456288377 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 51 out of 93 Training Loss: 0.1048408112771372 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 52 out of 93 Training Loss: 0.11426346183787385 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 53 out of 93 Training Loss: 0.12080473260055581 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 54 out of 93 Training Loss: 0.13058237166176834 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 55 out of 93 Training Loss: 0.1425124350286583 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 56 out of 93 Training Loss: 0.16133918420444526 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 57 out of 93 Training Loss: 0.17450771288047828 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 58 out of 93 Training Loss: 0.17932134720529713 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 59 out of 93 Training Loss: 0.18623071390237012 Test Loss: 0.010620811192149466\n",
      "Epoch: 3 Batch: 60 out of 93 Training Loss: 0.002118684485938771 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 61 out of 93 Training Loss: 0.010683325253811105 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 62 out of 93 Training Loss: 0.01811982518472932 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 63 out of 93 Training Loss: 0.038388451695826276 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 64 out of 93 Training Loss: 0.05502241863348745 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 65 out of 93 Training Loss: 0.061508758593734725 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 66 out of 93 Training Loss: 0.07236909367555044 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 67 out of 93 Training Loss: 0.08336651385241411 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 68 out of 93 Training Loss: 0.09716725595706366 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 69 out of 93 Training Loss: 0.10902427196913622 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 70 out of 93 Training Loss: 0.11968326069825552 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 71 out of 93 Training Loss: 0.12743573230170868 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 72 out of 93 Training Loss: 0.13499970002510808 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 73 out of 93 Training Loss: 0.1455273173231747 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 74 out of 93 Training Loss: 0.1522238302458193 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 75 out of 93 Training Loss: 0.16096087838002704 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 76 out of 93 Training Loss: 0.1734966450784471 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 77 out of 93 Training Loss: 0.18142749196329377 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 78 out of 93 Training Loss: 0.1983311587426927 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 79 out of 93 Training Loss: 0.20668013686725162 Test Loss: 0.01064108003100211\n",
      "Epoch: 3 Batch: 80 out of 93 Training Loss: 0.0023315056812419966 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 81 out of 93 Training Loss: 0.010123751743550546 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 82 out of 93 Training Loss: 0.019624709769006023 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 83 out of 93 Training Loss: 0.03019909622412468 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 84 out of 93 Training Loss: 0.039032899460609206 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 85 out of 93 Training Loss: 0.04929238433283115 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 86 out of 93 Training Loss: 0.05831523115080143 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 87 out of 93 Training Loss: 0.06702638106685424 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 88 out of 93 Training Loss: 0.07676623264592433 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 89 out of 93 Training Loss: 0.0833184878927126 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 90 out of 93 Training Loss: 0.0920801220115676 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 91 out of 93 Training Loss: 0.10001838781204367 Test Loss: 0.009984467432580212\n",
      "Epoch: 3 Batch: 92 out of 93 Training Loss: 0.11368248161193276 Test Loss: 0.009984467432580212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 0 out of 93 Training Loss: 0.00013487099079034662 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 1 out of 93 Training Loss: 0.010211097657360056 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 2 out of 93 Training Loss: 0.020479130208171825 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 3 out of 93 Training Loss: 0.02956600265917919 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 4 out of 93 Training Loss: 0.038286655790783386 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 5 out of 93 Training Loss: 0.05027411004868887 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 6 out of 93 Training Loss: 0.060639970585383396 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 7 out of 93 Training Loss: 0.07007073669103525 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 8 out of 93 Training Loss: 0.08261234557596586 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 9 out of 93 Training Loss: 0.09185838198629759 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 10 out of 93 Training Loss: 0.09798145631668709 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 11 out of 93 Training Loss: 0.10846071375874422 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 12 out of 93 Training Loss: 0.1214473309215679 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 13 out of 93 Training Loss: 0.13255669886586807 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 14 out of 93 Training Loss: 0.1410547426190748 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 15 out of 93 Training Loss: 0.1488982780125513 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 16 out of 93 Training Loss: 0.15995474124667786 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 17 out of 93 Training Loss: 0.170253321177937 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 18 out of 93 Training Loss: 0.18027179466900967 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 19 out of 93 Training Loss: 0.18929653765974186 Test Loss: 0.010103396775031631\n",
      "Epoch: 4 Batch: 20 out of 93 Training Loss: 0.0021553620137640377 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 21 out of 93 Training Loss: 0.00965984569866146 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 22 out of 93 Training Loss: 0.01970644606281008 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 23 out of 93 Training Loss: 0.03230686238039221 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 24 out of 93 Training Loss: 0.04048723759133305 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 25 out of 93 Training Loss: 0.054267527852244035 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 26 out of 93 Training Loss: 0.06188619727272476 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 27 out of 93 Training Loss: 0.06943028265613999 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 28 out of 93 Training Loss: 0.08228764714676823 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 29 out of 93 Training Loss: 0.0914907561056682 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 30 out of 93 Training Loss: 0.0999616769515344 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 31 out of 93 Training Loss: 0.1088903875463077 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 32 out of 93 Training Loss: 0.1174304004200169 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 33 out of 93 Training Loss: 0.12318145701665367 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 34 out of 93 Training Loss: 0.13502882452685322 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 35 out of 93 Training Loss: 0.14276348179209913 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 36 out of 93 Training Loss: 0.15243008485067572 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 37 out of 93 Training Loss: 0.15957809410992946 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 38 out of 93 Training Loss: 0.1689193731546768 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 39 out of 93 Training Loss: 0.18351243887100305 Test Loss: 0.010171042712913319\n",
      "Epoch: 4 Batch: 40 out of 93 Training Loss: 0.0021195653108984002 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 41 out of 93 Training Loss: 0.014229456822398283 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 42 out of 93 Training Loss: 0.024462825546982862 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 43 out of 93 Training Loss: 0.036831111575487234 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 44 out of 93 Training Loss: 0.04556580532622633 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 45 out of 93 Training Loss: 0.057706682871106245 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 46 out of 93 Training Loss: 0.06941888753605185 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 47 out of 93 Training Loss: 0.07716765247565804 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 48 out of 93 Training Loss: 0.08436119083595572 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 49 out of 93 Training Loss: 0.09148616476264892 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 50 out of 93 Training Loss: 0.09843582419959007 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 51 out of 93 Training Loss: 0.11692879481521545 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 52 out of 93 Training Loss: 0.12451241810155211 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 53 out of 93 Training Loss: 0.13900902301353513 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 54 out of 93 Training Loss: 0.14933457218391 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 55 out of 93 Training Loss: 0.165959263148728 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 56 out of 93 Training Loss: 0.17759794548434316 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 57 out of 93 Training Loss: 0.18733164102119504 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 58 out of 93 Training Loss: 0.19964241229278146 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 59 out of 93 Training Loss: 0.214143554608348 Test Loss: 0.009798674209212715\n",
      "Epoch: 4 Batch: 60 out of 93 Training Loss: 0.0024762233230837248 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 61 out of 93 Training Loss: 0.014008885759449806 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 62 out of 93 Training Loss: 0.030331409114933816 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 63 out of 93 Training Loss: 0.04129898785386166 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 64 out of 93 Training Loss: 0.050757138568497506 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 65 out of 93 Training Loss: 0.05921585629466852 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 66 out of 93 Training Loss: 0.06868235352221808 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 67 out of 93 Training Loss: 0.0796746942657598 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 68 out of 93 Training Loss: 0.08875718205008587 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 69 out of 93 Training Loss: 0.09602811145368895 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 70 out of 93 Training Loss: 0.10552684548083624 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 71 out of 93 Training Loss: 0.11620312141958555 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 72 out of 93 Training Loss: 0.12223242833051762 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 73 out of 93 Training Loss: 0.1305310898903378 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 74 out of 93 Training Loss: 0.1398060671496757 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 75 out of 93 Training Loss: 0.14884072406921467 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 76 out of 93 Training Loss: 0.15705250627133926 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 77 out of 93 Training Loss: 0.17239435999426922 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 78 out of 93 Training Loss: 0.1789130443218716 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 79 out of 93 Training Loss: 0.18949290509108385 Test Loss: 0.009944257622754032\n",
      "Epoch: 4 Batch: 80 out of 93 Training Loss: 0.002205180740760612 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 81 out of 93 Training Loss: 0.010352583407269763 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 82 out of 93 Training Loss: 0.019886095813320924 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 83 out of 93 Training Loss: 0.0247133479368258 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 84 out of 93 Training Loss: 0.03672567968334441 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 85 out of 93 Training Loss: 0.04279416221271639 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 86 out of 93 Training Loss: 0.050732331155644704 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 87 out of 93 Training Loss: 0.06563859599720126 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 88 out of 93 Training Loss: 0.07223398777733928 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 89 out of 93 Training Loss: 0.08200856837521678 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 90 out of 93 Training Loss: 0.09419936853538638 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 91 out of 93 Training Loss: 0.10715816523026114 Test Loss: 0.009688539303500544\n",
      "Epoch: 4 Batch: 92 out of 93 Training Loss: 0.11436427225436455 Test Loss: 0.009688539303500544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Batch: 0 out of 93 Training Loss: 0.0001209990111409977 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 1 out of 93 Training Loss: 0.015346496085566218 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 2 out of 93 Training Loss: 0.027852493989211257 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 3 out of 93 Training Loss: 0.03802545899425143 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 4 out of 93 Training Loss: 0.05249562831495398 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 5 out of 93 Training Loss: 0.06196494558725947 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 6 out of 93 Training Loss: 0.07176244603369826 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 7 out of 93 Training Loss: 0.08467850619826907 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 8 out of 93 Training Loss: 0.09844007128749484 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 9 out of 93 Training Loss: 0.1098069461983859 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 10 out of 93 Training Loss: 0.11984186777744883 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 11 out of 93 Training Loss: 0.12738253986362807 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 12 out of 93 Training Loss: 0.1390937461507737 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 13 out of 93 Training Loss: 0.14747453546051376 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 14 out of 93 Training Loss: 0.15719239851359718 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 15 out of 93 Training Loss: 0.17112211203102462 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 16 out of 93 Training Loss: 0.18131620501999252 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 17 out of 93 Training Loss: 0.19056356949631564 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 18 out of 93 Training Loss: 0.19767857547749274 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 19 out of 93 Training Loss: 0.20918228682030432 Test Loss: 0.010637175376442346\n",
      "Epoch: 5 Batch: 20 out of 93 Training Loss: 0.002337197084169361 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 21 out of 93 Training Loss: 0.01574919397030351 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 22 out of 93 Training Loss: 0.026606975365619633 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 23 out of 93 Training Loss: 0.04364755215142248 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 24 out of 93 Training Loss: 0.054995354507307984 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 25 out of 93 Training Loss: 0.06113097840939043 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 26 out of 93 Training Loss: 0.0705187742227721 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 27 out of 93 Training Loss: 0.07606652962062833 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 28 out of 93 Training Loss: 0.08636981552485702 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 29 out of 93 Training Loss: 0.09383157804627297 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 30 out of 93 Training Loss: 0.1029099912943351 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 31 out of 93 Training Loss: 0.1111430970581281 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 32 out of 93 Training Loss: 0.12269450422395346 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 33 out of 93 Training Loss: 0.13165396709014054 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 34 out of 93 Training Loss: 0.14149131789822456 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 35 out of 93 Training Loss: 0.15329571139086362 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 36 out of 93 Training Loss: 0.17062519561518308 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 37 out of 93 Training Loss: 0.18440927356500503 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 38 out of 93 Training Loss: 0.1952486576141822 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 39 out of 93 Training Loss: 0.2064439233021843 Test Loss: 0.009612435390326109\n",
      "Epoch: 5 Batch: 40 out of 93 Training Loss: 0.002280328850994943 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 41 out of 93 Training Loss: 0.011581177320133088 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 42 out of 93 Training Loss: 0.020454489122877954 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 43 out of 93 Training Loss: 0.027334879580686925 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 44 out of 93 Training Loss: 0.034966930243919725 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 45 out of 93 Training Loss: 0.042793403807829256 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 46 out of 93 Training Loss: 0.05192518928999745 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 47 out of 93 Training Loss: 0.06134322142208659 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 48 out of 93 Training Loss: 0.0722031094690665 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 49 out of 93 Training Loss: 0.08006635909880244 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 50 out of 93 Training Loss: 0.08779530921931111 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 51 out of 93 Training Loss: 0.09669310176725232 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 52 out of 93 Training Loss: 0.10421512598152005 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 53 out of 93 Training Loss: 0.11360103214139783 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 54 out of 93 Training Loss: 0.12419840270633542 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 55 out of 93 Training Loss: 0.12946751244346344 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 56 out of 93 Training Loss: 0.14441387444714748 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 57 out of 93 Training Loss: 0.1579176932198807 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 58 out of 93 Training Loss: 0.17150302972624742 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 59 out of 93 Training Loss: 0.1761837182192012 Test Loss: 0.009659627240828493\n",
      "Epoch: 5 Batch: 60 out of 93 Training Loss: 0.001990880125790946 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 61 out of 93 Training Loss: 0.012656484970480315 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 62 out of 93 Training Loss: 0.023030583055347317 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 63 out of 93 Training Loss: 0.030676493661135548 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 64 out of 93 Training Loss: 0.04172723857465398 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 65 out of 93 Training Loss: 0.050550726482302064 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 66 out of 93 Training Loss: 0.06297862833145272 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 67 out of 93 Training Loss: 0.06855949131551396 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 68 out of 93 Training Loss: 0.07724326966109883 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 69 out of 93 Training Loss: 0.08628525795194518 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 70 out of 93 Training Loss: 0.09489956335875403 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 71 out of 93 Training Loss: 0.10683198110911261 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 72 out of 93 Training Loss: 0.11816465506109607 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 73 out of 93 Training Loss: 0.12740681996348513 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 74 out of 93 Training Loss: 0.14088047785463942 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 75 out of 93 Training Loss: 0.15468505048903836 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 76 out of 93 Training Loss: 0.16501291120204342 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 77 out of 93 Training Loss: 0.1732720548823775 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 78 out of 93 Training Loss: 0.18285662052932394 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 79 out of 93 Training Loss: 0.19322105800512446 Test Loss: 0.010111592286689714\n",
      "Epoch: 5 Batch: 80 out of 93 Training Loss: 0.0021627822375909628 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 81 out of 93 Training Loss: 0.008620593459071332 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 82 out of 93 Training Loss: 0.01945990859037798 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 83 out of 93 Training Loss: 0.029367407546104604 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 84 out of 93 Training Loss: 0.03961924812263411 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 85 out of 93 Training Loss: 0.049959962949814016 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 86 out of 93 Training Loss: 0.06335326260394018 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 87 out of 93 Training Loss: 0.07681912066853207 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 88 out of 93 Training Loss: 0.09078244282370966 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 89 out of 93 Training Loss: 0.10127661353415411 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 90 out of 93 Training Loss: 0.11132083157515686 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 91 out of 93 Training Loss: 0.1254936146966235 Test Loss: 0.009616594761610031\n",
      "Epoch: 5 Batch: 92 out of 93 Training Loss: 0.1356950856230275 Test Loss: 0.009616594761610031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Batch: 0 out of 93 Training Loss: 8.464424360182977e-05 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 1 out of 93 Training Loss: 0.007499558445546896 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 2 out of 93 Training Loss: 0.014980163950953753 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 3 out of 93 Training Loss: 0.027424149740968018 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 4 out of 93 Training Loss: 0.0394631331456044 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 5 out of 93 Training Loss: 0.05390895024362591 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 6 out of 93 Training Loss: 0.06326440122399118 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 7 out of 93 Training Loss: 0.07521712739441183 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 8 out of 93 Training Loss: 0.08297567308369663 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 9 out of 93 Training Loss: 0.09086994134310272 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 10 out of 93 Training Loss: 0.0992686157715657 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 11 out of 93 Training Loss: 0.10801757831547049 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 12 out of 93 Training Loss: 0.11704357300374296 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 13 out of 93 Training Loss: 0.1277173567232826 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 14 out of 93 Training Loss: 0.1422692924407461 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 15 out of 93 Training Loss: 0.15159377787682798 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 16 out of 93 Training Loss: 0.1604677043524721 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 17 out of 93 Training Loss: 0.16605885426003125 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 18 out of 93 Training Loss: 0.17357731756243494 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 19 out of 93 Training Loss: 0.18151275214228418 Test Loss: 0.010027389211410826\n",
      "Epoch: 6 Batch: 20 out of 93 Training Loss: 0.0020327880654575775 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 21 out of 93 Training Loss: 0.01610765751936459 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 22 out of 93 Training Loss: 0.027600176227474446 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 23 out of 93 Training Loss: 0.03901635814943337 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 24 out of 93 Training Loss: 0.04785146151223206 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 25 out of 93 Training Loss: 0.06604399864115738 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 26 out of 93 Training Loss: 0.07453017548211836 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 27 out of 93 Training Loss: 0.08467375539728426 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 28 out of 93 Training Loss: 0.09261826944091343 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 29 out of 93 Training Loss: 0.09904054325559163 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 30 out of 93 Training Loss: 0.10777769569792771 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 31 out of 93 Training Loss: 0.11666144576408886 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 32 out of 93 Training Loss: 0.12756048102417017 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 33 out of 93 Training Loss: 0.14082351100184942 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 34 out of 93 Training Loss: 0.14671396423616434 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 35 out of 93 Training Loss: 0.15544480417408968 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 36 out of 93 Training Loss: 0.16399491072186256 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 37 out of 93 Training Loss: 0.17740182165541674 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 38 out of 93 Training Loss: 0.1853628424603584 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 39 out of 93 Training Loss: 0.19695628859587933 Test Loss: 0.010070641186426987\n",
      "Epoch: 6 Batch: 40 out of 93 Training Loss: 0.0022234725071418695 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 41 out of 93 Training Loss: 0.015808414132736964 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 42 out of 93 Training Loss: 0.028481954173587603 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 43 out of 93 Training Loss: 0.03754149230703001 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 44 out of 93 Training Loss: 0.047562226117991255 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 45 out of 93 Training Loss: 0.05951252433045988 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 46 out of 93 Training Loss: 0.07098808458580856 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 47 out of 93 Training Loss: 0.08818559108807926 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 48 out of 93 Training Loss: 0.1056002447618473 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 49 out of 93 Training Loss: 0.11521889681502466 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 50 out of 93 Training Loss: 0.12687445471569186 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 51 out of 93 Training Loss: 0.13455703335388786 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 52 out of 93 Training Loss: 0.14514921365811712 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 53 out of 93 Training Loss: 0.1565331699563492 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 54 out of 93 Training Loss: 0.16778059902175552 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 55 out of 93 Training Loss: 0.17369225288315898 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 56 out of 93 Training Loss: 0.18276660649611837 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 57 out of 93 Training Loss: 0.19371783866121656 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 58 out of 93 Training Loss: 0.20334185606374627 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 59 out of 93 Training Loss: 0.21868287904693967 Test Loss: 0.009830461391671137\n",
      "Epoch: 6 Batch: 60 out of 93 Training Loss: 0.0024606347407047343 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 61 out of 93 Training Loss: 0.01529832411062165 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 62 out of 93 Training Loss: 0.03059699985634728 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 63 out of 93 Training Loss: 0.03844127565127059 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 64 out of 93 Training Loss: 0.04825220111262246 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 65 out of 93 Training Loss: 0.06327149033051177 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 66 out of 93 Training Loss: 0.07424403860669776 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 67 out of 93 Training Loss: 0.08464994135748788 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 68 out of 93 Training Loss: 0.09619760367524072 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 69 out of 93 Training Loss: 0.1049561933005993 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 70 out of 93 Training Loss: 0.11444056912761374 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 71 out of 93 Training Loss: 0.12782113201420947 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 72 out of 93 Training Loss: 0.13262105401110097 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 73 out of 93 Training Loss: 0.13966868858289166 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 74 out of 93 Training Loss: 0.15191769826661988 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 75 out of 93 Training Loss: 0.1618272740716879 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 76 out of 93 Training Loss: 0.16906633780773922 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 77 out of 93 Training Loss: 0.1774117033320312 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 78 out of 93 Training Loss: 0.1857859212296848 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 79 out of 93 Training Loss: 0.19871245132263943 Test Loss: 0.009554801720448515\n",
      "Epoch: 6 Batch: 80 out of 93 Training Loss: 0.0021943296950218544 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 81 out of 93 Training Loss: 0.014811003179466473 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 82 out of 93 Training Loss: 0.021533343539750324 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 83 out of 93 Training Loss: 0.03236393497250055 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 84 out of 93 Training Loss: 0.043007989012813316 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 85 out of 93 Training Loss: 0.049100739480173336 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 86 out of 93 Training Loss: 0.05578183668590163 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 87 out of 93 Training Loss: 0.06388404785580014 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 88 out of 93 Training Loss: 0.07163526962435816 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 89 out of 93 Training Loss: 0.07828381291515206 Test Loss: 0.010466778989542614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Batch: 90 out of 93 Training Loss: 0.08466296742594813 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 91 out of 93 Training Loss: 0.09134764789528226 Test Loss: 0.010466778989542614\n",
      "Epoch: 6 Batch: 92 out of 93 Training Loss: 0.09333653265534912 Test Loss: 0.010466778989542614\n",
      "Epoch: 7 Batch: 0 out of 93 Training Loss: 9.126947211321964e-05 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 1 out of 93 Training Loss: 0.006226927833130924 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 2 out of 93 Training Loss: 0.017138804168871015 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 3 out of 93 Training Loss: 0.023661421471706003 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 4 out of 93 Training Loss: 0.036796087258926004 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 5 out of 93 Training Loss: 0.0419996095350593 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 6 out of 93 Training Loss: 0.04912484656037983 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 7 out of 93 Training Loss: 0.05823081578077969 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 8 out of 93 Training Loss: 0.06669221478464302 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 9 out of 93 Training Loss: 0.08521242472293075 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 10 out of 93 Training Loss: 0.09787521357598003 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 11 out of 93 Training Loss: 0.10244085858525928 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 12 out of 93 Training Loss: 0.11298429253461059 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 13 out of 93 Training Loss: 0.13478987324001487 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 14 out of 93 Training Loss: 0.1415566074191242 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 15 out of 93 Training Loss: 0.14968740087883767 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 16 out of 93 Training Loss: 0.16101862103628214 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 17 out of 93 Training Loss: 0.17147602871941622 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 18 out of 93 Training Loss: 0.1839165397449046 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 19 out of 93 Training Loss: 0.18832416259633597 Test Loss: 0.009912701619958336\n",
      "Epoch: 7 Batch: 20 out of 93 Training Loss: 0.0021526274126049327 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 21 out of 93 Training Loss: 0.00855497697778309 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 22 out of 93 Training Loss: 0.01671510616727436 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 23 out of 93 Training Loss: 0.027071861163156873 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 24 out of 93 Training Loss: 0.03432772037051881 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 25 out of 93 Training Loss: 0.0425184947293278 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 26 out of 93 Training Loss: 0.04955672108046973 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 27 out of 93 Training Loss: 0.06215170499347413 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 28 out of 93 Training Loss: 0.07250902902744974 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 29 out of 93 Training Loss: 0.0825633873086449 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 30 out of 93 Training Loss: 0.08924401745610203 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 31 out of 93 Training Loss: 0.09907518893890346 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 32 out of 93 Training Loss: 0.10612978328429426 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 33 out of 93 Training Loss: 0.1189969620522138 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 34 out of 93 Training Loss: 0.12708354562931026 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 35 out of 93 Training Loss: 0.13743129752986397 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 36 out of 93 Training Loss: 0.1502251216065165 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 37 out of 93 Training Loss: 0.15915725294910635 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 38 out of 93 Training Loss: 0.1650177669566151 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 39 out of 93 Training Loss: 0.17627008825950588 Test Loss: 0.00960759153928269\n",
      "Epoch: 7 Batch: 40 out of 93 Training Loss: 0.0019882047176801995 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 41 out of 93 Training Loss: 0.013834313079758867 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 42 out of 93 Training Loss: 0.022734833471521124 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 43 out of 93 Training Loss: 0.033497288413389906 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 44 out of 93 Training Loss: 0.048055097051008924 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 45 out of 93 Training Loss: 0.05537982895974754 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 46 out of 93 Training Loss: 0.062378426604017004 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 47 out of 93 Training Loss: 0.06913722079251645 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 48 out of 93 Training Loss: 0.08411457628016113 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 49 out of 93 Training Loss: 0.09557708218698142 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 50 out of 93 Training Loss: 0.1054160264879906 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 51 out of 93 Training Loss: 0.11339757923041223 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 52 out of 93 Training Loss: 0.12033793475483297 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 53 out of 93 Training Loss: 0.12556471820925114 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 54 out of 93 Training Loss: 0.13707810226832268 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 55 out of 93 Training Loss: 0.14383793674413797 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 56 out of 93 Training Loss: 0.15947556547824976 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 57 out of 93 Training Loss: 0.1709218533337557 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 58 out of 93 Training Loss: 0.17697600979362366 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 59 out of 93 Training Loss: 0.18921299051497814 Test Loss: 0.009694252586500212\n",
      "Epoch: 7 Batch: 60 out of 93 Training Loss: 0.002140569305949147 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 61 out of 93 Training Loss: 0.0074044128795790025 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 62 out of 93 Training Loss: 0.02073233741425746 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 63 out of 93 Training Loss: 0.030447188570134575 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 64 out of 93 Training Loss: 0.03798786438160413 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 65 out of 93 Training Loss: 0.04818077459090227 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 66 out of 93 Training Loss: 0.05826254821472639 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 67 out of 93 Training Loss: 0.06953475511723035 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 68 out of 93 Training Loss: 0.07641458678149456 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 69 out of 93 Training Loss: 0.08923710490667099 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 70 out of 93 Training Loss: 0.10150170306765312 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 71 out of 93 Training Loss: 0.1105424722572493 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 72 out of 93 Training Loss: 0.12059731762015098 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 73 out of 93 Training Loss: 0.13743903095924132 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 74 out of 93 Training Loss: 0.14714156444274656 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 75 out of 93 Training Loss: 0.16133575960152619 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 76 out of 93 Training Loss: 0.1717819653367328 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 77 out of 93 Training Loss: 0.18225873149090283 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 78 out of 93 Training Loss: 0.19297077859842054 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 79 out of 93 Training Loss: 0.20396272073590033 Test Loss: 0.009843037887053057\n",
      "Epoch: 7 Batch: 80 out of 93 Training Loss: 0.002285212396443453 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 81 out of 93 Training Loss: 0.01516265713310298 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 82 out of 93 Training Loss: 0.02674121700859126 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 83 out of 93 Training Loss: 0.03471532196438369 Test Loss: 0.009569583376022902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Batch: 84 out of 93 Training Loss: 0.05074143887198028 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 85 out of 93 Training Loss: 0.059832270375252336 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 86 out of 93 Training Loss: 0.07010327198243198 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 87 out of 93 Training Loss: 0.08147009380793628 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 88 out of 93 Training Loss: 0.08997769851869401 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 89 out of 93 Training Loss: 0.09811943594998178 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 90 out of 93 Training Loss: 0.1070464943785077 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 91 out of 93 Training Loss: 0.11713384289926347 Test Loss: 0.009569583376022902\n",
      "Epoch: 7 Batch: 92 out of 93 Training Loss: 0.12592993502205668 Test Loss: 0.009569583376022902\n",
      "Epoch: 8 Batch: 0 out of 93 Training Loss: 0.00011518179008396722 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 1 out of 93 Training Loss: 0.009229073771626078 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 2 out of 93 Training Loss: 0.019336451315671528 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 3 out of 93 Training Loss: 0.024736428172678078 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 4 out of 93 Training Loss: 0.032187584792614306 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 5 out of 93 Training Loss: 0.04890691862010988 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 6 out of 93 Training Loss: 0.058006416079938734 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 7 out of 93 Training Loss: 0.07271125447505744 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 8 out of 93 Training Loss: 0.08479242865496905 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 9 out of 93 Training Loss: 0.09556765296065839 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 10 out of 93 Training Loss: 0.10809697583818467 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 11 out of 93 Training Loss: 0.11517345905804666 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 12 out of 93 Training Loss: 0.12420953996980222 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 13 out of 93 Training Loss: 0.13317609951400788 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 14 out of 93 Training Loss: 0.14138121624046596 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 15 out of 93 Training Loss: 0.14937288780266078 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 16 out of 93 Training Loss: 0.1603816794281605 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 17 out of 93 Training Loss: 0.1692789811691049 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 18 out of 93 Training Loss: 0.17839524616056712 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 19 out of 93 Training Loss: 0.18622261938208373 Test Loss: 0.00984381287443367\n",
      "Epoch: 8 Batch: 20 out of 93 Training Loss: 0.002100973610409489 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 21 out of 93 Training Loss: 0.011866240982540837 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 22 out of 93 Training Loss: 0.022008956630357495 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 23 out of 93 Training Loss: 0.03099053281975626 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 24 out of 93 Training Loss: 0.040461152081021065 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 25 out of 93 Training Loss: 0.05169908351881146 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 26 out of 93 Training Loss: 0.061862596531041855 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 27 out of 93 Training Loss: 0.07190843972993731 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 28 out of 93 Training Loss: 0.08524092588258624 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 29 out of 93 Training Loss: 0.09343567233217596 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 30 out of 93 Training Loss: 0.10252743702066779 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 31 out of 93 Training Loss: 0.11537026729119658 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 32 out of 93 Training Loss: 0.12764973122311474 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 33 out of 93 Training Loss: 0.13375850788933874 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 34 out of 93 Training Loss: 0.1427414270458901 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 35 out of 93 Training Loss: 0.14960082130191565 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 36 out of 93 Training Loss: 0.1552214908754611 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 37 out of 93 Training Loss: 0.1724363441920066 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 38 out of 93 Training Loss: 0.18469152295646907 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 39 out of 93 Training Loss: 0.19431573221144915 Test Loss: 0.009615502574227074\n",
      "Epoch: 8 Batch: 40 out of 93 Training Loss: 0.002182499803368533 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 41 out of 93 Training Loss: 0.010802543088559591 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 42 out of 93 Training Loss: 0.01639360638311931 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 43 out of 93 Training Loss: 0.023468926846746885 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 44 out of 93 Training Loss: 0.030446578114990675 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 45 out of 93 Training Loss: 0.03762914968631097 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 46 out of 93 Training Loss: 0.04289925128198095 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 47 out of 93 Training Loss: 0.053226558789376696 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 48 out of 93 Training Loss: 0.06520570285594889 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 49 out of 93 Training Loss: 0.07730348996794649 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 50 out of 93 Training Loss: 0.08388199500716158 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 51 out of 93 Training Loss: 0.09793670598394104 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 52 out of 93 Training Loss: 0.10698411457217165 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 53 out of 93 Training Loss: 0.11436008645198174 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 54 out of 93 Training Loss: 0.12267110852501222 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 55 out of 93 Training Loss: 0.13572218345454762 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 56 out of 93 Training Loss: 0.14829978981963227 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 57 out of 93 Training Loss: 0.15881343727907965 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 58 out of 93 Training Loss: 0.17018043884984085 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 59 out of 93 Training Loss: 0.181406191382323 Test Loss: 0.009585246189751408\n",
      "Epoch: 8 Batch: 60 out of 93 Training Loss: 0.0020748172638291773 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 61 out of 93 Training Loss: 0.010457481325911944 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 62 out of 93 Training Loss: 0.022549214841174548 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 63 out of 93 Training Loss: 0.02849301283557958 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 64 out of 93 Training Loss: 0.03854289518048114 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 65 out of 93 Training Loss: 0.047614180085557165 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 66 out of 93 Training Loss: 0.06389539928485698 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 67 out of 93 Training Loss: 0.07678414468963689 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 68 out of 93 Training Loss: 0.08380557650079078 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 69 out of 93 Training Loss: 0.0984257558112318 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 70 out of 93 Training Loss: 0.1121224580084259 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 71 out of 93 Training Loss: 0.11654644047220296 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 72 out of 93 Training Loss: 0.12493089267362899 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 73 out of 93 Training Loss: 0.13867338427652665 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 74 out of 93 Training Loss: 0.14650691726555176 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 75 out of 93 Training Loss: 0.16210590822001048 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 76 out of 93 Training Loss: 0.17358563614388056 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 77 out of 93 Training Loss: 0.17962845375140257 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 78 out of 93 Training Loss: 0.19189071615536757 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 79 out of 93 Training Loss: 0.20281319839258738 Test Loss: 0.009639688301831484\n",
      "Epoch: 8 Batch: 80 out of 93 Training Loss: 0.0022682206442172113 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 81 out of 93 Training Loss: 0.013644970368366916 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 82 out of 93 Training Loss: 0.024543918084126193 Test Loss: 0.009577234317971901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Batch: 83 out of 93 Training Loss: 0.034010068859677986 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 84 out of 93 Training Loss: 0.04005789279169984 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 85 out of 93 Training Loss: 0.051920528404187635 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 86 out of 93 Training Loss: 0.06982879421837278 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 87 out of 93 Training Loss: 0.07937869045115657 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 88 out of 93 Training Loss: 0.08758305001236148 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 89 out of 93 Training Loss: 0.09689445222891517 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 90 out of 93 Training Loss: 0.10359158587805577 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 91 out of 93 Training Loss: 0.11224123870186158 Test Loss: 0.009577234317971901\n",
      "Epoch: 8 Batch: 92 out of 93 Training Loss: 0.12247463257215567 Test Loss: 0.009577234317971901\n",
      "Epoch: 9 Batch: 0 out of 93 Training Loss: 0.00010970351035876941 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 1 out of 93 Training Loss: 0.012503352246537645 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 2 out of 93 Training Loss: 0.02060774923051878 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 3 out of 93 Training Loss: 0.03025580406869932 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 4 out of 93 Training Loss: 0.0359673688628821 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 5 out of 93 Training Loss: 0.04337753751064821 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 6 out of 93 Training Loss: 0.04847034835006281 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 7 out of 93 Training Loss: 0.05757190496410414 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 8 out of 93 Training Loss: 0.06535120564262553 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 9 out of 93 Training Loss: 0.07223004544834777 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 10 out of 93 Training Loss: 0.08736756420443936 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 11 out of 93 Training Loss: 0.09636721736501141 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 12 out of 93 Training Loss: 0.10363234835426494 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 13 out of 93 Training Loss: 0.11561939934949561 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 14 out of 93 Training Loss: 0.12564763868908568 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 15 out of 93 Training Loss: 0.13676735548983499 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 16 out of 93 Training Loss: 0.14467182080011054 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 17 out of 93 Training Loss: 0.15418696379969998 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 18 out of 93 Training Loss: 0.16119974548678084 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 19 out of 93 Training Loss: 0.17647406155924483 Test Loss: 0.009629485028033907\n",
      "Epoch: 9 Batch: 20 out of 93 Training Loss: 0.001984844772839256 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 21 out of 93 Training Loss: 0.011068290888094611 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 22 out of 93 Training Loss: 0.01601025913538904 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 23 out of 93 Training Loss: 0.027297554394924352 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 24 out of 93 Training Loss: 0.03545916438730926 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 25 out of 93 Training Loss: 0.05031043276818962 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 26 out of 93 Training Loss: 0.05480306115629644 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 27 out of 93 Training Loss: 0.06531163942697019 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 28 out of 93 Training Loss: 0.07245254595460862 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 29 out of 93 Training Loss: 0.08156758253159493 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 30 out of 93 Training Loss: 0.09129700050386161 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 31 out of 93 Training Loss: 0.10071727898600072 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 32 out of 93 Training Loss: 0.10846467022480935 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 33 out of 93 Training Loss: 0.1160146946522829 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 34 out of 93 Training Loss: 0.1280624418023464 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 35 out of 93 Training Loss: 0.1406805504548785 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 36 out of 93 Training Loss: 0.1518366447153923 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 37 out of 93 Training Loss: 0.1612655630770561 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 38 out of 93 Training Loss: 0.17292995736631125 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 39 out of 93 Training Loss: 0.18227623202534884 Test Loss: 0.010201362892985344\n",
      "Epoch: 9 Batch: 40 out of 93 Training Loss: 0.0020566314984267727 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 41 out of 93 Training Loss: 0.016149734385699607 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 42 out of 93 Training Loss: 0.02187617612668929 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 43 out of 93 Training Loss: 0.031882862799257616 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 44 out of 93 Training Loss: 0.04382493463942466 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 45 out of 93 Training Loss: 0.05356352427640615 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 46 out of 93 Training Loss: 0.06113248575145064 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 47 out of 93 Training Loss: 0.07025924954229651 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 48 out of 93 Training Loss: 0.07986569568181096 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 49 out of 93 Training Loss: 0.09196614451372681 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 50 out of 93 Training Loss: 0.10231321375483332 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 51 out of 93 Training Loss: 0.11427271436566172 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 52 out of 93 Training Loss: 0.12193743396216689 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 53 out of 93 Training Loss: 0.1339013844322568 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 54 out of 93 Training Loss: 0.1449619594555503 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 55 out of 93 Training Loss: 0.1553680605780488 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 56 out of 93 Training Loss: 0.1643674920838481 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 57 out of 93 Training Loss: 0.17394761439079104 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 58 out of 93 Training Loss: 0.18191473815256415 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 59 out of 93 Training Loss: 0.19550426467919407 Test Loss: 0.009756819980049675\n",
      "Epoch: 9 Batch: 60 out of 93 Training Loss: 0.0021559815348758554 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 61 out of 93 Training Loss: 0.01213955464378737 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 62 out of 93 Training Loss: 0.018592970954038127 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 63 out of 93 Training Loss: 0.02897014706001423 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 64 out of 93 Training Loss: 0.03870690106020115 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 65 out of 93 Training Loss: 0.047474761070467456 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 66 out of 93 Training Loss: 0.05790226119176291 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 67 out of 93 Training Loss: 0.06793960886613272 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 68 out of 93 Training Loss: 0.07761124851719282 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 69 out of 93 Training Loss: 0.08657715706691883 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 70 out of 93 Training Loss: 0.09688334776625775 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 71 out of 93 Training Loss: 0.10779902296677969 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 72 out of 93 Training Loss: 0.12079632705614708 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 73 out of 93 Training Loss: 0.13313242937044045 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 74 out of 93 Training Loss: 0.14191678124085805 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 75 out of 93 Training Loss: 0.14957666634187838 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 76 out of 93 Training Loss: 0.16555117680297038 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 77 out of 93 Training Loss: 0.17472862503097197 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 78 out of 93 Training Loss: 0.18460625740305325 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 79 out of 93 Training Loss: 0.19399564484671494 Test Loss: 0.009531110652129759\n",
      "Epoch: 9 Batch: 80 out of 93 Training Loss: 0.002222853337662594 Test Loss: 0.009517410219731655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Batch: 81 out of 93 Training Loss: 0.014950975400643723 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 82 out of 93 Training Loss: 0.028618760031896014 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 83 out of 93 Training Loss: 0.03414855254559435 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 84 out of 93 Training Loss: 0.043164507576512474 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 85 out of 93 Training Loss: 0.05412605320780196 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 86 out of 93 Training Loss: 0.0667425748866669 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 87 out of 93 Training Loss: 0.07709072166799225 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 88 out of 93 Training Loss: 0.09079120969145216 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 89 out of 93 Training Loss: 0.10172845618544735 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 90 out of 93 Training Loss: 0.11641395939259447 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 91 out of 93 Training Loss: 0.12258341638116754 Test Loss: 0.009517410219731655\n",
      "Epoch: 9 Batch: 92 out of 93 Training Loss: 0.13160070785998024 Test Loss: 0.009517410219731655\n",
      "Epoch: 10 Batch: 0 out of 93 Training Loss: 0.0001700703174837174 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 1 out of 93 Training Loss: 0.010389902268446261 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 2 out of 93 Training Loss: 0.024094012205398852 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 3 out of 93 Training Loss: 0.03068863890404182 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 4 out of 93 Training Loss: 0.039222557202822736 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 5 out of 93 Training Loss: 0.04947454399699645 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 6 out of 93 Training Loss: 0.057957327098495534 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 7 out of 93 Training Loss: 0.06601578007722574 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 8 out of 93 Training Loss: 0.07293917947719174 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 9 out of 93 Training Loss: 0.08206022215346175 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 10 out of 93 Training Loss: 0.09291704894313889 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 11 out of 93 Training Loss: 0.09869336580196696 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 12 out of 93 Training Loss: 0.1090964317441948 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 13 out of 93 Training Loss: 0.11663291008482056 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 14 out of 93 Training Loss: 0.12424553335914688 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 15 out of 93 Training Loss: 0.13504112289557535 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 16 out of 93 Training Loss: 0.14580234197238762 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 17 out of 93 Training Loss: 0.15603303128192503 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 18 out of 93 Training Loss: 0.16208466758290607 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 19 out of 93 Training Loss: 0.1709712805346616 Test Loss: 0.00964046282355081\n",
      "Epoch: 10 Batch: 20 out of 93 Training Loss: 0.0019362854722537104 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 21 out of 93 Training Loss: 0.01252344129119292 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 22 out of 93 Training Loss: 0.02162040555361882 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 23 out of 93 Training Loss: 0.03132987273369208 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 24 out of 93 Training Loss: 0.03900516470406428 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 25 out of 93 Training Loss: 0.05043240142498389 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 26 out of 93 Training Loss: 0.058823805949429424 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 27 out of 93 Training Loss: 0.0662638626012601 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 28 out of 93 Training Loss: 0.07599112735573427 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 29 out of 93 Training Loss: 0.08644157619659082 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 30 out of 93 Training Loss: 0.09724003793183938 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 31 out of 93 Training Loss: 0.11299027101580278 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 32 out of 93 Training Loss: 0.12271300116066114 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 33 out of 93 Training Loss: 0.13246811059747116 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 34 out of 93 Training Loss: 0.13995029143501297 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 35 out of 93 Training Loss: 0.14824458974857108 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 36 out of 93 Training Loss: 0.16183635703731553 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 37 out of 93 Training Loss: 0.17601318489272372 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 38 out of 93 Training Loss: 0.18884601369012372 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 39 out of 93 Training Loss: 0.19681482500840203 Test Loss: 0.009943384550173174\n",
      "Epoch: 10 Batch: 40 out of 93 Training Loss: 0.002264869030792971 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 41 out of 93 Training Loss: 0.014292455841083785 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 42 out of 93 Training Loss: 0.01980961263575103 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 43 out of 93 Training Loss: 0.029702869806666633 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 44 out of 93 Training Loss: 0.041886705536623256 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 45 out of 93 Training Loss: 0.051689744558413284 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 46 out of 93 Training Loss: 0.05832976205669786 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 47 out of 93 Training Loss: 0.0682094945611849 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 48 out of 93 Training Loss: 0.07909338372223522 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 49 out of 93 Training Loss: 0.09403910870127823 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 50 out of 93 Training Loss: 0.10559792826347496 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 51 out of 93 Training Loss: 0.1131362455005243 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 52 out of 93 Training Loss: 0.12361965384729649 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 53 out of 93 Training Loss: 0.13161786608793286 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 54 out of 93 Training Loss: 0.14029038288094547 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 55 out of 93 Training Loss: 0.14743160978354958 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 56 out of 93 Training Loss: 0.16208517421432284 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 57 out of 93 Training Loss: 0.17144691534735707 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 58 out of 93 Training Loss: 0.17847570414118913 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 59 out of 93 Training Loss: 0.20076432088785318 Test Loss: 0.00951338800686327\n",
      "Epoch: 10 Batch: 60 out of 93 Training Loss: 0.002254013093331055 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 61 out of 93 Training Loss: 0.009254340203621583 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 62 out of 93 Training Loss: 0.0180248377194192 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 63 out of 93 Training Loss: 0.029055849770226676 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 64 out of 93 Training Loss: 0.0386791295652773 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 65 out of 93 Training Loss: 0.04552345828167458 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 66 out of 93 Training Loss: 0.053370538288870055 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 67 out of 93 Training Loss: 0.06217388168922921 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 68 out of 93 Training Loss: 0.07733789936653634 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 69 out of 93 Training Loss: 0.08548358560554048 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 70 out of 93 Training Loss: 0.0931593134348776 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 71 out of 93 Training Loss: 0.10388518323204776 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 72 out of 93 Training Loss: 0.1118507945259597 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 73 out of 93 Training Loss: 0.12196079281173011 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 74 out of 93 Training Loss: 0.13717170537463685 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 75 out of 93 Training Loss: 0.14934806537828704 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 76 out of 93 Training Loss: 0.15278634930043777 Test Loss: 0.009556075473400679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Batch: 77 out of 93 Training Loss: 0.16397632135092577 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 78 out of 93 Training Loss: 0.17378746660440525 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 79 out of 93 Training Loss: 0.17887675052522978 Test Loss: 0.009556075473400679\n",
      "Epoch: 10 Batch: 80 out of 93 Training Loss: 0.0019896020199518007 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 81 out of 93 Training Loss: 0.01618286865012453 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 82 out of 93 Training Loss: 0.024527384808228473 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 83 out of 93 Training Loss: 0.03466639844881103 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 84 out of 93 Training Loss: 0.044096296561823345 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 85 out of 93 Training Loss: 0.05494833355949924 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 86 out of 93 Training Loss: 0.061371630897686935 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 87 out of 93 Training Loss: 0.07090413747535751 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 88 out of 93 Training Loss: 0.07757015843124793 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 89 out of 93 Training Loss: 0.09536720756621764 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 90 out of 93 Training Loss: 0.1033180946502249 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 91 out of 93 Training Loss: 0.11147602964373038 Test Loss: 0.009507972408424725\n",
      "Epoch: 10 Batch: 92 out of 93 Training Loss: 0.12520835027279065 Test Loss: 0.009507972408424725\n",
      "Epoch: 11 Batch: 0 out of 93 Training Loss: 9.375221786960478e-05 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 1 out of 93 Training Loss: 0.007944990580360736 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 2 out of 93 Training Loss: 0.014565597919206466 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 3 out of 93 Training Loss: 0.023415731897036877 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 4 out of 93 Training Loss: 0.035907019560615865 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 5 out of 93 Training Loss: 0.04029763366786703 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 6 out of 93 Training Loss: 0.05110654018578991 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 7 out of 93 Training Loss: 0.06454320594428048 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 8 out of 93 Training Loss: 0.07433553755043015 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 9 out of 93 Training Loss: 0.08249838806448445 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 10 out of 93 Training Loss: 0.09202610485015378 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 11 out of 93 Training Loss: 0.10026702347902522 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 12 out of 93 Training Loss: 0.110150771846454 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 13 out of 93 Training Loss: 0.12227043106911645 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 14 out of 93 Training Loss: 0.12894719063995347 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 15 out of 93 Training Loss: 0.1359500565355824 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 16 out of 93 Training Loss: 0.1472101404121326 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 17 out of 93 Training Loss: 0.1566224132454203 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 18 out of 93 Training Loss: 0.1644749340390967 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 19 out of 93 Training Loss: 0.17433285869417653 Test Loss: 0.009865192527120764\n",
      "Epoch: 11 Batch: 20 out of 93 Training Loss: 0.0019673647587079373 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 21 out of 93 Training Loss: 0.0149345191423673 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 22 out of 93 Training Loss: 0.020670287147619183 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 23 out of 93 Training Loss: 0.029289456852235253 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 24 out of 93 Training Loss: 0.03726772295729106 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 25 out of 93 Training Loss: 0.0455113675673384 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 26 out of 93 Training Loss: 0.05506267102853244 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 27 out of 93 Training Loss: 0.062136526987411435 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 28 out of 93 Training Loss: 0.06832208095536416 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 29 out of 93 Training Loss: 0.07810064668253844 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 30 out of 93 Training Loss: 0.08953987418160622 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 31 out of 93 Training Loss: 0.10189659784034674 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 32 out of 93 Training Loss: 0.11061092263505642 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 33 out of 93 Training Loss: 0.11798537925303762 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 34 out of 93 Training Loss: 0.12802115485490148 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 35 out of 93 Training Loss: 0.1382391977351982 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 36 out of 93 Training Loss: 0.15235815041185205 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 37 out of 93 Training Loss: 0.1608844216209728 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 38 out of 93 Training Loss: 0.16936851881862466 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 39 out of 93 Training Loss: 0.17552279757753794 Test Loss: 0.009504514661702242\n",
      "Epoch: 11 Batch: 40 out of 93 Training Loss: 0.002000796346499478 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 41 out of 93 Training Loss: 0.013968425540759121 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 42 out of 93 Training Loss: 0.02369486234946445 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 43 out of 93 Training Loss: 0.028563292434944664 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 44 out of 93 Training Loss: 0.03707692839069561 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 45 out of 93 Training Loss: 0.04440727012096242 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 46 out of 93 Training Loss: 0.05382583683220939 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 47 out of 93 Training Loss: 0.06474813280491427 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 48 out of 93 Training Loss: 0.07726136429099635 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 49 out of 93 Training Loss: 0.08946698738126353 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 50 out of 93 Training Loss: 0.09750239198891715 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 51 out of 93 Training Loss: 0.10951674191145495 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 52 out of 93 Training Loss: 0.12046258451281384 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 53 out of 93 Training Loss: 0.12722937457127764 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 54 out of 93 Training Loss: 0.13742005053771927 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 55 out of 93 Training Loss: 0.1467338909567733 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 56 out of 93 Training Loss: 0.15292429897798493 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 57 out of 93 Training Loss: 0.1697683816017051 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 58 out of 93 Training Loss: 0.1809849783019562 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 59 out of 93 Training Loss: 0.19684064455194666 Test Loss: 0.010695299895649607\n",
      "Epoch: 11 Batch: 60 out of 93 Training Loss: 0.0022085883616454064 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 61 out of 93 Training Loss: 0.013345522522831624 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 62 out of 93 Training Loss: 0.023015943072641556 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 63 out of 93 Training Loss: 0.03521445644637412 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 64 out of 93 Training Loss: 0.042261188566590016 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 65 out of 93 Training Loss: 0.05618717120578355 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 66 out of 93 Training Loss: 0.06399805908640689 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 67 out of 93 Training Loss: 0.0733998796864397 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 68 out of 93 Training Loss: 0.09036543657621211 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 69 out of 93 Training Loss: 0.09822730784853763 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 70 out of 93 Training Loss: 0.10879811336835689 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 71 out of 93 Training Loss: 0.1253993815704233 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 72 out of 93 Training Loss: 0.13335521211465662 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 73 out of 93 Training Loss: 0.14718497236808603 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 74 out of 93 Training Loss: 0.1601528643592245 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 75 out of 93 Training Loss: 0.17009384063621824 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 76 out of 93 Training Loss: 0.17647134391208952 Test Loss: 0.009557563218880783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Batch: 77 out of 93 Training Loss: 0.18571970117827719 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 78 out of 93 Training Loss: 0.198653760082925 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 79 out of 93 Training Loss: 0.21027045668652122 Test Loss: 0.009557563218880783\n",
      "Epoch: 11 Batch: 80 out of 93 Training Loss: 0.0023411762800097536 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 81 out of 93 Training Loss: 0.019294551470506197 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 82 out of 93 Training Loss: 0.02310203121900857 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 83 out of 93 Training Loss: 0.03156356433332265 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 84 out of 93 Training Loss: 0.03960581602454484 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 85 out of 93 Training Loss: 0.046886948998275405 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 86 out of 93 Training Loss: 0.059958089272680884 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 87 out of 93 Training Loss: 0.06832836395055354 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 88 out of 93 Training Loss: 0.08081879666000903 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 89 out of 93 Training Loss: 0.09128269193619788 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 90 out of 93 Training Loss: 0.09918194835931361 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 91 out of 93 Training Loss: 0.10757556384355128 Test Loss: 0.009465552781793203\n",
      "Epoch: 11 Batch: 92 out of 93 Training Loss: 0.11704535609036983 Test Loss: 0.009465552781793203\n",
      "Epoch: 12 Batch: 0 out of 93 Training Loss: 0.00013398961915123847 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 1 out of 93 Training Loss: 0.007948690334395055 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 2 out of 93 Training Loss: 0.014209192486539964 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 3 out of 93 Training Loss: 0.023988246226743344 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 4 out of 93 Training Loss: 0.03151865420682776 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 5 out of 93 Training Loss: 0.04215173540456641 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 6 out of 93 Training Loss: 0.050997390652135495 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 7 out of 93 Training Loss: 0.06374952686770308 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 8 out of 93 Training Loss: 0.07106503775163042 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 9 out of 93 Training Loss: 0.07826431119634258 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 10 out of 93 Training Loss: 0.09137638123525726 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 11 out of 93 Training Loss: 0.10211248161090958 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 12 out of 93 Training Loss: 0.11087350664480078 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 13 out of 93 Training Loss: 0.1188732974772011 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 14 out of 93 Training Loss: 0.13012733800156462 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 15 out of 93 Training Loss: 0.13667716109944927 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 16 out of 93 Training Loss: 0.1492831690777694 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 17 out of 93 Training Loss: 0.16429522936983454 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 18 out of 93 Training Loss: 0.17341573281033384 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 19 out of 93 Training Loss: 0.18217809708608734 Test Loss: 0.009730273180387237\n",
      "Epoch: 12 Batch: 20 out of 93 Training Loss: 0.0020467672042194605 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 21 out of 93 Training Loss: 0.01393905612748301 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 22 out of 93 Training Loss: 0.022645929410273193 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 23 out of 93 Training Loss: 0.0354629890538398 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 24 out of 93 Training Loss: 0.04599233022999203 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 25 out of 93 Training Loss: 0.055462046108359454 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 26 out of 93 Training Loss: 0.07045819475423729 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 27 out of 93 Training Loss: 0.0820887410558406 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 28 out of 93 Training Loss: 0.09092561579417621 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 29 out of 93 Training Loss: 0.10099747642051612 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 30 out of 93 Training Loss: 0.11306040785383617 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 31 out of 93 Training Loss: 0.11906731692251002 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 32 out of 93 Training Loss: 0.12747492645975386 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 33 out of 93 Training Loss: 0.1387601737409476 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 34 out of 93 Training Loss: 0.14660830241736209 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 35 out of 93 Training Loss: 0.15791780320104395 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 36 out of 93 Training Loss: 0.16812795777913367 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 37 out of 93 Training Loss: 0.1764124036311749 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 38 out of 93 Training Loss: 0.18651714366968905 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 39 out of 93 Training Loss: 0.20261460174914633 Test Loss: 0.00959338912401687\n",
      "Epoch: 12 Batch: 40 out of 93 Training Loss: 0.00229743138127071 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 41 out of 93 Training Loss: 0.013786455767080609 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 42 out of 93 Training Loss: 0.024548403138146227 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 43 out of 93 Training Loss: 0.03348084461115343 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 44 out of 93 Training Loss: 0.04231782924555284 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 45 out of 93 Training Loss: 0.04846557850189549 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 46 out of 93 Training Loss: 0.0573393326828633 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 47 out of 93 Training Loss: 0.0669425154680763 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 48 out of 93 Training Loss: 0.07966608802028043 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 49 out of 93 Training Loss: 0.09012755999513013 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 50 out of 93 Training Loss: 0.10069405256010634 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 51 out of 93 Training Loss: 0.10984043186701876 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 52 out of 93 Training Loss: 0.12023455804385287 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 53 out of 93 Training Loss: 0.1313049343059216 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 54 out of 93 Training Loss: 0.13937792150475126 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 55 out of 93 Training Loss: 0.15130758626557927 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 56 out of 93 Training Loss: 0.1621753765846048 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 57 out of 93 Training Loss: 0.17155501214958768 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 58 out of 93 Training Loss: 0.18342299448497634 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 59 out of 93 Training Loss: 0.1884024168009315 Test Loss: 0.009624521925368092\n",
      "Epoch: 12 Batch: 60 out of 93 Training Loss: 0.00213436331381481 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 61 out of 93 Training Loss: 0.011624970620176008 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 62 out of 93 Training Loss: 0.018339727220734764 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 63 out of 93 Training Loss: 0.029764495005509067 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 64 out of 93 Training Loss: 0.0375515722275285 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 65 out of 93 Training Loss: 0.043363663980415273 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 66 out of 93 Training Loss: 0.051187326812794615 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 67 out of 93 Training Loss: 0.05796665480141919 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 68 out of 93 Training Loss: 0.06558756451835315 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 69 out of 93 Training Loss: 0.07538789618541877 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 70 out of 93 Training Loss: 0.08556894413908403 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 71 out of 93 Training Loss: 0.09632526445915382 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 72 out of 93 Training Loss: 0.10343550206874888 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 73 out of 93 Training Loss: 0.11157518407111447 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 74 out of 93 Training Loss: 0.11981646870975773 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 75 out of 93 Training Loss: 0.1368429601729421 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 76 out of 93 Training Loss: 0.14498787900214474 Test Loss: 0.010011413473297249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Batch: 77 out of 93 Training Loss: 0.15682794028674643 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 78 out of 93 Training Loss: 0.1656318592444567 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 79 out of 93 Training Loss: 0.1767552445248274 Test Loss: 0.010011413473297249\n",
      "Epoch: 12 Batch: 80 out of 93 Training Loss: 0.001987276695525035 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 81 out of 93 Training Loss: 0.009420867174720153 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 82 out of 93 Training Loss: 0.022099613308762894 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 83 out of 93 Training Loss: 0.030722160920357094 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 84 out of 93 Training Loss: 0.036605402923976765 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 85 out of 93 Training Loss: 0.044588911443805085 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 86 out of 93 Training Loss: 0.053263216376160966 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 87 out of 93 Training Loss: 0.06631050734207569 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 88 out of 93 Training Loss: 0.07148280842587887 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 89 out of 93 Training Loss: 0.08262422597393929 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 90 out of 93 Training Loss: 0.09172381563470303 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 91 out of 93 Training Loss: 0.09678570563331781 Test Loss: 0.009548681407150898\n",
      "Epoch: 12 Batch: 92 out of 93 Training Loss: 0.10185301639497815 Test Loss: 0.009548681407150898\n",
      "Epoch: 13 Batch: 0 out of 93 Training Loss: 9.382555201168983e-05 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 1 out of 93 Training Loss: 0.012098288554097376 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 2 out of 93 Training Loss: 0.018142405822272265 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 3 out of 93 Training Loss: 0.023023938685054743 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 4 out of 93 Training Loss: 0.03196199696450945 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 5 out of 93 Training Loss: 0.04273539587795254 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 6 out of 93 Training Loss: 0.057911495603020155 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 7 out of 93 Training Loss: 0.06807201255052801 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 8 out of 93 Training Loss: 0.08392357911854502 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 9 out of 93 Training Loss: 0.09029928168221828 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 10 out of 93 Training Loss: 0.10037699101432677 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 11 out of 93 Training Loss: 0.11024649573429939 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 12 out of 93 Training Loss: 0.11484809957384583 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 13 out of 93 Training Loss: 0.12345986712663885 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 14 out of 93 Training Loss: 0.13376149807065246 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 15 out of 93 Training Loss: 0.1466658831754279 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 16 out of 93 Training Loss: 0.15480602525114534 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 17 out of 93 Training Loss: 0.16536249771654127 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 18 out of 93 Training Loss: 0.17573054686128614 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 19 out of 93 Training Loss: 0.1844623982155275 Test Loss: 0.009497251784936949\n",
      "Epoch: 13 Batch: 20 out of 93 Training Loss: 0.0020916546857100145 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 21 out of 93 Training Loss: 0.010929692645941319 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 22 out of 93 Training Loss: 0.018553457797889056 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 23 out of 93 Training Loss: 0.025368461494718136 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 24 out of 93 Training Loss: 0.0326453495001536 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 25 out of 93 Training Loss: 0.041362327632878365 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 26 out of 93 Training Loss: 0.05248737066743189 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 27 out of 93 Training Loss: 0.059135120937053504 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 28 out of 93 Training Loss: 0.06806257424635107 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 29 out of 93 Training Loss: 0.08051629049462491 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 30 out of 93 Training Loss: 0.09143410494310075 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 31 out of 93 Training Loss: 0.09999131841344053 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 32 out of 93 Training Loss: 0.10756816632804686 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 33 out of 93 Training Loss: 0.12043956398722941 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 34 out of 93 Training Loss: 0.12782260413971955 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 35 out of 93 Training Loss: 0.14247683167216593 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 36 out of 93 Training Loss: 0.1491225623687845 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 37 out of 93 Training Loss: 0.16312224801388794 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 38 out of 93 Training Loss: 0.16890422824529225 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 39 out of 93 Training Loss: 0.1765709515823167 Test Loss: 0.00969479347325184\n",
      "Epoch: 13 Batch: 40 out of 93 Training Loss: 0.002034924960599195 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 41 out of 93 Training Loss: 0.010642210057840596 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 42 out of 93 Training Loss: 0.021537471598849545 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 43 out of 93 Training Loss: 0.03466633990304256 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 44 out of 93 Training Loss: 0.04797622471945072 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 45 out of 93 Training Loss: 0.060306741892264135 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 46 out of 93 Training Loss: 0.07159935779766823 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 47 out of 93 Training Loss: 0.07863169260220314 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 48 out of 93 Training Loss: 0.08967920582638765 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 49 out of 93 Training Loss: 0.10199367258595014 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 50 out of 93 Training Loss: 0.10991671126233125 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 51 out of 93 Training Loss: 0.11628944802926565 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 52 out of 93 Training Loss: 0.12609671618686702 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 53 out of 93 Training Loss: 0.13966727726058747 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 54 out of 93 Training Loss: 0.14630284722791698 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 55 out of 93 Training Loss: 0.15564744271235254 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 56 out of 93 Training Loss: 0.16850664377109792 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 57 out of 93 Training Loss: 0.17620823908494498 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 58 out of 93 Training Loss: 0.18496488410876777 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 59 out of 93 Training Loss: 0.19284660186515834 Test Loss: 0.009578087959777225\n",
      "Epoch: 13 Batch: 60 out of 93 Training Loss: 0.0022213886726176637 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 61 out of 93 Training Loss: 0.014656413276818981 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 62 out of 93 Training Loss: 0.020723099750605812 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 63 out of 93 Training Loss: 0.029539327067462196 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 64 out of 93 Training Loss: 0.03592916291686653 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 65 out of 93 Training Loss: 0.044033595291105496 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 66 out of 93 Training Loss: 0.05377217840882896 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 67 out of 93 Training Loss: 0.05901711147758125 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 68 out of 93 Training Loss: 0.06898992501678823 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 69 out of 93 Training Loss: 0.08499968789997457 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 70 out of 93 Training Loss: 0.09138509903582691 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 71 out of 93 Training Loss: 0.09815364789697288 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 72 out of 93 Training Loss: 0.110641092923609 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 73 out of 93 Training Loss: 0.12010998178753017 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 74 out of 93 Training Loss: 0.1265276865336573 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 75 out of 93 Training Loss: 0.14222299766275048 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 76 out of 93 Training Loss: 0.15039864734294772 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 77 out of 93 Training Loss: 0.16095897548380256 Test Loss: 0.009628118989481167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Batch: 78 out of 93 Training Loss: 0.17371902421596408 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 79 out of 93 Training Loss: 0.18504906811299682 Test Loss: 0.009628118989481167\n",
      "Epoch: 13 Batch: 80 out of 93 Training Loss: 0.002130423361306844 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 81 out of 93 Training Loss: 0.011804028102999388 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 82 out of 93 Training Loss: 0.019932121658569037 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 83 out of 93 Training Loss: 0.02906839611554688 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 84 out of 93 Training Loss: 0.033834169307594954 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 85 out of 93 Training Loss: 0.04602638321543282 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 86 out of 93 Training Loss: 0.05638135938519782 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 87 out of 93 Training Loss: 0.06766289813989942 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 88 out of 93 Training Loss: 0.07347225878933614 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 89 out of 93 Training Loss: 0.0842841008360142 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 90 out of 93 Training Loss: 0.09166365225413983 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 91 out of 93 Training Loss: 0.10492377091745084 Test Loss: 0.009517677255313505\n",
      "Epoch: 13 Batch: 92 out of 93 Training Loss: 0.11089583734596317 Test Loss: 0.009517677255313505\n",
      "Epoch: 14 Batch: 0 out of 93 Training Loss: 5.819035133206716e-05 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 1 out of 93 Training Loss: 0.008098712576533197 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 2 out of 93 Training Loss: 0.015356110328788398 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 3 out of 93 Training Loss: 0.02618738907759869 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 4 out of 93 Training Loss: 0.03682861934786522 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 5 out of 93 Training Loss: 0.04356118050774419 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 6 out of 93 Training Loss: 0.054553312635029196 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 7 out of 93 Training Loss: 0.062077326154316305 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 8 out of 93 Training Loss: 0.06964180563947045 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 9 out of 93 Training Loss: 0.0790517659988054 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 10 out of 93 Training Loss: 0.08567025271614873 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 11 out of 93 Training Loss: 0.0977658176924833 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 12 out of 93 Training Loss: 0.11017474756946646 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 13 out of 93 Training Loss: 0.11901077666690432 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 14 out of 93 Training Loss: 0.12574873670374834 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 15 out of 93 Training Loss: 0.13116573580911242 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 16 out of 93 Training Loss: 0.14175064527800166 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 17 out of 93 Training Loss: 0.15083278009107196 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 18 out of 93 Training Loss: 0.16226988324334704 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 19 out of 93 Training Loss: 0.17210621797850215 Test Loss: 0.009459278419275175\n",
      "Epoch: 14 Batch: 20 out of 93 Training Loss: 0.0019379660067977533 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 21 out of 93 Training Loss: 0.012711697402399979 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 22 out of 93 Training Loss: 0.0209288682369413 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 23 out of 93 Training Loss: 0.02980357702354809 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 24 out of 93 Training Loss: 0.03710238091180941 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 25 out of 93 Training Loss: 0.04528695825020691 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 26 out of 93 Training Loss: 0.05453713718335053 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 27 out of 93 Training Loss: 0.06256831358651539 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 28 out of 93 Training Loss: 0.06954582003409883 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 29 out of 93 Training Loss: 0.08100430821533224 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 30 out of 93 Training Loss: 0.09400149309541007 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 31 out of 93 Training Loss: 0.10729323761070987 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 32 out of 93 Training Loss: 0.11726688095833084 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 33 out of 93 Training Loss: 0.12640657523180265 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 34 out of 93 Training Loss: 0.1351728743454339 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 35 out of 93 Training Loss: 0.1483596494933488 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 36 out of 93 Training Loss: 0.15511749699245114 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 37 out of 93 Training Loss: 0.16131865154485125 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 38 out of 93 Training Loss: 0.173386759269041 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 39 out of 93 Training Loss: 0.182239527958435 Test Loss: 0.009555688762867992\n",
      "Epoch: 14 Batch: 40 out of 93 Training Loss: 0.0020941362020768808 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 41 out of 93 Training Loss: 0.007887609885374515 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 42 out of 93 Training Loss: 0.013328981236855 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 43 out of 93 Training Loss: 0.027351103374103514 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 44 out of 93 Training Loss: 0.035551897832492796 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 45 out of 93 Training Loss: 0.046915046521762815 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 46 out of 93 Training Loss: 0.05955727736030003 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 47 out of 93 Training Loss: 0.06917508004728934 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 48 out of 93 Training Loss: 0.07732640544448277 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 49 out of 93 Training Loss: 0.08537862508122107 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 50 out of 93 Training Loss: 0.10251426382486006 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 51 out of 93 Training Loss: 0.11026979482297083 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 52 out of 93 Training Loss: 0.12145988701407096 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 53 out of 93 Training Loss: 0.13281732669595858 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 54 out of 93 Training Loss: 0.14097234292451521 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 55 out of 93 Training Loss: 0.1477911068362393 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 56 out of 93 Training Loss: 0.15950964542601487 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 57 out of 93 Training Loss: 0.16532098373893878 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 58 out of 93 Training Loss: 0.17336103005830428 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 59 out of 93 Training Loss: 0.18258508911852023 Test Loss: 0.009513256224718962\n",
      "Epoch: 14 Batch: 60 out of 93 Training Loss: 0.0020954488809202714 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 61 out of 93 Training Loss: 0.00982300853682504 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 62 out of 93 Training Loss: 0.02588630384160028 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 63 out of 93 Training Loss: 0.034181033357510424 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 64 out of 93 Training Loss: 0.040578016642013645 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 65 out of 93 Training Loss: 0.05475863311765537 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 66 out of 93 Training Loss: 0.06314765904424534 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 67 out of 93 Training Loss: 0.07492404673574314 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 68 out of 93 Training Loss: 0.08769697100249872 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 69 out of 93 Training Loss: 0.1004662230643426 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 70 out of 93 Training Loss: 0.10887556486157522 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 71 out of 93 Training Loss: 0.1228483129325305 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 72 out of 93 Training Loss: 0.13121746629802333 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 73 out of 93 Training Loss: 0.13899439722625362 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 74 out of 93 Training Loss: 0.14893189967004405 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 75 out of 93 Training Loss: 0.15702272262750017 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 76 out of 93 Training Loss: 0.16718145981756793 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 77 out of 93 Training Loss: 0.17689281478849994 Test Loss: 0.009486186191100966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Batch: 78 out of 93 Training Loss: 0.186286026242653 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 79 out of 93 Training Loss: 0.19940198362199413 Test Loss: 0.009486186191100966\n",
      "Epoch: 14 Batch: 80 out of 93 Training Loss: 0.0023610227331068295 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 81 out of 93 Training Loss: 0.0075971538722064275 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 82 out of 93 Training Loss: 0.01646368937189839 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 83 out of 93 Training Loss: 0.02910848321016095 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 84 out of 93 Training Loss: 0.03788116281475089 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 85 out of 93 Training Loss: 0.04806193602825663 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 86 out of 93 Training Loss: 0.054472757834896066 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 87 out of 93 Training Loss: 0.06736755776773355 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 88 out of 93 Training Loss: 0.07364188726614378 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 89 out of 93 Training Loss: 0.08462299253414533 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 90 out of 93 Training Loss: 0.09323532260577343 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 91 out of 93 Training Loss: 0.1020713330767598 Test Loss: 0.009522587653588165\n",
      "Epoch: 14 Batch: 92 out of 93 Training Loss: 0.1058573544546511 Test Loss: 0.009522587653588165\n",
      "Epoch: 15 Batch: 0 out of 93 Training Loss: 0.00011372363935875636 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 1 out of 93 Training Loss: 0.011885601866950271 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 2 out of 93 Training Loss: 0.018601586881984947 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 3 out of 93 Training Loss: 0.02734694613884854 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 4 out of 93 Training Loss: 0.03759934644024539 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 5 out of 93 Training Loss: 0.049500600274612186 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 6 out of 93 Training Loss: 0.060803468178917644 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 7 out of 93 Training Loss: 0.06859376917402911 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 8 out of 93 Training Loss: 0.0774427846334474 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 9 out of 93 Training Loss: 0.09140621947865653 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 10 out of 93 Training Loss: 0.09921587242554593 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 11 out of 93 Training Loss: 0.10884081742535996 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 12 out of 93 Training Loss: 0.1264139989651339 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 13 out of 93 Training Loss: 0.13940973843257592 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 14 out of 93 Training Loss: 0.14566800541054176 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 15 out of 93 Training Loss: 0.1523037649020152 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 16 out of 93 Training Loss: 0.1600909860133724 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 17 out of 93 Training Loss: 0.16764979793499873 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 18 out of 93 Training Loss: 0.17824138312410281 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 19 out of 93 Training Loss: 0.18522969850148724 Test Loss: 0.009495346468280663\n",
      "Epoch: 15 Batch: 20 out of 93 Training Loss: 0.0020859654104352816 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 21 out of 93 Training Loss: 0.014609353900516115 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 22 out of 93 Training Loss: 0.024624256700837694 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 23 out of 93 Training Loss: 0.030957832284832083 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 24 out of 93 Training Loss: 0.03938361751344212 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 25 out of 93 Training Loss: 0.046998773985171396 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 26 out of 93 Training Loss: 0.05477953777145751 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 27 out of 93 Training Loss: 0.06222574953418382 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 28 out of 93 Training Loss: 0.07322458439003833 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 29 out of 93 Training Loss: 0.08296576310377725 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 30 out of 93 Training Loss: 0.0908835137305797 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 31 out of 93 Training Loss: 0.10385349188428529 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 32 out of 93 Training Loss: 0.1135716343341411 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 33 out of 93 Training Loss: 0.1223669776735843 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 34 out of 93 Training Loss: 0.1274846366649509 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 35 out of 93 Training Loss: 0.138681855336392 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 36 out of 93 Training Loss: 0.14557598233651525 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 37 out of 93 Training Loss: 0.15285517815929062 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 38 out of 93 Training Loss: 0.16496871859263784 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 39 out of 93 Training Loss: 0.17703398578297502 Test Loss: 0.009544107639654116\n",
      "Epoch: 15 Batch: 40 out of 93 Training Loss: 0.0020056168919749804 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 41 out of 93 Training Loss: 0.012695663366312558 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 42 out of 93 Training Loss: 0.021082774493689116 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 43 out of 93 Training Loss: 0.03411545207708555 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 44 out of 93 Training Loss: 0.044437192138130244 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 45 out of 93 Training Loss: 0.05546829545855003 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 46 out of 93 Training Loss: 0.0662752396798082 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 47 out of 93 Training Loss: 0.07825820030629116 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 48 out of 93 Training Loss: 0.08682149158775287 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 49 out of 93 Training Loss: 0.09570931339621025 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 50 out of 93 Training Loss: 0.10614628227412182 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 51 out of 93 Training Loss: 0.11606121132134872 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 52 out of 93 Training Loss: 0.12192427715271192 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 53 out of 93 Training Loss: 0.1300600857844897 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 54 out of 93 Training Loss: 0.13834433978408056 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 55 out of 93 Training Loss: 0.1525932622513123 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 56 out of 93 Training Loss: 0.1636156157961436 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 57 out of 93 Training Loss: 0.17529687261193472 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 58 out of 93 Training Loss: 0.18516243224948126 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 59 out of 93 Training Loss: 0.197222308013434 Test Loss: 0.009453031996434385\n",
      "Epoch: 15 Batch: 60 out of 93 Training Loss: 0.0022478848469417024 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 61 out of 93 Training Loss: 0.010701206490115397 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 62 out of 93 Training Loss: 0.020082629263118976 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 63 out of 93 Training Loss: 0.030447605043367618 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 64 out of 93 Training Loss: 0.03536410161824631 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 65 out of 93 Training Loss: 0.04321112202079701 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 66 out of 93 Training Loss: 0.06178337053926396 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 67 out of 93 Training Loss: 0.06931213939161705 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 68 out of 93 Training Loss: 0.08144786299736904 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 69 out of 93 Training Loss: 0.09414116570325302 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 70 out of 93 Training Loss: 0.10344651618571686 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 71 out of 93 Training Loss: 0.11966331766517567 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 72 out of 93 Training Loss: 0.12741439368175078 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 73 out of 93 Training Loss: 0.1366976289783578 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 74 out of 93 Training Loss: 0.14135232705222178 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 75 out of 93 Training Loss: 0.15056491780625392 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 76 out of 93 Training Loss: 0.1595114669536214 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 77 out of 93 Training Loss: 0.1645778282319169 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 78 out of 93 Training Loss: 0.17246803592489768 Test Loss: 0.009585066783157263\n",
      "Epoch: 15 Batch: 79 out of 93 Training Loss: 0.17887305233226825 Test Loss: 0.009585066783157263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Batch: 80 out of 93 Training Loss: 0.002039498013391141 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 81 out of 93 Training Loss: 0.010682829950883035 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 82 out of 93 Training Loss: 0.018809682984783298 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 83 out of 93 Training Loss: 0.03135222878022397 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 84 out of 93 Training Loss: 0.04091075371666158 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 85 out of 93 Training Loss: 0.04872581857903207 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 86 out of 93 Training Loss: 0.05740174028737748 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 87 out of 93 Training Loss: 0.07164137560589517 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 88 out of 93 Training Loss: 0.08052092540724481 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 89 out of 93 Training Loss: 0.08563932386873568 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 90 out of 93 Training Loss: 0.09663767536817397 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 91 out of 93 Training Loss: 0.10611921718059386 Test Loss: 0.009534647180275484\n",
      "Epoch: 15 Batch: 92 out of 93 Training Loss: 0.11538795737264956 Test Loss: 0.009534647180275484\n",
      "Epoch: 16 Batch: 0 out of 93 Training Loss: 6.087256535407036e-05 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 1 out of 93 Training Loss: 0.008469355683172903 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 2 out of 93 Training Loss: 0.01910558195724603 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 3 out of 93 Training Loss: 0.026261849253768885 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 4 out of 93 Training Loss: 0.03456311851679798 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 5 out of 93 Training Loss: 0.04454739384293076 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 6 out of 93 Training Loss: 0.05553748637377735 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 7 out of 93 Training Loss: 0.06447111993968006 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 8 out of 93 Training Loss: 0.06981735063656684 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 9 out of 93 Training Loss: 0.08357237225338336 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 10 out of 93 Training Loss: 0.09025227131261941 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 11 out of 93 Training Loss: 0.09984306285097715 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 12 out of 93 Training Loss: 0.10948126429631826 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 13 out of 93 Training Loss: 0.12437353322222348 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 14 out of 93 Training Loss: 0.13068915849491475 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 15 out of 93 Training Loss: 0.1399162555293691 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 16 out of 93 Training Loss: 0.1477634845481765 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 17 out of 93 Training Loss: 0.15868312159493086 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 18 out of 93 Training Loss: 0.16579767029672382 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 19 out of 93 Training Loss: 0.17077519969954608 Test Loss: 0.009620842185210098\n",
      "Epoch: 16 Batch: 20 out of 93 Training Loss: 0.0019189663203268408 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 21 out of 93 Training Loss: 0.013261350241473234 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 22 out of 93 Training Loss: 0.017275873010030307 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 23 out of 93 Training Loss: 0.02801021970527462 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 24 out of 93 Training Loss: 0.040373238173296966 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 25 out of 93 Training Loss: 0.045241160687974014 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 26 out of 93 Training Loss: 0.05725291687714151 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 27 out of 93 Training Loss: 0.07087513655143074 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 28 out of 93 Training Loss: 0.08091708868460945 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 29 out of 93 Training Loss: 0.09357879347818188 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 30 out of 93 Training Loss: 0.10275117158608726 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 31 out of 93 Training Loss: 0.11344794965999416 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 32 out of 93 Training Loss: 0.12432170882540039 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 33 out of 93 Training Loss: 0.13173841163235 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 34 out of 93 Training Loss: 0.14533104598241142 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 35 out of 93 Training Loss: 0.15452030338065914 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 36 out of 93 Training Loss: 0.1603905618162661 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 37 out of 93 Training Loss: 0.16987494416253857 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 38 out of 93 Training Loss: 0.17633461244123272 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 39 out of 93 Training Loss: 0.18782274368869356 Test Loss: 0.009537444280629808\n",
      "Epoch: 16 Batch: 40 out of 93 Training Loss: 0.002115930775050097 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 41 out of 93 Training Loss: 0.018538355240467957 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 42 out of 93 Training Loss: 0.0301692014279403 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 43 out of 93 Training Loss: 0.04702695214176553 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 44 out of 93 Training Loss: 0.05562004202032464 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 45 out of 93 Training Loss: 0.06743320764029401 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 46 out of 93 Training Loss: 0.07673622050309556 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 47 out of 93 Training Loss: 0.08425221906030553 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 48 out of 93 Training Loss: 0.09306465589428323 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 49 out of 93 Training Loss: 0.10102123615318911 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 50 out of 93 Training Loss: 0.11375386022949593 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 51 out of 93 Training Loss: 0.12152854946428912 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 52 out of 93 Training Loss: 0.13023877550834792 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 53 out of 93 Training Loss: 0.13563991884687918 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 54 out of 93 Training Loss: 0.14592886704007166 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 55 out of 93 Training Loss: 0.15066537016058343 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 56 out of 93 Training Loss: 0.15945154090398925 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 57 out of 93 Training Loss: 0.16842080221498387 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 58 out of 93 Training Loss: 0.1773311638923921 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 59 out of 93 Training Loss: 0.18761168920421975 Test Loss: 0.009447181047025051\n",
      "Epoch: 16 Batch: 60 out of 93 Training Loss: 0.00211250250628178 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 61 out of 93 Training Loss: 0.011424703797604487 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 62 out of 93 Training Loss: 0.024055588333215163 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 63 out of 93 Training Loss: 0.03097564195683186 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 64 out of 93 Training Loss: 0.04036972699155991 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 65 out of 93 Training Loss: 0.04953219117065852 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 66 out of 93 Training Loss: 0.057081142714526106 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 67 out of 93 Training Loss: 0.06461630416503136 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 68 out of 93 Training Loss: 0.07463192315241282 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 69 out of 93 Training Loss: 0.08330109090944712 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 70 out of 93 Training Loss: 0.09603787892898505 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 71 out of 93 Training Loss: 0.10282867056718056 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 72 out of 93 Training Loss: 0.11380021115234082 Test Loss: 0.009498044213449413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Batch: 73 out of 93 Training Loss: 0.12559505206933205 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 74 out of 93 Training Loss: 0.13312441417416518 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 75 out of 93 Training Loss: 0.14127574795326178 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 76 out of 93 Training Loss: 0.15411328458627646 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 77 out of 93 Training Loss: 0.16465297834416812 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 78 out of 93 Training Loss: 0.17761728288074916 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 79 out of 93 Training Loss: 0.18756184288701003 Test Loss: 0.009498044213449413\n",
      "Epoch: 16 Batch: 80 out of 93 Training Loss: 0.002120994745386179 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 81 out of 93 Training Loss: 0.011700719377589758 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 82 out of 93 Training Loss: 0.026871738738012844 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 83 out of 93 Training Loss: 0.03795158561934286 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 84 out of 93 Training Loss: 0.04694134999681764 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 85 out of 93 Training Loss: 0.055205919562471444 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 86 out of 93 Training Loss: 0.0640483766483193 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 87 out of 93 Training Loss: 0.07163726563249284 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 88 out of 93 Training Loss: 0.08278490069006139 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 89 out of 93 Training Loss: 0.08751380654667074 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 90 out of 93 Training Loss: 0.09827077786122018 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 91 out of 93 Training Loss: 0.10473533698041493 Test Loss: 0.009492371104318987\n",
      "Epoch: 16 Batch: 92 out of 93 Training Loss: 0.12946671412010247 Test Loss: 0.009492371104318987\n",
      "Epoch: 17 Batch: 0 out of 93 Training Loss: 8.691915421075719e-05 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 1 out of 93 Training Loss: 0.012889582413418959 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 2 out of 93 Training Loss: 0.02027639255969114 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 3 out of 93 Training Loss: 0.027444598014636706 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 4 out of 93 Training Loss: 0.03442031073494144 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 5 out of 93 Training Loss: 0.0437545378051538 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 6 out of 93 Training Loss: 0.05824476781411357 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 7 out of 93 Training Loss: 0.07021036672516055 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 8 out of 93 Training Loss: 0.08045463937444873 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 9 out of 93 Training Loss: 0.09016271236462779 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 10 out of 93 Training Loss: 0.09978366850985475 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 11 out of 93 Training Loss: 0.10557573869003244 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 12 out of 93 Training Loss: 0.11844615007860847 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 13 out of 93 Training Loss: 0.12903694364352414 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 14 out of 93 Training Loss: 0.14095641601009556 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 15 out of 93 Training Loss: 0.1482252043030996 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 16 out of 93 Training Loss: 0.1570743557356138 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 17 out of 93 Training Loss: 0.17242899279220292 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 18 out of 93 Training Loss: 0.183733591093852 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 19 out of 93 Training Loss: 0.19525008297635504 Test Loss: 0.009662746578793634\n",
      "Epoch: 17 Batch: 20 out of 93 Training Loss: 0.002176781087144236 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 21 out of 93 Training Loss: 0.011548713683112482 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 22 out of 93 Training Loss: 0.018286305381878237 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 23 out of 93 Training Loss: 0.025522693893833975 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 24 out of 93 Training Loss: 0.03577436683230005 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 25 out of 93 Training Loss: 0.04408993934743009 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 26 out of 93 Training Loss: 0.05603128475598894 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 27 out of 93 Training Loss: 0.062426401204391344 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 28 out of 93 Training Loss: 0.06933167846403919 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 29 out of 93 Training Loss: 0.0760857378085693 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 30 out of 93 Training Loss: 0.08513235570393406 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 31 out of 93 Training Loss: 0.09586260457985006 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 32 out of 93 Training Loss: 0.10443180696569286 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 33 out of 93 Training Loss: 0.11322142639659248 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 34 out of 93 Training Loss: 0.12046136185057364 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 35 out of 93 Training Loss: 0.13066378924526179 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 36 out of 93 Training Loss: 0.1391321377781769 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 37 out of 93 Training Loss: 0.1454354705242058 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 38 out of 93 Training Loss: 0.15577477905429804 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 39 out of 93 Training Loss: 0.17108329142905676 Test Loss: 0.00941946217790246\n",
      "Epoch: 17 Batch: 40 out of 93 Training Loss: 0.0020165811282308087 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 41 out of 93 Training Loss: 0.01959697669266772 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 42 out of 93 Training Loss: 0.02879258198558402 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 43 out of 93 Training Loss: 0.03951317584036898 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 44 out of 93 Training Loss: 0.057760618160358185 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 45 out of 93 Training Loss: 0.06849348125874113 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 46 out of 93 Training Loss: 0.08052725018828225 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 47 out of 93 Training Loss: 0.08917008573084187 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 48 out of 93 Training Loss: 0.09549569245322656 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 49 out of 93 Training Loss: 0.10337892081483316 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 50 out of 93 Training Loss: 0.11298489570512485 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 51 out of 93 Training Loss: 0.12874972820176792 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 52 out of 93 Training Loss: 0.13522602151974153 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 53 out of 93 Training Loss: 0.1443671053264947 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 54 out of 93 Training Loss: 0.1520132473659427 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 55 out of 93 Training Loss: 0.1604011016887338 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 56 out of 93 Training Loss: 0.16949564544378828 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 57 out of 93 Training Loss: 0.17940318630903077 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 58 out of 93 Training Loss: 0.187659660818866 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 59 out of 93 Training Loss: 0.1947845648963661 Test Loss: 0.009544133928350428\n",
      "Epoch: 17 Batch: 60 out of 93 Training Loss: 0.0021433239422064647 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 61 out of 93 Training Loss: 0.009828884209625088 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 62 out of 93 Training Loss: 0.019707107040218674 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 63 out of 93 Training Loss: 0.03084916440688785 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 64 out of 93 Training Loss: 0.039736321251026954 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 65 out of 93 Training Loss: 0.052154021392695274 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 66 out of 93 Training Loss: 0.0682381745978516 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 67 out of 93 Training Loss: 0.07586193323054131 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 68 out of 93 Training Loss: 0.09299555659212884 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 69 out of 93 Training Loss: 0.09880363993265447 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 70 out of 93 Training Loss: 0.10547857690521296 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 71 out of 93 Training Loss: 0.11369300592609462 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 72 out of 93 Training Loss: 0.12428852874704417 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 73 out of 93 Training Loss: 0.13174579484306986 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 74 out of 93 Training Loss: 0.1396687813653868 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 75 out of 93 Training Loss: 0.14864140288823063 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 76 out of 93 Training Loss: 0.15746683726005967 Test Loss: 0.009416729169474407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Batch: 77 out of 93 Training Loss: 0.16644767770462449 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 78 out of 93 Training Loss: 0.17473141109489615 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 79 out of 93 Training Loss: 0.18457614077173884 Test Loss: 0.009416729169474407\n",
      "Epoch: 17 Batch: 80 out of 93 Training Loss: 0.0020803980415058836 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 81 out of 93 Training Loss: 0.009775663155813286 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 82 out of 93 Training Loss: 0.014306645735849211 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 83 out of 93 Training Loss: 0.02024974523832328 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 84 out of 93 Training Loss: 0.02865709959318168 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 85 out of 93 Training Loss: 0.03806519544472224 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 86 out of 93 Training Loss: 0.04613459891428477 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 87 out of 93 Training Loss: 0.055080180417497224 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 88 out of 93 Training Loss: 0.0656449025382591 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 89 out of 93 Training Loss: 0.07725005912383325 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 90 out of 93 Training Loss: 0.08623011912008054 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 91 out of 93 Training Loss: 0.09857851396103627 Test Loss: 0.00954313792118972\n",
      "Epoch: 17 Batch: 92 out of 93 Training Loss: 0.10240277078722841 Test Loss: 0.00954313792118972\n",
      "Epoch: 18 Batch: 0 out of 93 Training Loss: 0.00010155600004939623 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 1 out of 93 Training Loss: 0.011349921044643206 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 2 out of 93 Training Loss: 0.019139574770565312 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 3 out of 93 Training Loss: 0.025743626919324678 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 4 out of 93 Training Loss: 0.0360718927576497 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 5 out of 93 Training Loss: 0.04970855631374864 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 6 out of 93 Training Loss: 0.05685066378184704 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 7 out of 93 Training Loss: 0.07199247686633019 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 8 out of 93 Training Loss: 0.08035795150765328 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 9 out of 93 Training Loss: 0.08956638111242203 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 10 out of 93 Training Loss: 0.09508734748208074 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 11 out of 93 Training Loss: 0.10258660505035071 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 12 out of 93 Training Loss: 0.11468729788878111 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 13 out of 93 Training Loss: 0.1196660440645471 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 14 out of 93 Training Loss: 0.12883205099471956 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 15 out of 93 Training Loss: 0.1357172675661102 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 16 out of 93 Training Loss: 0.14399331754514128 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 17 out of 93 Training Loss: 0.1512719056799367 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 18 out of 93 Training Loss: 0.16298151474886685 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 19 out of 93 Training Loss: 0.17444791730933934 Test Loss: 0.009726441846313801\n",
      "Epoch: 18 Batch: 20 out of 93 Training Loss: 0.0019811428274672037 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 21 out of 93 Training Loss: 0.01521132133264947 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 22 out of 93 Training Loss: 0.026691060432390452 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 23 out of 93 Training Loss: 0.034970620566205264 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 24 out of 93 Training Loss: 0.04729026175399232 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 25 out of 93 Training Loss: 0.05650870680172849 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 26 out of 93 Training Loss: 0.06563643078614878 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 27 out of 93 Training Loss: 0.07223552224521446 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 28 out of 93 Training Loss: 0.07733148095493125 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 29 out of 93 Training Loss: 0.08579899316017436 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 30 out of 93 Training Loss: 0.09187774212394761 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 31 out of 93 Training Loss: 0.10163005293642091 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 32 out of 93 Training Loss: 0.10774293021594095 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 33 out of 93 Training Loss: 0.11504398883615541 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 34 out of 93 Training Loss: 0.12134420910213994 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 35 out of 93 Training Loss: 0.13235005774830388 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 36 out of 93 Training Loss: 0.1380118452135186 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 37 out of 93 Training Loss: 0.1469839405391078 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 38 out of 93 Training Loss: 0.15595222023610877 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 39 out of 93 Training Loss: 0.1669359713915448 Test Loss: 0.009418621193617582\n",
      "Epoch: 18 Batch: 40 out of 93 Training Loss: 0.0019009800028293158 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 41 out of 93 Training Loss: 0.01153847364301372 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 42 out of 93 Training Loss: 0.018013351350673917 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 43 out of 93 Training Loss: 0.02612808787787843 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 44 out of 93 Training Loss: 0.04292510890925813 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 45 out of 93 Training Loss: 0.05184104362095285 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 46 out of 93 Training Loss: 0.05831554553995419 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 47 out of 93 Training Loss: 0.06557108809764076 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 48 out of 93 Training Loss: 0.07630342406744647 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 49 out of 93 Training Loss: 0.09287886669451881 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 50 out of 93 Training Loss: 0.10441359666700054 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 51 out of 93 Training Loss: 0.11622789850051093 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 52 out of 93 Training Loss: 0.12546671809221197 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 53 out of 93 Training Loss: 0.13776595959032464 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 54 out of 93 Training Loss: 0.14490010099078107 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 55 out of 93 Training Loss: 0.1546857904043286 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 56 out of 93 Training Loss: 0.1623517738055675 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 57 out of 93 Training Loss: 0.17799817001039672 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 58 out of 93 Training Loss: 0.189359636731097 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 59 out of 93 Training Loss: 0.19867625807935882 Test Loss: 0.009645982772450556\n",
      "Epoch: 18 Batch: 60 out of 93 Training Loss: 0.0022277782234095423 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 61 out of 93 Training Loss: 0.008983221513285526 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 62 out of 93 Training Loss: 0.019573829745008833 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 63 out of 93 Training Loss: 0.02766028293694485 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 64 out of 93 Training Loss: 0.04001902104641426 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 65 out of 93 Training Loss: 0.04906073273445595 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 66 out of 93 Training Loss: 0.06043133479829062 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 67 out of 93 Training Loss: 0.06998147690023411 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 68 out of 93 Training Loss: 0.07861165763164986 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 69 out of 93 Training Loss: 0.08683462874318588 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 70 out of 93 Training Loss: 0.09688471039111841 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 71 out of 93 Training Loss: 0.10460554310406674 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 72 out of 93 Training Loss: 0.11157232304389704 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 73 out of 93 Training Loss: 0.12087161061640013 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 74 out of 93 Training Loss: 0.1348437347012304 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 75 out of 93 Training Loss: 0.14606240545685517 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 76 out of 93 Training Loss: 0.15238976725663173 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 77 out of 93 Training Loss: 0.16145260692860114 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 78 out of 93 Training Loss: 0.17110267621751057 Test Loss: 0.009451526217162609\n",
      "Epoch: 18 Batch: 79 out of 93 Training Loss: 0.18187144459511267 Test Loss: 0.009451526217162609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Batch: 80 out of 93 Training Loss: 0.0020387643232488107 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 81 out of 93 Training Loss: 0.007569221335604138 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 82 out of 93 Training Loss: 0.015291417504801699 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 83 out of 93 Training Loss: 0.024471307660593935 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 84 out of 93 Training Loss: 0.031079118627264448 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 85 out of 93 Training Loss: 0.040140093791439485 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 86 out of 93 Training Loss: 0.0516754626512074 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 87 out of 93 Training Loss: 0.06166920300866599 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 88 out of 93 Training Loss: 0.07139690827871795 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 89 out of 93 Training Loss: 0.0868765791162395 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 90 out of 93 Training Loss: 0.09819452327038283 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 91 out of 93 Training Loss: 0.10692126725162501 Test Loss: 0.00940821785479784\n",
      "Epoch: 18 Batch: 92 out of 93 Training Loss: 0.11786473125512834 Test Loss: 0.00940821785479784\n",
      "Epoch: 19 Batch: 0 out of 93 Training Loss: 0.00011841263822329942 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 1 out of 93 Training Loss: 0.007454114595568309 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 2 out of 93 Training Loss: 0.01853349488428844 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 3 out of 93 Training Loss: 0.028886802123999723 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 4 out of 93 Training Loss: 0.0350454175864817 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 5 out of 93 Training Loss: 0.04530965313515676 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 6 out of 93 Training Loss: 0.05297732223025573 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 7 out of 93 Training Loss: 0.06181303281537307 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 8 out of 93 Training Loss: 0.07786763701788199 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 9 out of 93 Training Loss: 0.0846742928048135 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 10 out of 93 Training Loss: 0.08870793795413387 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 11 out of 93 Training Loss: 0.09478565780646217 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 12 out of 93 Training Loss: 0.10476458268440378 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 13 out of 93 Training Loss: 0.11224975446439397 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 14 out of 93 Training Loss: 0.12126907995170963 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 15 out of 93 Training Loss: 0.1344339783323468 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 16 out of 93 Training Loss: 0.14682344077355278 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 17 out of 93 Training Loss: 0.15276511674947155 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 18 out of 93 Training Loss: 0.16357669243789327 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 19 out of 93 Training Loss: 0.18026786307073248 Test Loss: 0.009456174405799671\n",
      "Epoch: 19 Batch: 20 out of 93 Training Loss: 0.00205967603453776 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 21 out of 93 Training Loss: 0.012808046585110632 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 22 out of 93 Training Loss: 0.0207103577462711 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 23 out of 93 Training Loss: 0.03370252466031691 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 24 out of 93 Training Loss: 0.04008553996884605 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 25 out of 93 Training Loss: 0.0494449798037984 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 26 out of 93 Training Loss: 0.05732607264274141 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 27 out of 93 Training Loss: 0.07092653382771989 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 28 out of 93 Training Loss: 0.07781829945945522 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 29 out of 93 Training Loss: 0.08858288154208442 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 30 out of 93 Training Loss: 0.09923455100708982 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 31 out of 93 Training Loss: 0.10637064445966264 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 32 out of 93 Training Loss: 0.12237001750820657 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 33 out of 93 Training Loss: 0.12891828809135936 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 34 out of 93 Training Loss: 0.136011492276726 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 35 out of 93 Training Loss: 0.15052941609376452 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 36 out of 93 Training Loss: 0.15788764301889918 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 37 out of 93 Training Loss: 0.16877764724397443 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 38 out of 93 Training Loss: 0.17894568741524003 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 39 out of 93 Training Loss: 0.18993501160496257 Test Loss: 0.009326455098661509\n",
      "Epoch: 19 Batch: 40 out of 93 Training Loss: 0.002177133152823912 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 41 out of 93 Training Loss: 0.012545941325467096 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 42 out of 93 Training Loss: 0.022496277394335733 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 43 out of 93 Training Loss: 0.03265432633702086 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 44 out of 93 Training Loss: 0.03919674728308247 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 45 out of 93 Training Loss: 0.04889760997388886 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 46 out of 93 Training Loss: 0.061941876004081235 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 47 out of 93 Training Loss: 0.0705275935331397 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 48 out of 93 Training Loss: 0.07574384931717443 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 49 out of 93 Training Loss: 0.09089161355290937 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 50 out of 93 Training Loss: 0.10293986454848575 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 51 out of 93 Training Loss: 0.11231149539593982 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 52 out of 93 Training Loss: 0.11914746083561706 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 53 out of 93 Training Loss: 0.12981534928325938 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 54 out of 93 Training Loss: 0.13703456746656822 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 55 out of 93 Training Loss: 0.14739015551526474 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 56 out of 93 Training Loss: 0.15695323275644707 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 57 out of 93 Training Loss: 0.16522612275798249 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 58 out of 93 Training Loss: 0.1793438703032248 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 59 out of 93 Training Loss: 0.18953254113216328 Test Loss: 0.009277439337562431\n",
      "Epoch: 19 Batch: 60 out of 93 Training Loss: 0.00212356505677789 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 61 out of 93 Training Loss: 0.009027993881560262 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 62 out of 93 Training Loss: 0.020144874805069858 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 63 out of 93 Training Loss: 0.03490324375913232 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 64 out of 93 Training Loss: 0.042497811482704574 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 65 out of 93 Training Loss: 0.05505743587151855 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 66 out of 93 Training Loss: 0.06266500998050659 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 67 out of 93 Training Loss: 0.07129463564366786 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 68 out of 93 Training Loss: 0.08373902562767951 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 69 out of 93 Training Loss: 0.09139541221544831 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 70 out of 93 Training Loss: 0.10416457673297017 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 71 out of 93 Training Loss: 0.11437478253380148 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 72 out of 93 Training Loss: 0.12421822139755576 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 73 out of 93 Training Loss: 0.13268180249021141 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 74 out of 93 Training Loss: 0.13908662145719616 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 75 out of 93 Training Loss: 0.147401174174107 Test Loss: 0.010159824974834919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Batch: 76 out of 93 Training Loss: 0.1594127208156952 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 77 out of 93 Training Loss: 0.16670080657810657 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 78 out of 93 Training Loss: 0.17333015205100863 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 79 out of 93 Training Loss: 0.17995023878053515 Test Loss: 0.010159824974834919\n",
      "Epoch: 19 Batch: 80 out of 93 Training Loss: 0.001965785077296832 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 81 out of 93 Training Loss: 0.011586416190945246 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 82 out of 93 Training Loss: 0.018790746251576285 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 83 out of 93 Training Loss: 0.02672203085803304 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 84 out of 93 Training Loss: 0.03469812511884485 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 85 out of 93 Training Loss: 0.043207952896588186 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 86 out of 93 Training Loss: 0.05059587317936931 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 87 out of 93 Training Loss: 0.05922726649277721 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 88 out of 93 Training Loss: 0.07202352318399463 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 89 out of 93 Training Loss: 0.07735438333310996 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 90 out of 93 Training Loss: 0.08637026013293181 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 91 out of 93 Training Loss: 0.09220158230000053 Test Loss: 0.009476633954115889\n",
      "Epoch: 19 Batch: 92 out of 93 Training Loss: 0.09516943586476956 Test Loss: 0.009476633954115889\n",
      "Epoch: 20 Batch: 0 out of 93 Training Loss: 0.00011267840501762206 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 1 out of 93 Training Loss: 0.008110606592268713 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 2 out of 93 Training Loss: 0.014430679861576326 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 3 out of 93 Training Loss: 0.02499167389807201 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 4 out of 93 Training Loss: 0.03366588104155756 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 5 out of 93 Training Loss: 0.03897812172409988 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 6 out of 93 Training Loss: 0.04705872714158989 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 7 out of 93 Training Loss: 0.05893533352401949 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 8 out of 93 Training Loss: 0.06766632619884706 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 9 out of 93 Training Loss: 0.07405400883045889 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 10 out of 93 Training Loss: 0.08154062416043974 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 11 out of 93 Training Loss: 0.0988395684548924 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 12 out of 93 Training Loss: 0.10525295884919263 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 13 out of 93 Training Loss: 0.1134227151201377 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 14 out of 93 Training Loss: 0.1240747952983985 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 15 out of 93 Training Loss: 0.13694585581880903 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 16 out of 93 Training Loss: 0.1450008461402068 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 17 out of 93 Training Loss: 0.15497527645540332 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 18 out of 93 Training Loss: 0.1630100477637062 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 19 out of 93 Training Loss: 0.17187910435360765 Test Loss: 0.009426065297289328\n",
      "Epoch: 20 Batch: 20 out of 93 Training Loss: 0.0019817038350701433 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 21 out of 93 Training Loss: 0.009170505488753328 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 22 out of 93 Training Loss: 0.01575670138978959 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 23 out of 93 Training Loss: 0.027303350428581247 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 24 out of 93 Training Loss: 0.03755196862900258 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 25 out of 93 Training Loss: 0.04403601613873244 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 26 out of 93 Training Loss: 0.054668639125764376 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 27 out of 93 Training Loss: 0.06630160872870684 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 28 out of 93 Training Loss: 0.07325249937409163 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 29 out of 93 Training Loss: 0.08838971172505618 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 30 out of 93 Training Loss: 0.09955914159351588 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 31 out of 93 Training Loss: 0.10613745524594188 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 32 out of 93 Training Loss: 0.11610063704708219 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 33 out of 93 Training Loss: 0.12190073289954663 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 34 out of 93 Training Loss: 0.12875396133446693 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 35 out of 93 Training Loss: 0.1380586831741929 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 36 out of 93 Training Loss: 0.14956204433971643 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 37 out of 93 Training Loss: 0.15858210836583375 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 38 out of 93 Training Loss: 0.16920255524152517 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 39 out of 93 Training Loss: 0.18283343249046802 Test Loss: 0.009379635437984358\n",
      "Epoch: 20 Batch: 40 out of 93 Training Loss: 0.002054048574844996 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 41 out of 93 Training Loss: 0.010257497302571932 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 42 out of 93 Training Loss: 0.018102238758782543 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 43 out of 93 Training Loss: 0.026827459096292652 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 44 out of 93 Training Loss: 0.04181567194251219 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 45 out of 93 Training Loss: 0.05040644469050566 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 46 out of 93 Training Loss: 0.056027386210242905 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 47 out of 93 Training Loss: 0.06764078686324755 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 48 out of 93 Training Loss: 0.07738637259540955 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 49 out of 93 Training Loss: 0.08548216229615609 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 50 out of 93 Training Loss: 0.09994967175958554 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 51 out of 93 Training Loss: 0.1119427437552611 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 52 out of 93 Training Loss: 0.11941075684813658 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 53 out of 93 Training Loss: 0.1288786082187096 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 54 out of 93 Training Loss: 0.1389586365440289 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 55 out of 93 Training Loss: 0.1450448938303987 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 56 out of 93 Training Loss: 0.15420088811157145 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 57 out of 93 Training Loss: 0.16209533332226672 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 58 out of 93 Training Loss: 0.17029676480529704 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 59 out of 93 Training Loss: 0.17786099981827536 Test Loss: 0.009723319206386805\n",
      "Epoch: 20 Batch: 60 out of 93 Training Loss: 0.0019800777693603413 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 61 out of 93 Training Loss: 0.013973842669031419 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 62 out of 93 Training Loss: 0.023378984365365306 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 63 out of 93 Training Loss: 0.03508802812149266 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 64 out of 93 Training Loss: 0.04673567164292077 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 65 out of 93 Training Loss: 0.05566141783793906 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 66 out of 93 Training Loss: 0.06641670718391876 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 67 out of 93 Training Loss: 0.07518389850457649 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 68 out of 93 Training Loss: 0.08339753031333427 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 69 out of 93 Training Loss: 0.09070815594469647 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 70 out of 93 Training Loss: 0.10336563998436074 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 71 out of 93 Training Loss: 0.11138489918386082 Test Loss: 0.009282324801791798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Batch: 72 out of 93 Training Loss: 0.11933670455192666 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 73 out of 93 Training Loss: 0.12971064431195597 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 74 out of 93 Training Loss: 0.1436214380112801 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 75 out of 93 Training Loss: 0.1545026281995092 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 76 out of 93 Training Loss: 0.16217510757570605 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 77 out of 93 Training Loss: 0.17382334621225934 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 78 out of 93 Training Loss: 0.1841577895132218 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 79 out of 93 Training Loss: 0.19405715429758172 Test Loss: 0.009282324801791798\n",
      "Epoch: 20 Batch: 80 out of 93 Training Loss: 0.0022017781113558284 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 81 out of 93 Training Loss: 0.013269426529603529 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 82 out of 93 Training Loss: 0.023207728048162984 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 83 out of 93 Training Loss: 0.03473626100023322 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 84 out of 93 Training Loss: 0.044731299621360826 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 85 out of 93 Training Loss: 0.0515362105754488 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 86 out of 93 Training Loss: 0.0579230978575819 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 87 out of 93 Training Loss: 0.06283905147139959 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 88 out of 93 Training Loss: 0.07733486382787161 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 89 out of 93 Training Loss: 0.08252406189714603 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 90 out of 93 Training Loss: 0.09661093051110439 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 91 out of 93 Training Loss: 0.1065486136314743 Test Loss: 0.009321686981076544\n",
      "Epoch: 20 Batch: 92 out of 93 Training Loss: 0.11434321081762247 Test Loss: 0.009321686981076544\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import SmoothL1Loss\n",
    "\n",
    "trainlosses, testlosses = model.fit(trainloader = trainloader,\n",
    "                                    validationloader = valloader,\n",
    "                                    loss = SmoothL1Loss,\n",
    "                                    optim = Adam,\n",
    "                                    lr=0.001,\n",
    "                                    epochs = 20,\n",
    "                                    val_per_batch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainlosses, label = 'Training Losses')\n",
    "#plt.plot(testlosses, label = 'Testing Losses')\n",
    "plt.ylabel('Losses')\n",
    "plt.xlabel('Batch * Epoch')\n",
    "plt.legend()\n",
    "plt.title('Losses Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, expected = model.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_from = 0\n",
    "val_to = 20\n",
    "\n",
    "plt.plot(result[val_from:val_to], label = 'Result')\n",
    "plt.plot(expected[val_from:val_to], label = 'Expected')\n",
    "plt.ylabel('Wind Speed (Normalized)')\n",
    "plt.xticks(range(val_from, val_to))\n",
    "plt.xlabel('Time Range')\n",
    "plt.legend()\n",
    "plt.title('Model Output Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('v1', save_optim=True, cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
