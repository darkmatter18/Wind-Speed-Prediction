{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Copyright 2020 Arkadip Bhattacharya\n",
    "\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import Normalize_df, WindSpeedDataset, ComposeTransform, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>air_temperature_mean</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370203</td>\n",
       "      <td>0.103164</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.322799</td>\n",
       "      <td>0.268912</td>\n",
       "      <td>0.838440</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.302483</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.988858</td>\n",
       "      <td>0.260417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.246050</td>\n",
       "      <td>0.850758</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.827372</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  air_temperature_mean  pressure  wind_direction  wind_speed\n",
       "0  0.000000              0.370203  0.103164        0.732591    0.625000\n",
       "1  0.000011              0.322799  0.268912        0.838440    0.354167\n",
       "2  0.000022              0.302483  0.709078        0.988858    0.260417\n",
       "3  0.000033              0.246050  0.850758        0.239554    0.093750\n",
       "4  0.000044              0.194131  0.827372        0.345404    0.291667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Normalize_df(pd.read_csv('./dataset-daily.csv'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(dataset, test_size = 0.1)\n",
    "trainset, valset = train_test_split(trainset, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                    0.335806\n",
       "air_temperature_mean    0.677201\n",
       "pressure                0.488308\n",
       "wind_direction          0.791086\n",
       "wind_speed              0.385417\n",
       "Name: 1279, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindSpeedDataset(trainset,transform=ComposeTransform([ToTensor()]))\n",
    "test_dataset = WindSpeedDataset(testset, transform=ComposeTransform([ToTensor()]))\n",
    "val_dataset = WindSpeedDataset(valset, transform=ComposeTransform([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6772, 0.4883, 0.7911], dtype=torch.float64),\n",
       " tensor([0.3854], dtype=torch.float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "trainloader = dataloader.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valloader = dataloader.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "testloader = dataloader.DataLoader(test_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7472, 0.5028, 0.7660],\n",
       "        [0.4244, 0.4484, 0.3760],\n",
       "        [0.6885, 0.6080, 0.6379],\n",
       "        [0.2709, 0.8349, 0.2145],\n",
       "        [0.5327, 0.5213, 0.9499]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (fc1): Linear(in_features=3, out_features=30, bias=True)\n",
      "  (fc2): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0 out of 592 Training Loss: 9.588803466711495e-05 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 1 out of 592 Training Loss: 0.009737060399021249 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 2 out of 592 Training Loss: 0.03256644587334548 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 3 out of 592 Training Loss: 0.06418556253727828 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 4 out of 592 Training Loss: 0.06693279626149987 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 5 out of 592 Training Loss: 0.08810988137800549 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 6 out of 592 Training Loss: 0.12240469897467945 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 7 out of 592 Training Loss: 0.14113242807168816 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 8 out of 592 Training Loss: 0.1642647765048251 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 9 out of 592 Training Loss: 0.1742849803932414 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 10 out of 592 Training Loss: 0.18091907421429013 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 11 out of 592 Training Loss: 0.23753761196811055 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 12 out of 592 Training Loss: 0.2530578195520148 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 13 out of 592 Training Loss: 0.2910267456837401 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 14 out of 592 Training Loss: 0.31935551645536275 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 15 out of 592 Training Loss: 0.3789900902130112 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 16 out of 592 Training Loss: 0.41160100953955503 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 17 out of 592 Training Loss: 0.4359362486221299 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 18 out of 592 Training Loss: 0.44964635068852754 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 19 out of 592 Training Loss: 0.5095999992855296 Test Loss: 0.026363551242728576\n",
      "Epoch: 0 Batch: 20 out of 592 Training Loss: 0.0008948057700263865 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 21 out of 592 Training Loss: 0.010551175456153096 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 22 out of 592 Training Loss: 0.014047269364477933 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 23 out of 592 Training Loss: 0.06216666533577615 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 24 out of 592 Training Loss: 0.0999916238986271 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 25 out of 592 Training Loss: 0.11843031889724905 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 26 out of 592 Training Loss: 0.1406379925363081 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 27 out of 592 Training Loss: 0.14616847748834066 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 28 out of 592 Training Loss: 0.2381711673267501 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 29 out of 592 Training Loss: 0.26147092099267416 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 30 out of 592 Training Loss: 0.2820926212050336 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 31 out of 592 Training Loss: 0.3070378747113364 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 32 out of 592 Training Loss: 0.3316448718585389 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 33 out of 592 Training Loss: 0.33803166554275327 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 34 out of 592 Training Loss: 0.3805181394607144 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 35 out of 592 Training Loss: 0.40319450677338414 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 36 out of 592 Training Loss: 0.4702758836835938 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 37 out of 592 Training Loss: 0.4897531970680075 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 38 out of 592 Training Loss: 0.5597863673508483 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 39 out of 592 Training Loss: 0.581203137645157 Test Loss: 0.023189768848665568\n",
      "Epoch: 0 Batch: 40 out of 592 Training Loss: 0.0009953242436051902 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 41 out of 592 Training Loss: 0.009795937937498146 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 42 out of 592 Training Loss: 0.04314154411554342 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 43 out of 592 Training Loss: 0.06964801090657716 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 44 out of 592 Training Loss: 0.10473018723428254 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 45 out of 592 Training Loss: 0.12948717760741715 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 46 out of 592 Training Loss: 0.16320686998665337 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 47 out of 592 Training Loss: 0.2412632873922587 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 48 out of 592 Training Loss: 0.2508947693452239 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 49 out of 592 Training Loss: 0.28853382474631073 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 50 out of 592 Training Loss: 0.32840305126160385 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 51 out of 592 Training Loss: 0.3344860042154789 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 52 out of 592 Training Loss: 0.34177728208601477 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 53 out of 592 Training Loss: 0.3516530965164304 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 54 out of 592 Training Loss: 0.37560161507576706 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 55 out of 592 Training Loss: 0.3797750112310052 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 56 out of 592 Training Loss: 0.41808852671533825 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 57 out of 592 Training Loss: 0.43808995864242317 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 58 out of 592 Training Loss: 0.47686358845680954 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 59 out of 592 Training Loss: 0.4868904308125377 Test Loss: 0.023247476192685128\n",
      "Epoch: 0 Batch: 60 out of 592 Training Loss: 0.0008495416971552815 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 61 out of 592 Training Loss: 0.024037577632318778 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 62 out of 592 Training Loss: 0.038524881425749107 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 63 out of 592 Training Loss: 0.048920870247732444 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 64 out of 592 Training Loss: 0.05913571124101762 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 65 out of 592 Training Loss: 0.10963796679998522 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 66 out of 592 Training Loss: 0.14347411175395136 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 67 out of 592 Training Loss: 0.1460551023031283 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 68 out of 592 Training Loss: 0.1546495127970982 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 69 out of 592 Training Loss: 0.1896946001345921 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 70 out of 592 Training Loss: 0.23788859876940732 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 71 out of 592 Training Loss: 0.2537049203659106 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 72 out of 592 Training Loss: 0.28978063906679635 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 73 out of 592 Training Loss: 0.3012167391846586 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 74 out of 592 Training Loss: 0.31037275768021827 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 75 out of 592 Training Loss: 0.3334907283554483 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 76 out of 592 Training Loss: 0.36256826198796516 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 77 out of 592 Training Loss: 0.40242979205350166 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 78 out of 592 Training Loss: 0.4671298814485479 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 79 out of 592 Training Loss: 0.5287880925009656 Test Loss: 0.023145380900876426\n",
      "Epoch: 0 Batch: 80 out of 592 Training Loss: 0.0009580337098740927 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 81 out of 592 Training Loss: 0.08268642505918973 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 82 out of 592 Training Loss: 0.09619967906092637 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 83 out of 592 Training Loss: 0.11282606875812047 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 84 out of 592 Training Loss: 0.15825610315715305 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 85 out of 592 Training Loss: 0.1764057174912595 Test Loss: 0.02279581904153791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 86 out of 592 Training Loss: 0.19806776648675434 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 87 out of 592 Training Loss: 0.21679062290405265 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 88 out of 592 Training Loss: 0.23334762720679275 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 89 out of 592 Training Loss: 0.2638444126239919 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 90 out of 592 Training Loss: 0.28971091179465286 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 91 out of 592 Training Loss: 0.304055305070331 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 92 out of 592 Training Loss: 0.36966028238003484 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 93 out of 592 Training Loss: 0.3991687128907942 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 94 out of 592 Training Loss: 0.4231626613653206 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 95 out of 592 Training Loss: 0.4475152640259766 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 96 out of 592 Training Loss: 0.4672433080739283 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 97 out of 592 Training Loss: 0.5129275190300249 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 98 out of 592 Training Loss: 0.532973836979916 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 99 out of 592 Training Loss: 0.5499993542796873 Test Loss: 0.02279581904153791\n",
      "Epoch: 0 Batch: 100 out of 592 Training Loss: 0.00098821773436533 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 101 out of 592 Training Loss: 0.039116530411510336 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 102 out of 592 Training Loss: 0.0708544093301581 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 103 out of 592 Training Loss: 0.08600780247953402 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 104 out of 592 Training Loss: 0.09259657581579553 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 105 out of 592 Training Loss: 0.10121449941080676 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 106 out of 592 Training Loss: 0.10666824710798131 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 107 out of 592 Training Loss: 0.1418998560054289 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 108 out of 592 Training Loss: 0.17824366894793378 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 109 out of 592 Training Loss: 0.21644257587623464 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 110 out of 592 Training Loss: 0.22171002273690568 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 111 out of 592 Training Loss: 0.2553554961575256 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 112 out of 592 Training Loss: 0.34200983886849745 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 113 out of 592 Training Loss: 0.3482943109354065 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 114 out of 592 Training Loss: 0.358055024140148 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 115 out of 592 Training Loss: 0.3814097593655871 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 116 out of 592 Training Loss: 0.3977826300194548 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 117 out of 592 Training Loss: 0.4075991015067385 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 118 out of 592 Training Loss: 0.4570855337312506 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 119 out of 592 Training Loss: 0.4884420919349478 Test Loss: 0.022621968000859135\n",
      "Epoch: 0 Batch: 120 out of 592 Training Loss: 0.0008321634168318274 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 121 out of 592 Training Loss: 0.05525320830294404 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 122 out of 592 Training Loss: 0.065152454809337 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 123 out of 592 Training Loss: 0.08678093335130725 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 124 out of 592 Training Loss: 0.11091153292456184 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 125 out of 592 Training Loss: 0.1247943943972281 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 126 out of 592 Training Loss: 0.14822525592991626 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 127 out of 592 Training Loss: 0.1518015986448667 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 128 out of 592 Training Loss: 0.2098426422244451 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 129 out of 592 Training Loss: 0.22013150380463814 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 130 out of 592 Training Loss: 0.24162081295163845 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 131 out of 592 Training Loss: 0.25494881758005833 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 132 out of 592 Training Loss: 0.2721847523144028 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 133 out of 592 Training Loss: 0.2955842096498749 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 134 out of 592 Training Loss: 0.2993019241048536 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 135 out of 592 Training Loss: 0.3073140661372385 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 136 out of 592 Training Loss: 0.3218318176596007 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 137 out of 592 Training Loss: 0.3450502072601161 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 138 out of 592 Training Loss: 0.35447587716863965 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 139 out of 592 Training Loss: 0.3861665939121089 Test Loss: 0.022537665123254003\n",
      "Epoch: 0 Batch: 140 out of 592 Training Loss: 0.0006613838241194184 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 141 out of 592 Training Loss: 0.05286596666432366 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 142 out of 592 Training Loss: 0.06861344862140165 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 143 out of 592 Training Loss: 0.09696786615408406 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 144 out of 592 Training Loss: 0.11234485960907684 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 145 out of 592 Training Loss: 0.12073974541730628 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 146 out of 592 Training Loss: 0.18050566173500762 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 147 out of 592 Training Loss: 0.1909330696553682 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 148 out of 592 Training Loss: 0.24807651943869338 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 149 out of 592 Training Loss: 0.33091533041901333 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 150 out of 592 Training Loss: 0.33508744693703396 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 151 out of 592 Training Loss: 0.34581407576210244 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 152 out of 592 Training Loss: 0.36620639122433407 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 153 out of 592 Training Loss: 0.4389701545924638 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 154 out of 592 Training Loss: 0.4638149202556108 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 155 out of 592 Training Loss: 0.4829451557934974 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 156 out of 592 Training Loss: 0.5799465444625114 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 157 out of 592 Training Loss: 0.6723795351327155 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 158 out of 592 Training Loss: 0.7008156128913854 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 159 out of 592 Training Loss: 0.7350754686386083 Test Loss: 0.023701975499562017\n",
      "Epoch: 0 Batch: 160 out of 592 Training Loss: 0.0012594894277511977 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 161 out of 592 Training Loss: 0.012487750011926785 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 162 out of 592 Training Loss: 0.03809491496110548 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 163 out of 592 Training Loss: 0.05061510435814251 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 164 out of 592 Training Loss: 0.06608348197812428 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 165 out of 592 Training Loss: 0.07521981510842432 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 166 out of 592 Training Loss: 0.10592624309981455 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 167 out of 592 Training Loss: 0.1505166827472507 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 168 out of 592 Training Loss: 0.1817054929914533 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 169 out of 592 Training Loss: 0.2089855666043817 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 170 out of 592 Training Loss: 0.23235991175795187 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 171 out of 592 Training Loss: 0.2652099649193346 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 172 out of 592 Training Loss: 0.31290270339274995 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 173 out of 592 Training Loss: 0.3552009346666395 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 174 out of 592 Training Loss: 0.37169869940305345 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 175 out of 592 Training Loss: 0.4102493184390127 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 176 out of 592 Training Loss: 0.44432344611311547 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 177 out of 592 Training Loss: 0.473743708315736 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 178 out of 592 Training Loss: 0.48961565967226617 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 179 out of 592 Training Loss: 0.5074066738369524 Test Loss: 0.022358677484038653\n",
      "Epoch: 0 Batch: 180 out of 592 Training Loss: 0.0009253106537404997 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 181 out of 592 Training Loss: 0.0031133582807275993 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 182 out of 592 Training Loss: 0.011377703397640918 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 183 out of 592 Training Loss: 0.021328535861441823 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 184 out of 592 Training Loss: 0.043225132859418126 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 185 out of 592 Training Loss: 0.0767613211638424 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 186 out of 592 Training Loss: 0.10316180158323976 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 187 out of 592 Training Loss: 0.22145133931107255 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 188 out of 592 Training Loss: 0.22607139963574144 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 189 out of 592 Training Loss: 0.2461509918785784 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 190 out of 592 Training Loss: 0.26308291684753626 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 191 out of 592 Training Loss: 0.29095278587109774 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 192 out of 592 Training Loss: 0.30901486333377093 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 193 out of 592 Training Loss: 0.32619091402656764 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 194 out of 592 Training Loss: 0.3440642552442047 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 195 out of 592 Training Loss: 0.3603272801256868 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 196 out of 592 Training Loss: 0.3696263851589653 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 197 out of 592 Training Loss: 0.3787007452911827 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 198 out of 592 Training Loss: 0.40283432509667605 Test Loss: 0.022228012496436186\n",
      "Epoch: 0 Batch: 199 out of 592 Training Loss: 0.43131166044540137 Test Loss: 0.022228012496436186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 200 out of 592 Training Loss: 0.0007475658675415245 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 201 out of 592 Training Loss: 0.026972896175621243 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 202 out of 592 Training Loss: 0.0748146239092333 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 203 out of 592 Training Loss: 0.08959999376737901 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 204 out of 592 Training Loss: 0.10602958941662141 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 205 out of 592 Training Loss: 0.14583842525088617 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 206 out of 592 Training Loss: 0.15436950208449193 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 207 out of 592 Training Loss: 0.17084067197346517 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 208 out of 592 Training Loss: 0.17807011980393359 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 209 out of 592 Training Loss: 0.19685159180143305 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 210 out of 592 Training Loss: 0.2626140445826335 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 211 out of 592 Training Loss: 0.2958458952961249 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 212 out of 592 Training Loss: 0.31413096118548345 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 213 out of 592 Training Loss: 0.3268472163362188 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 214 out of 592 Training Loss: 0.3436152695102377 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 215 out of 592 Training Loss: 0.3776428459567709 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 216 out of 592 Training Loss: 0.4006807918084545 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 217 out of 592 Training Loss: 0.41088596008547973 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 218 out of 592 Training Loss: 0.42108672879466247 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 219 out of 592 Training Loss: 0.4538157398236675 Test Loss: 0.022121815614148298\n",
      "Epoch: 0 Batch: 220 out of 592 Training Loss: 0.0007766563019598428 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 221 out of 592 Training Loss: 0.02648043292973885 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 222 out of 592 Training Loss: 0.0410195182583178 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 223 out of 592 Training Loss: 0.12676241654370674 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 224 out of 592 Training Loss: 0.1454301721922005 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 225 out of 592 Training Loss: 0.1579440192825044 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 226 out of 592 Training Loss: 0.211810223156854 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 227 out of 592 Training Loss: 0.24176437161330827 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 228 out of 592 Training Loss: 0.27175179250006326 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 229 out of 592 Training Loss: 0.35484222448830255 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 230 out of 592 Training Loss: 0.39834359041814454 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 231 out of 592 Training Loss: 0.4130374755284632 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 232 out of 592 Training Loss: 0.4356034420004929 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 233 out of 592 Training Loss: 0.4495137774891103 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 234 out of 592 Training Loss: 0.4604323779738391 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 235 out of 592 Training Loss: 0.4972904225385631 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 236 out of 592 Training Loss: 0.5766350453293765 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 237 out of 592 Training Loss: 0.6122093436813796 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 238 out of 592 Training Loss: 0.6390267418271744 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 239 out of 592 Training Loss: 0.6583291621453011 Test Loss: 0.021948816464635347\n",
      "Epoch: 0 Batch: 240 out of 592 Training Loss: 0.0011787696769119456 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 241 out of 592 Training Loss: 0.008667308653670665 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 242 out of 592 Training Loss: 0.032971868107992526 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 243 out of 592 Training Loss: 0.06348874587841641 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 244 out of 592 Training Loss: 0.06789861740820419 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 245 out of 592 Training Loss: 0.10117666917197715 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 246 out of 592 Training Loss: 0.11571155423574458 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 247 out of 592 Training Loss: 0.16766727278285037 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 248 out of 592 Training Loss: 0.24760961095147144 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 249 out of 592 Training Loss: 0.26544166850202094 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 250 out of 592 Training Loss: 0.2837056585615159 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 251 out of 592 Training Loss: 0.3032163058435203 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 252 out of 592 Training Loss: 0.3160859358524562 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 253 out of 592 Training Loss: 0.32980406219415676 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 254 out of 592 Training Loss: 0.33878412964098464 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 255 out of 592 Training Loss: 0.3532154013430358 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 256 out of 592 Training Loss: 0.37400091188304435 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 257 out of 592 Training Loss: 0.3964648258899928 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 258 out of 592 Training Loss: 0.40824472746156465 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 259 out of 592 Training Loss: 0.4200739202117206 Test Loss: 0.021988588295326652\n",
      "Epoch: 0 Batch: 260 out of 592 Training Loss: 0.0007242458528017274 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 261 out of 592 Training Loss: 0.013441539428207801 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 262 out of 592 Training Loss: 0.09049644233939402 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 263 out of 592 Training Loss: 0.09491177514475935 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 264 out of 592 Training Loss: 0.11523303255719297 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 265 out of 592 Training Loss: 0.18175296351594084 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 266 out of 592 Training Loss: 0.2280911119310295 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 267 out of 592 Training Loss: 0.24388139561099165 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 268 out of 592 Training Loss: 0.27738075942081564 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 269 out of 592 Training Loss: 0.2945671440689003 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 270 out of 592 Training Loss: 0.3036435866383945 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 271 out of 592 Training Loss: 0.3081194344563519 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 272 out of 592 Training Loss: 0.31519031018299215 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 273 out of 592 Training Loss: 0.3306000303132569 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 274 out of 592 Training Loss: 0.3501202180637156 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 275 out of 592 Training Loss: 0.36343034688663833 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 276 out of 592 Training Loss: 0.3735555210737621 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 277 out of 592 Training Loss: 0.37675682368037455 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 278 out of 592 Training Loss: 0.4139132528221344 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 279 out of 592 Training Loss: 0.430962775892589 Test Loss: 0.022265700221231036\n",
      "Epoch: 0 Batch: 280 out of 592 Training Loss: 0.0007557072819295726 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 281 out of 592 Training Loss: 0.007208186216008982 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 282 out of 592 Training Loss: 0.018679721295190174 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 283 out of 592 Training Loss: 0.06386728772456678 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 284 out of 592 Training Loss: 0.08111567796702417 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 285 out of 592 Training Loss: 0.10678128899569543 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 286 out of 592 Training Loss: 0.17512099446291957 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 287 out of 592 Training Loss: 0.22775637359852824 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 288 out of 592 Training Loss: 0.2501756269627336 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 289 out of 592 Training Loss: 0.26030579210753474 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 290 out of 592 Training Loss: 0.2846463062011722 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 291 out of 592 Training Loss: 0.3384281955801967 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 292 out of 592 Training Loss: 0.34529344552691016 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 293 out of 592 Training Loss: 0.3663758151375774 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 294 out of 592 Training Loss: 0.45058910922641787 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 295 out of 592 Training Loss: 0.4642493240677837 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 296 out of 592 Training Loss: 0.48204913364286456 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 297 out of 592 Training Loss: 0.5461248196804049 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 298 out of 592 Training Loss: 0.5544686522477391 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 299 out of 592 Training Loss: 0.5922807809108022 Test Loss: 0.021743936342483557\n",
      "Epoch: 0 Batch: 300 out of 592 Training Loss: 0.0010447415391324746 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 301 out of 592 Training Loss: 0.008795863190513729 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 302 out of 592 Training Loss: 0.016855884516459585 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 303 out of 592 Training Loss: 0.02631305361710179 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 304 out of 592 Training Loss: 0.03462474903635371 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 305 out of 592 Training Loss: 0.057270690040510774 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 306 out of 592 Training Loss: 0.0815007325129925 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 307 out of 592 Training Loss: 0.09940785652570117 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 308 out of 592 Training Loss: 0.11156624759159672 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 309 out of 592 Training Loss: 0.15623182261906254 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 310 out of 592 Training Loss: 0.20741804892740834 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 311 out of 592 Training Loss: 0.22322230430863488 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 312 out of 592 Training Loss: 0.2486144510897218 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 313 out of 592 Training Loss: 0.26216744928233016 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 314 out of 592 Training Loss: 0.3043189301954924 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 315 out of 592 Training Loss: 0.3228481404351412 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 316 out of 592 Training Loss: 0.3544793829189955 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 317 out of 592 Training Loss: 0.3565268713849007 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 318 out of 592 Training Loss: 0.36599865107409346 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 319 out of 592 Training Loss: 0.3913112778084694 Test Loss: 0.021561634716239165\n",
      "Epoch: 0 Batch: 320 out of 592 Training Loss: 0.0006709771172386366 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 321 out of 592 Training Loss: 0.0026708745339196877 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 322 out of 592 Training Loss: 0.025850424257783288 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 323 out of 592 Training Loss: 0.04737464310017049 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 324 out of 592 Training Loss: 0.06232166354891479 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 325 out of 592 Training Loss: 0.07735982601877868 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 326 out of 592 Training Loss: 0.08934709028687894 Test Loss: 0.021935759830133367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 327 out of 592 Training Loss: 0.09464198508646905 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 328 out of 592 Training Loss: 0.12169701204982221 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 329 out of 592 Training Loss: 0.14067897544589458 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 330 out of 592 Training Loss: 0.1531966195383352 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 331 out of 592 Training Loss: 0.1973964252450746 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 332 out of 592 Training Loss: 0.24424931564715324 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 333 out of 592 Training Loss: 0.2547992520907205 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 334 out of 592 Training Loss: 0.2671425488719028 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 335 out of 592 Training Loss: 0.2963329532125276 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 336 out of 592 Training Loss: 0.32311663561847626 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 337 out of 592 Training Loss: 0.36533234426167427 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 338 out of 592 Training Loss: 0.3932231903949064 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 339 out of 592 Training Loss: 0.40312788420703827 Test Loss: 0.021935759830133367\n",
      "Epoch: 0 Batch: 340 out of 592 Training Loss: 0.0007248074314898806 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 341 out of 592 Training Loss: 0.009664732081384119 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 342 out of 592 Training Loss: 0.021740664777845795 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 343 out of 592 Training Loss: 0.028861756020725424 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 344 out of 592 Training Loss: 0.06627858667963808 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 345 out of 592 Training Loss: 0.0822425042777664 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 346 out of 592 Training Loss: 0.14055597088629074 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 347 out of 592 Training Loss: 0.1838396755545742 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 348 out of 592 Training Loss: 0.20919229700559444 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 349 out of 592 Training Loss: 0.22826823643738098 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 350 out of 592 Training Loss: 0.24307237523937292 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 351 out of 592 Training Loss: 0.24707022500553794 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 352 out of 592 Training Loss: 0.25820561782786317 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 353 out of 592 Training Loss: 0.32361570285031266 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 354 out of 592 Training Loss: 0.36433194489547677 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 355 out of 592 Training Loss: 0.3795764651648111 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 356 out of 592 Training Loss: 0.4166206699363298 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 357 out of 592 Training Loss: 0.43451992427328295 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 358 out of 592 Training Loss: 0.47159469132641024 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 359 out of 592 Training Loss: 0.517210654964239 Test Loss: 0.021368826106261236\n",
      "Epoch: 0 Batch: 360 out of 592 Training Loss: 0.0008977949745989842 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 361 out of 592 Training Loss: 0.015806415130946737 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 362 out of 592 Training Loss: 0.04556070548817883 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 363 out of 592 Training Loss: 0.05587865786031256 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 364 out of 592 Training Loss: 0.07381934301332008 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 365 out of 592 Training Loss: 0.12717495859936248 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 366 out of 592 Training Loss: 0.1955984681971408 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 367 out of 592 Training Loss: 0.23885053397491943 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 368 out of 592 Training Loss: 0.2607123572506047 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 369 out of 592 Training Loss: 0.26942489390597113 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 370 out of 592 Training Loss: 0.3001413317523933 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 371 out of 592 Training Loss: 0.314484832821045 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 372 out of 592 Training Loss: 0.3283949047453738 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 373 out of 592 Training Loss: 0.3404282266400791 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 374 out of 592 Training Loss: 0.35417980236336954 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 375 out of 592 Training Loss: 0.40678600130007037 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 376 out of 592 Training Loss: 0.4207431804515458 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 377 out of 592 Training Loss: 0.43199738488868245 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 378 out of 592 Training Loss: 0.4369877232365347 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 379 out of 592 Training Loss: 0.5488858996085859 Test Loss: 0.021264676211020826\n",
      "Epoch: 0 Batch: 380 out of 592 Training Loss: 0.00095988645037964 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 381 out of 592 Training Loss: 0.030072797302215844 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 382 out of 592 Training Loss: 0.04959445467969731 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 383 out of 592 Training Loss: 0.08124737835308865 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 384 out of 592 Training Loss: 0.10794588385900811 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 385 out of 592 Training Loss: 0.11358349651849226 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 386 out of 592 Training Loss: 0.14710056290781454 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 387 out of 592 Training Loss: 0.15833406125014976 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 388 out of 592 Training Loss: 0.17654992614692405 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 389 out of 592 Training Loss: 0.19559644323593334 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 390 out of 592 Training Loss: 0.25222851869469837 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 391 out of 592 Training Loss: 0.2689886519015427 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 392 out of 592 Training Loss: 0.2944529258907433 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 393 out of 592 Training Loss: 0.29940758713340715 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 394 out of 592 Training Loss: 0.3098742596135135 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 395 out of 592 Training Loss: 0.3387655119673009 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 396 out of 592 Training Loss: 0.40710009143507436 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 397 out of 592 Training Loss: 0.4178353471011515 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 398 out of 592 Training Loss: 0.43767669026559547 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 399 out of 592 Training Loss: 0.4705234130174513 Test Loss: 0.02123664082627687\n",
      "Epoch: 0 Batch: 400 out of 592 Training Loss: 0.0008212831307555969 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 401 out of 592 Training Loss: 0.03343442273031729 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 402 out of 592 Training Loss: 0.11000196826349753 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 403 out of 592 Training Loss: 0.12352119204293746 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 404 out of 592 Training Loss: 0.13932264228056926 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 405 out of 592 Training Loss: 0.15562237058471698 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 406 out of 592 Training Loss: 0.16603452344130534 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 407 out of 592 Training Loss: 0.17965529025237817 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 408 out of 592 Training Loss: 0.20265315570574063 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 409 out of 592 Training Loss: 0.22442371495704908 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 410 out of 592 Training Loss: 0.2273285557020809 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 411 out of 592 Training Loss: 0.2784907203603889 Test Loss: 0.02113141328322165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 412 out of 592 Training Loss: 0.28792825904380337 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 413 out of 592 Training Loss: 0.33423485052477375 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 414 out of 592 Training Loss: 0.3799740951944496 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 415 out of 592 Training Loss: 0.39482795730870024 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 416 out of 592 Training Loss: 0.4018040830705668 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 417 out of 592 Training Loss: 0.4273033542994763 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 418 out of 592 Training Loss: 0.4305278496321644 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 419 out of 592 Training Loss: 0.4383419710647668 Test Loss: 0.02113141328322165\n",
      "Epoch: 0 Batch: 420 out of 592 Training Loss: 0.000777378620974452 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 421 out of 592 Training Loss: 0.013670179004244238 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 422 out of 592 Training Loss: 0.03918128548043194 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 423 out of 592 Training Loss: 0.09883194876687947 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 424 out of 592 Training Loss: 0.1582524375416559 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 425 out of 592 Training Loss: 0.1627512568965775 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 426 out of 592 Training Loss: 0.19497517444046558 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 427 out of 592 Training Loss: 0.24859939835818828 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 428 out of 592 Training Loss: 0.2931721615031582 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 429 out of 592 Training Loss: 0.29471288697181613 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 430 out of 592 Training Loss: 0.30121896771101625 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 431 out of 592 Training Loss: 0.33024777082113893 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 432 out of 592 Training Loss: 0.3411903491368738 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 433 out of 592 Training Loss: 0.3519835405176726 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 434 out of 592 Training Loss: 0.3627322286492434 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 435 out of 592 Training Loss: 0.3786443106955138 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 436 out of 592 Training Loss: 0.4108798101669398 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 437 out of 592 Training Loss: 0.4445698157077876 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 438 out of 592 Training Loss: 0.4621626183515635 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 439 out of 592 Training Loss: 0.4934548705941287 Test Loss: 0.021012578951547657\n",
      "Epoch: 0 Batch: 440 out of 592 Training Loss: 0.000876194075073115 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 441 out of 592 Training Loss: 0.024914024606670252 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 442 out of 592 Training Loss: 0.061408190265621054 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 443 out of 592 Training Loss: 0.08122885248097311 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 444 out of 592 Training Loss: 0.11622730663451086 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 445 out of 592 Training Loss: 0.13096328473123442 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 446 out of 592 Training Loss: 0.16073279662462603 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 447 out of 592 Training Loss: 0.18638173325392138 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 448 out of 592 Training Loss: 0.2037957312765969 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 449 out of 592 Training Loss: 0.2918759213987244 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 450 out of 592 Training Loss: 0.309787862211789 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 451 out of 592 Training Loss: 0.3293580098304404 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 452 out of 592 Training Loss: 0.3470806183520211 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 453 out of 592 Training Loss: 0.3678646909299744 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 454 out of 592 Training Loss: 0.43818046918543707 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 455 out of 592 Training Loss: 0.4574284127208842 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 456 out of 592 Training Loss: 0.4620004982914282 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 457 out of 592 Training Loss: 0.4741772569011403 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 458 out of 592 Training Loss: 0.4996720931719495 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 459 out of 592 Training Loss: 0.5095825853984309 Test Loss: 0.021432110016713992\n",
      "Epoch: 0 Batch: 460 out of 592 Training Loss: 0.0009135716083855164 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 461 out of 592 Training Loss: 0.06350484867127842 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 462 out of 592 Training Loss: 0.09052580785485215 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 463 out of 592 Training Loss: 0.09923637383165068 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 464 out of 592 Training Loss: 0.1312885368663282 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 465 out of 592 Training Loss: 0.13758541321860857 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 466 out of 592 Training Loss: 0.15316490507232255 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 467 out of 592 Training Loss: 0.17461292496429986 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 468 out of 592 Training Loss: 0.18666963888811178 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 469 out of 592 Training Loss: 0.2031538994471938 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 470 out of 592 Training Loss: 0.21909156492458887 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 471 out of 592 Training Loss: 0.2339547030116827 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 472 out of 592 Training Loss: 0.24818558296667642 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 473 out of 592 Training Loss: 0.2714060550044686 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 474 out of 592 Training Loss: 0.2827763739462406 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 475 out of 592 Training Loss: 0.2854436256877542 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 476 out of 592 Training Loss: 0.31455234186271436 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 477 out of 592 Training Loss: 0.32311309808412797 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 478 out of 592 Training Loss: 0.37060336702982194 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 479 out of 592 Training Loss: 0.38850161367574937 Test Loss: 0.021360632360235533\n",
      "Epoch: 0 Batch: 480 out of 592 Training Loss: 0.0006711874618055652 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 481 out of 592 Training Loss: 0.02365125161547683 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 482 out of 592 Training Loss: 0.04730701637406372 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 483 out of 592 Training Loss: 0.06544746276874565 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 484 out of 592 Training Loss: 0.06804935685310982 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 485 out of 592 Training Loss: 0.07826767074827574 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 486 out of 592 Training Loss: 0.1309178392863693 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 487 out of 592 Training Loss: 0.15521476375822446 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 488 out of 592 Training Loss: 0.19119568499688527 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 489 out of 592 Training Loss: 0.2324459474063339 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 490 out of 592 Training Loss: 0.24738532821599862 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 491 out of 592 Training Loss: 0.27720600883428476 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 492 out of 592 Training Loss: 0.2935800753063383 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 493 out of 592 Training Loss: 0.30258404980365655 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 494 out of 592 Training Loss: 0.3097664290568652 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 495 out of 592 Training Loss: 0.35260299830649516 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 496 out of 592 Training Loss: 0.37794803506646774 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 497 out of 592 Training Loss: 0.38765742680941245 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 498 out of 592 Training Loss: 0.39633512175593993 Test Loss: 0.020996540453228536\n",
      "Epoch: 0 Batch: 499 out of 592 Training Loss: 0.4196195756546082 Test Loss: 0.020996540453228536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 500 out of 592 Training Loss: 0.0007237565375673915 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 501 out of 592 Training Loss: 0.05549160732401492 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 502 out of 592 Training Loss: 0.0730748620487559 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 503 out of 592 Training Loss: 0.08498142814708831 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 504 out of 592 Training Loss: 0.09994621264262082 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 505 out of 592 Training Loss: 0.1256847164265621 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 506 out of 592 Training Loss: 0.1347775130204666 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 507 out of 592 Training Loss: 0.14023983121363404 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 508 out of 592 Training Loss: 0.1483475036591101 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 509 out of 592 Training Loss: 0.17025765885112765 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 510 out of 592 Training Loss: 0.1856605039581466 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 511 out of 592 Training Loss: 0.23316078275887253 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 512 out of 592 Training Loss: 0.24841904189882758 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 513 out of 592 Training Loss: 0.2693681075840998 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 514 out of 592 Training Loss: 0.3035233758061934 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 515 out of 592 Training Loss: 0.3244929976433325 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 516 out of 592 Training Loss: 0.3464907530367184 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 517 out of 592 Training Loss: 0.38446346406338694 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 518 out of 592 Training Loss: 0.44335651133654597 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 519 out of 592 Training Loss: 0.4828173529028702 Test Loss: 0.021019472842894946\n",
      "Epoch: 0 Batch: 520 out of 592 Training Loss: 0.0008978726033423076 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 521 out of 592 Training Loss: 0.03823579043786265 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 522 out of 592 Training Loss: 0.1066098207298968 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 523 out of 592 Training Loss: 0.13541895278434493 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 524 out of 592 Training Loss: 0.1505383815307472 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 525 out of 592 Training Loss: 0.21479318697641828 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 526 out of 592 Training Loss: 0.23215721581767537 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 527 out of 592 Training Loss: 0.2477428953785751 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 528 out of 592 Training Loss: 0.2615993650321219 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 529 out of 592 Training Loss: 0.3044483916525099 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 530 out of 592 Training Loss: 0.31567999289076065 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 531 out of 592 Training Loss: 0.3465705675695393 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 532 out of 592 Training Loss: 0.3596062142093036 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 533 out of 592 Training Loss: 0.3873288504470441 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 534 out of 592 Training Loss: 0.4133651446242187 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 535 out of 592 Training Loss: 0.41860738151411747 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 536 out of 592 Training Loss: 0.43649762064854836 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 537 out of 592 Training Loss: 0.45893038564066624 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 538 out of 592 Training Loss: 0.4704484393511269 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 539 out of 592 Training Loss: 0.5277720329974148 Test Loss: 0.02047096112757131\n",
      "Epoch: 0 Batch: 540 out of 592 Training Loss: 0.0010320781122346138 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 541 out of 592 Training Loss: 0.01174073054896786 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 542 out of 592 Training Loss: 0.04720727487908795 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 543 out of 592 Training Loss: 0.05370825569586947 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 544 out of 592 Training Loss: 0.08979734818892672 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 545 out of 592 Training Loss: 0.10687381814675524 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 546 out of 592 Training Loss: 0.1177935837160249 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 547 out of 592 Training Loss: 0.1263601476650138 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 548 out of 592 Training Loss: 0.1478371298264165 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 549 out of 592 Training Loss: 0.1796085661600728 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 550 out of 592 Training Loss: 0.2098105712185998 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 551 out of 592 Training Loss: 0.22872854716586066 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 552 out of 592 Training Loss: 0.2623229394059081 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 553 out of 592 Training Loss: 0.27597562916087104 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 554 out of 592 Training Loss: 0.3005130360226531 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 555 out of 592 Training Loss: 0.34260056487726165 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 556 out of 592 Training Loss: 0.3747483039598365 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 557 out of 592 Training Loss: 0.4111220831375022 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 558 out of 592 Training Loss: 0.42640013195084525 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 559 out of 592 Training Loss: 0.4381201383049984 Test Loss: 0.02091875033114444\n",
      "Epoch: 0 Batch: 560 out of 592 Training Loss: 0.0007615610177394941 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 561 out of 592 Training Loss: 0.017790710475551804 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 562 out of 592 Training Loss: 0.04863692998539182 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 563 out of 592 Training Loss: 0.06269068169991943 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 564 out of 592 Training Loss: 0.0797153235564706 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 565 out of 592 Training Loss: 0.09385480444531413 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 566 out of 592 Training Loss: 0.12361977707247707 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 567 out of 592 Training Loss: 0.16638701494436237 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 568 out of 592 Training Loss: 0.18069414507250758 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 569 out of 592 Training Loss: 0.21602735381226512 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 570 out of 592 Training Loss: 0.23160586498330804 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 571 out of 592 Training Loss: 0.2465262430871603 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 572 out of 592 Training Loss: 0.27071859493504735 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 573 out of 592 Training Loss: 0.2824191264028427 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 574 out of 592 Training Loss: 0.3298588059062836 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 575 out of 592 Training Loss: 0.3482589273865101 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 576 out of 592 Training Loss: 0.367092378359544 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 577 out of 592 Training Loss: 0.3998661338503239 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 578 out of 592 Training Loss: 0.41220182172607156 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 579 out of 592 Training Loss: 0.4469416066224453 Test Loss: 0.020280160443772646\n",
      "Epoch: 0 Batch: 580 out of 592 Training Loss: 0.0007866969978931984 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 581 out of 592 Training Loss: 0.025119332102549418 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 582 out of 592 Training Loss: 0.057707352069628584 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 583 out of 592 Training Loss: 0.07255640669674908 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 584 out of 592 Training Loss: 0.08164180020750796 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 585 out of 592 Training Loss: 0.11153702574731146 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 586 out of 592 Training Loss: 0.12887809934974942 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 587 out of 592 Training Loss: 0.14547039705277715 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 588 out of 592 Training Loss: 0.16785684178532395 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 589 out of 592 Training Loss: 0.19382804210246835 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 590 out of 592 Training Loss: 0.19888753673942122 Test Loss: 0.02022380264348943\n",
      "Epoch: 0 Batch: 591 out of 592 Training Loss: 0.21949810578436885 Test Loss: 0.02022380264348943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 0 out of 592 Training Loss: 2.8454805598468392e-05 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 1 out of 592 Training Loss: 0.03447431057213327 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 2 out of 592 Training Loss: 0.07906416162370225 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 3 out of 592 Training Loss: 0.1212659786593773 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 4 out of 592 Training Loss: 0.16832710161326905 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 5 out of 592 Training Loss: 0.194222695301161 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 6 out of 592 Training Loss: 0.25408927641330736 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 7 out of 592 Training Loss: 0.26121759611634987 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 8 out of 592 Training Loss: 0.29025049332051056 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 9 out of 592 Training Loss: 0.30720556910602825 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 10 out of 592 Training Loss: 0.31052514851546364 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 11 out of 592 Training Loss: 0.3241358179271825 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 12 out of 592 Training Loss: 0.43309064243441897 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 13 out of 592 Training Loss: 0.5080181810677655 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 14 out of 592 Training Loss: 0.5225828339949854 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 15 out of 592 Training Loss: 0.5414290645763882 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 16 out of 592 Training Loss: 0.5451969349429615 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 17 out of 592 Training Loss: 0.5531265357182034 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 18 out of 592 Training Loss: 0.5957267501995571 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 19 out of 592 Training Loss: 0.6441037946984776 Test Loss: 0.02017661108755309\n",
      "Epoch: 1 Batch: 20 out of 592 Training Loss: 0.0011151779143568645 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 21 out of 592 Training Loss: 0.013838578171884693 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 22 out of 592 Training Loss: 0.03770759717360703 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 23 out of 592 Training Loss: 0.06560299374893871 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 24 out of 592 Training Loss: 0.09925554894641606 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 25 out of 592 Training Loss: 0.1241032873912212 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 26 out of 592 Training Loss: 0.14693916977182594 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 27 out of 592 Training Loss: 0.17314743266240326 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 28 out of 592 Training Loss: 0.1830969435794708 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 29 out of 592 Training Loss: 0.1896571257783529 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 30 out of 592 Training Loss: 0.19706502991527883 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 31 out of 592 Training Loss: 0.20796332011730043 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 32 out of 592 Training Loss: 0.23325917000681726 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 33 out of 592 Training Loss: 0.25106852936357826 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 34 out of 592 Training Loss: 0.26557350779682964 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 35 out of 592 Training Loss: 0.2686103670092729 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 36 out of 592 Training Loss: 0.2940286429765132 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 37 out of 592 Training Loss: 0.34920966198467285 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 38 out of 592 Training Loss: 0.35687323512308866 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 39 out of 592 Training Loss: 0.3921902127730158 Test Loss: 0.020141501440031625\n",
      "Epoch: 1 Batch: 40 out of 592 Training Loss: 0.0006860353914999523 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 41 out of 592 Training Loss: 0.005572820447941974 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 42 out of 592 Training Loss: 0.03298931419160004 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 43 out of 592 Training Loss: 0.03792449076901908 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 44 out of 592 Training Loss: 0.04701303773801561 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 45 out of 592 Training Loss: 0.08224964970033402 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 46 out of 592 Training Loss: 0.08891716636027927 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 47 out of 592 Training Loss: 0.10253973528053398 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 48 out of 592 Training Loss: 0.11180713250543708 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 49 out of 592 Training Loss: 0.14377127438094253 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 50 out of 592 Training Loss: 0.1462299308205002 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 51 out of 592 Training Loss: 0.15544938914719755 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 52 out of 592 Training Loss: 0.21702056624237234 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 53 out of 592 Training Loss: 0.267480185853874 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 54 out of 592 Training Loss: 0.2811473176280015 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 55 out of 592 Training Loss: 0.35471150432023935 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 56 out of 592 Training Loss: 0.3682963891351574 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 57 out of 592 Training Loss: 0.38294784318897657 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 58 out of 592 Training Loss: 0.39192905988786153 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 59 out of 592 Training Loss: 0.4021340949857586 Test Loss: 0.02005038731802029\n",
      "Epoch: 1 Batch: 60 out of 592 Training Loss: 0.000693957962640998 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 61 out of 592 Training Loss: 0.004375014629760263 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 62 out of 592 Training Loss: 0.021841062408485885 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 63 out of 592 Training Loss: 0.033511360351541515 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 64 out of 592 Training Loss: 0.04821317194829273 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 65 out of 592 Training Loss: 0.06739937252233313 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 66 out of 592 Training Loss: 0.07966333179602908 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 67 out of 592 Training Loss: 0.0918056985257058 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 68 out of 592 Training Loss: 0.10196887439916419 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 69 out of 592 Training Loss: 0.12005752622018145 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 70 out of 592 Training Loss: 0.16862913890610026 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 71 out of 592 Training Loss: 0.1941516466332464 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 72 out of 592 Training Loss: 0.21956575213799762 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 73 out of 592 Training Loss: 0.2600696340096979 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 74 out of 592 Training Loss: 0.31442980392704295 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 75 out of 592 Training Loss: 0.32486416323910045 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 76 out of 592 Training Loss: 0.3433431602908163 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 77 out of 592 Training Loss: 0.3582187985895066 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 78 out of 592 Training Loss: 0.4056177681205659 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 79 out of 592 Training Loss: 0.4290707653282075 Test Loss: 0.01994757510714628\n",
      "Epoch: 1 Batch: 80 out of 592 Training Loss: 0.0007377257085905333 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 81 out of 592 Training Loss: 0.01683396748076537 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 82 out of 592 Training Loss: 0.023406632661776332 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 83 out of 592 Training Loss: 0.04486805614824751 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 84 out of 592 Training Loss: 0.05480808061357 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 85 out of 592 Training Loss: 0.10015580844874838 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 86 out of 592 Training Loss: 0.10615847674007872 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 87 out of 592 Training Loss: 0.11149881076808432 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 88 out of 592 Training Loss: 0.1561925936936901 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 89 out of 592 Training Loss: 0.16516096279765824 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 90 out of 592 Training Loss: 0.17855862011011103 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 91 out of 592 Training Loss: 0.19983665500576953 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 92 out of 592 Training Loss: 0.22989805084462622 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 93 out of 592 Training Loss: 0.2584971975981758 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 94 out of 592 Training Loss: 0.26919219777728776 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 95 out of 592 Training Loss: 0.2755914008691476 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 96 out of 592 Training Loss: 0.3063661208822892 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 97 out of 592 Training Loss: 0.31696406514193276 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 98 out of 592 Training Loss: 0.34454640508438805 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 99 out of 592 Training Loss: 0.3949877074434922 Test Loss: 0.02004706463600142\n",
      "Epoch: 1 Batch: 100 out of 592 Training Loss: 0.0007024958714766555 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 101 out of 592 Training Loss: 0.0422503642722888 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 102 out of 592 Training Loss: 0.07267916872947645 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 103 out of 592 Training Loss: 0.09762748814969492 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 104 out of 592 Training Loss: 0.11141615565716458 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 105 out of 592 Training Loss: 0.15651358868776988 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 106 out of 592 Training Loss: 0.16129079479574393 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 107 out of 592 Training Loss: 0.16888757347910832 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 108 out of 592 Training Loss: 0.18189577217995356 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 109 out of 592 Training Loss: 0.18813766557573508 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 110 out of 592 Training Loss: 0.196636516785614 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 111 out of 592 Training Loss: 0.229076739585392 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 112 out of 592 Training Loss: 0.31201316062211226 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 113 out of 592 Training Loss: 0.3187855418756528 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 114 out of 592 Training Loss: 0.3352404545977635 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 115 out of 592 Training Loss: 0.3517654791459365 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 116 out of 592 Training Loss: 0.37486759126036595 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 117 out of 592 Training Loss: 0.37993484474568795 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 118 out of 592 Training Loss: 0.40911030694692563 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 119 out of 592 Training Loss: 0.4395055670276208 Test Loss: 0.019808954712723127\n",
      "Epoch: 1 Batch: 120 out of 592 Training Loss: 0.0008670714564243756 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 121 out of 592 Training Loss: 0.024891112825981755 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 122 out of 592 Training Loss: 0.10365284689437546 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 123 out of 592 Training Loss: 0.11370864872556127 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 124 out of 592 Training Loss: 0.16367006663899816 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 125 out of 592 Training Loss: 0.18940118876021303 Test Loss: 0.019977255043517234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 126 out of 592 Training Loss: 0.2070960941604892 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 127 out of 592 Training Loss: 0.22684427526514925 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 128 out of 592 Training Loss: 0.2372034060649196 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 129 out of 592 Training Loss: 0.26041805919926564 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 130 out of 592 Training Loss: 0.2824328279934446 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 131 out of 592 Training Loss: 0.3253110385380308 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 132 out of 592 Training Loss: 0.33545578633170525 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 133 out of 592 Training Loss: 0.3475386769391219 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 134 out of 592 Training Loss: 0.3615376929632942 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 135 out of 592 Training Loss: 0.3737853419847409 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 136 out of 592 Training Loss: 0.37962797692712347 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 137 out of 592 Training Loss: 0.424898095789514 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 138 out of 592 Training Loss: 0.44853381103571455 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 139 out of 592 Training Loss: 0.4595192264645914 Test Loss: 0.019977255043517234\n",
      "Epoch: 1 Batch: 140 out of 592 Training Loss: 0.0008465534119941663 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 141 out of 592 Training Loss: 0.0069079853638150165 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 142 out of 592 Training Loss: 0.010761244739133901 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 143 out of 592 Training Loss: 0.018812656919200012 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 144 out of 592 Training Loss: 0.04081372595830638 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 145 out of 592 Training Loss: 0.056442030290801116 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 146 out of 592 Training Loss: 0.06830190173788744 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 147 out of 592 Training Loss: 0.09117874182267863 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 148 out of 592 Training Loss: 0.13428614712758738 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 149 out of 592 Training Loss: 0.1622881633739062 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 150 out of 592 Training Loss: 0.1741557331333943 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 151 out of 592 Training Loss: 0.18518998618676144 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 152 out of 592 Training Loss: 0.19248892371981222 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 153 out of 592 Training Loss: 0.19956416455535966 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 154 out of 592 Training Loss: 0.20593196298434216 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 155 out of 592 Training Loss: 0.26642889674260095 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 156 out of 592 Training Loss: 0.2799327956090756 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 157 out of 592 Training Loss: 0.2871168996716924 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 158 out of 592 Training Loss: 0.32220520782722906 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 159 out of 592 Training Loss: 0.3438008907760568 Test Loss: 0.01963993331071723\n",
      "Epoch: 1 Batch: 160 out of 592 Training Loss: 0.0006284052247375162 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 161 out of 592 Training Loss: 0.02923745938783585 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 162 out of 592 Training Loss: 0.047565200340208405 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 163 out of 592 Training Loss: 0.056917272482272976 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 164 out of 592 Training Loss: 0.10248469931280314 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 165 out of 592 Training Loss: 0.11381746919101178 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 166 out of 592 Training Loss: 0.1307524302715629 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 167 out of 592 Training Loss: 0.1396797876546353 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 168 out of 592 Training Loss: 0.14275820620564816 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 169 out of 592 Training Loss: 0.14656385194717048 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 170 out of 592 Training Loss: 0.16074836693910954 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 171 out of 592 Training Loss: 0.19902953200249074 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 172 out of 592 Training Loss: 0.20240167199840842 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 173 out of 592 Training Loss: 0.21565489328673182 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 174 out of 592 Training Loss: 0.23361516974141894 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 175 out of 592 Training Loss: 0.24221647295376358 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 176 out of 592 Training Loss: 0.27302731151899395 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 177 out of 592 Training Loss: 0.3017088840456873 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 178 out of 592 Training Loss: 0.30928699995597897 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 179 out of 592 Training Loss: 0.35388409669717846 Test Loss: 0.01960532428435699\n",
      "Epoch: 1 Batch: 180 out of 592 Training Loss: 0.0006229995861149569 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 181 out of 592 Training Loss: 0.004978224832514598 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 182 out of 592 Training Loss: 0.016131710458496883 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 183 out of 592 Training Loss: 0.04069763069222339 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 184 out of 592 Training Loss: 0.04967065082112916 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 185 out of 592 Training Loss: 0.06187310741464265 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 186 out of 592 Training Loss: 0.07203463577846654 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 187 out of 592 Training Loss: 0.07827542229736813 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 188 out of 592 Training Loss: 0.08461988504225977 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 189 out of 592 Training Loss: 0.09866782250995405 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 190 out of 592 Training Loss: 0.1196797934472061 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 191 out of 592 Training Loss: 0.1786344048678375 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 192 out of 592 Training Loss: 0.18242183137866325 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 193 out of 592 Training Loss: 0.2129366155359394 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 194 out of 592 Training Loss: 0.2486506793293602 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 195 out of 592 Training Loss: 0.28833961297007865 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 196 out of 592 Training Loss: 0.32353918809624976 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 197 out of 592 Training Loss: 0.38325362462731666 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 198 out of 592 Training Loss: 0.39302239470365113 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 199 out of 592 Training Loss: 0.4064013091492898 Test Loss: 0.01958490236288388\n",
      "Epoch: 1 Batch: 200 out of 592 Training Loss: 0.0006994788449388914 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 201 out of 592 Training Loss: 0.017556925947604794 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 202 out of 592 Training Loss: 0.03356345155352177 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 203 out of 592 Training Loss: 0.08764600911253514 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 204 out of 592 Training Loss: 0.09698513400816741 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 205 out of 592 Training Loss: 0.13142651570105376 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 206 out of 592 Training Loss: 0.14993128617369952 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 207 out of 592 Training Loss: 0.18668793578707996 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 208 out of 592 Training Loss: 0.20607680519187274 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 209 out of 592 Training Loss: 0.25331059162580793 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 210 out of 592 Training Loss: 0.26023351698258346 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 211 out of 592 Training Loss: 0.2729382982391996 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 212 out of 592 Training Loss: 0.3176431527275724 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 213 out of 592 Training Loss: 0.33372005465516275 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 214 out of 592 Training Loss: 0.35748321394511884 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 215 out of 592 Training Loss: 0.3875064888883514 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 216 out of 592 Training Loss: 0.42806897330173677 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 217 out of 592 Training Loss: 0.4581921884704513 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 218 out of 592 Training Loss: 0.4826402684588117 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 219 out of 592 Training Loss: 0.5598188882608098 Test Loss: 0.02022969098691123\n",
      "Epoch: 1 Batch: 220 out of 592 Training Loss: 0.000978516836673975 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 221 out of 592 Training Loss: 0.028252436270029307 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 222 out of 592 Training Loss: 0.03764421535006261 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 223 out of 592 Training Loss: 0.04383679009324408 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 224 out of 592 Training Loss: 0.05438094488626814 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 225 out of 592 Training Loss: 0.0632214135984192 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 226 out of 592 Training Loss: 0.07347104787176943 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 227 out of 592 Training Loss: 0.08022591281032658 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 228 out of 592 Training Loss: 0.10728763225770092 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 229 out of 592 Training Loss: 0.1381747908452759 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 230 out of 592 Training Loss: 0.14847288369482853 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 231 out of 592 Training Loss: 0.15459109652137853 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 232 out of 592 Training Loss: 0.23781615305042364 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 233 out of 592 Training Loss: 0.27655330854749777 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 234 out of 592 Training Loss: 0.30012117880678274 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 235 out of 592 Training Loss: 0.3289867993006716 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 236 out of 592 Training Loss: 0.36272090482807257 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 237 out of 592 Training Loss: 0.37855374994969465 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 238 out of 592 Training Loss: 0.39121292340850927 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 239 out of 592 Training Loss: 0.41748143728172876 Test Loss: 0.01941309525511425\n",
      "Epoch: 1 Batch: 240 out of 592 Training Loss: 0.0007562444016001129 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 241 out of 592 Training Loss: 0.010585046599771728 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 242 out of 592 Training Loss: 0.033229188288714635 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 243 out of 592 Training Loss: 0.04233907372054075 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 244 out of 592 Training Loss: 0.05398373980369781 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 245 out of 592 Training Loss: 0.0827638741984627 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 246 out of 592 Training Loss: 0.10589580167379592 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 247 out of 592 Training Loss: 0.15145467896309112 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 248 out of 592 Training Loss: 0.1843175508483908 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 249 out of 592 Training Loss: 0.20142101612236712 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 250 out of 592 Training Loss: 0.21018008817281936 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 251 out of 592 Training Loss: 0.2866482578619978 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 252 out of 592 Training Loss: 0.3116953142389319 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 253 out of 592 Training Loss: 0.315682583714988 Test Loss: 0.019368024447678843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 254 out of 592 Training Loss: 0.3342109203740618 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 255 out of 592 Training Loss: 0.3569799133047125 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 256 out of 592 Training Loss: 0.3780405644729397 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 257 out of 592 Training Loss: 0.38571212696206186 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 258 out of 592 Training Loss: 0.39798473542850826 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 259 out of 592 Training Loss: 0.415318017586287 Test Loss: 0.019368024447678843\n",
      "Epoch: 1 Batch: 260 out of 592 Training Loss: 0.0008001532999382768 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 261 out of 592 Training Loss: 0.01987113623298271 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 262 out of 592 Training Loss: 0.08807971893228157 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 263 out of 592 Training Loss: 0.10303098062403067 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 264 out of 592 Training Loss: 0.10767311748466953 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 265 out of 592 Training Loss: 0.12954898324213487 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 266 out of 592 Training Loss: 0.1425260599365042 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 267 out of 592 Training Loss: 0.1788058984687136 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 268 out of 592 Training Loss: 0.18079318275093598 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 269 out of 592 Training Loss: 0.19814584386884732 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 270 out of 592 Training Loss: 0.20634232630371613 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 271 out of 592 Training Loss: 0.21982908045291466 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 272 out of 592 Training Loss: 0.2527756037318663 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 273 out of 592 Training Loss: 0.28441664372920555 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 274 out of 592 Training Loss: 0.29022283508792085 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 275 out of 592 Training Loss: 0.3231541405560033 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 276 out of 592 Training Loss: 0.3388229648710744 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 277 out of 592 Training Loss: 0.3445154980884449 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 278 out of 592 Training Loss: 0.36101614619954986 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 279 out of 592 Training Loss: 0.4078733020590202 Test Loss: 0.019442695420531727\n",
      "Epoch: 1 Batch: 280 out of 592 Training Loss: 0.0007092803422691277 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 281 out of 592 Training Loss: 0.020276550174164958 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 282 out of 592 Training Loss: 0.04815193076472301 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 283 out of 592 Training Loss: 0.06638866287510414 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 284 out of 592 Training Loss: 0.07170267726148982 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 285 out of 592 Training Loss: 0.07955739597644229 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 286 out of 592 Training Loss: 0.09697530182864089 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 287 out of 592 Training Loss: 0.1026400434490623 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 288 out of 592 Training Loss: 0.1071969000328662 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 289 out of 592 Training Loss: 0.12198765200417061 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 290 out of 592 Training Loss: 0.14046835151355286 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 291 out of 592 Training Loss: 0.1892534866635086 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 292 out of 592 Training Loss: 0.20399185267131348 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 293 out of 592 Training Loss: 0.21730911449383516 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 294 out of 592 Training Loss: 0.2233535018455567 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 295 out of 592 Training Loss: 0.23153329311724324 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 296 out of 592 Training Loss: 0.2545735020827832 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 297 out of 592 Training Loss: 0.3056215722751202 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 298 out of 592 Training Loss: 0.31346346574689765 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 299 out of 592 Training Loss: 0.33952907237078567 Test Loss: 0.019264958038658016\n",
      "Epoch: 1 Batch: 300 out of 592 Training Loss: 0.000599348979694447 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 301 out of 592 Training Loss: 0.01461600239078482 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 302 out of 592 Training Loss: 0.027980900431675514 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 303 out of 592 Training Loss: 0.03214954823876699 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 304 out of 592 Training Loss: 0.050114585614455545 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 305 out of 592 Training Loss: 0.05733717052887162 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 306 out of 592 Training Loss: 0.06543772953252276 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 307 out of 592 Training Loss: 0.09511204163233718 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 308 out of 592 Training Loss: 0.15363311494390447 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 309 out of 592 Training Loss: 0.1650918080161043 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 310 out of 592 Training Loss: 0.1965147061894365 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 311 out of 592 Training Loss: 0.2216064511487909 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 312 out of 592 Training Loss: 0.2325366866464444 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 313 out of 592 Training Loss: 0.2680448908979722 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 314 out of 592 Training Loss: 0.2841802072400876 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 315 out of 592 Training Loss: 0.2878113943773039 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 316 out of 592 Training Loss: 0.29551652778561194 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 317 out of 592 Training Loss: 0.30546670366699774 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 318 out of 592 Training Loss: 0.31632109132285197 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 319 out of 592 Training Loss: 0.33013972506548245 Test Loss: 0.019219373395158487\n",
      "Epoch: 1 Batch: 320 out of 592 Training Loss: 0.0005834437679679826 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 321 out of 592 Training Loss: 0.02112078850680904 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 322 out of 592 Training Loss: 0.04645227638596611 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 323 out of 592 Training Loss: 0.05241672226751165 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 324 out of 592 Training Loss: 0.08418029078805761 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 325 out of 592 Training Loss: 0.09609027390891628 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 326 out of 592 Training Loss: 0.1111885057535203 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 327 out of 592 Training Loss: 0.13521945939922647 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 328 out of 592 Training Loss: 0.15258127541804628 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 329 out of 592 Training Loss: 0.16534340565616207 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 330 out of 592 Training Loss: 0.2192896883707674 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 331 out of 592 Training Loss: 0.23195137267524318 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 332 out of 592 Training Loss: 0.26184297909552173 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 333 out of 592 Training Loss: 0.2777727190178068 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 334 out of 592 Training Loss: 0.3081822450976999 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 335 out of 592 Training Loss: 0.33722253327781276 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 336 out of 592 Training Loss: 0.34886277744138555 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 337 out of 592 Training Loss: 0.3594445567351015 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 338 out of 592 Training Loss: 0.39206722562724666 Test Loss: 0.019193625031744665\n",
      "Epoch: 1 Batch: 339 out of 592 Training Loss: 0.4164611696969898 Test Loss: 0.019193625031744665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 340 out of 592 Training Loss: 0.0008138108217856221 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 341 out of 592 Training Loss: 0.022114521559490855 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 342 out of 592 Training Loss: 0.046718085851683314 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 343 out of 592 Training Loss: 0.0666845950448653 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 344 out of 592 Training Loss: 0.08277727916480511 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 345 out of 592 Training Loss: 0.09129591725410431 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 346 out of 592 Training Loss: 0.10414005975814074 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 347 out of 592 Training Loss: 0.12782649483175484 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 348 out of 592 Training Loss: 0.1327773553469917 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 349 out of 592 Training Loss: 0.1488709018716356 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 350 out of 592 Training Loss: 0.16283426966161935 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 351 out of 592 Training Loss: 0.17769961684586016 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 352 out of 592 Training Loss: 0.1916935068705818 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 353 out of 592 Training Loss: 0.20897648829610554 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 354 out of 592 Training Loss: 0.2362167925724527 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 355 out of 592 Training Loss: 0.24550647914053408 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 356 out of 592 Training Loss: 0.2560681297594449 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 357 out of 592 Training Loss: 0.2858754440361401 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 358 out of 592 Training Loss: 0.2985427684301278 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 359 out of 592 Training Loss: 0.3302437297219178 Test Loss: 0.0191737466851588\n",
      "Epoch: 1 Batch: 360 out of 592 Training Loss: 0.0006021775593417555 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 361 out of 592 Training Loss: 0.03249142992397722 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 362 out of 592 Training Loss: 0.09071631657978471 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 363 out of 592 Training Loss: 0.09716999179771121 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 364 out of 592 Training Loss: 0.11450647956779178 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 365 out of 592 Training Loss: 0.1376220772331017 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 366 out of 592 Training Loss: 0.16818870811810668 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 367 out of 592 Training Loss: 0.2272523103570002 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 368 out of 592 Training Loss: 0.2617472170208949 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 369 out of 592 Training Loss: 0.26860741407697974 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 370 out of 592 Training Loss: 0.2772654879195052 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 371 out of 592 Training Loss: 0.28987050217663823 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 372 out of 592 Training Loss: 0.3469326512244898 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 373 out of 592 Training Loss: 0.3892881476608473 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 374 out of 592 Training Loss: 0.4008754675564724 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 375 out of 592 Training Loss: 0.4463272688267189 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 376 out of 592 Training Loss: 0.45959690281277477 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 377 out of 592 Training Loss: 0.4664155815084833 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 378 out of 592 Training Loss: 0.47561420335521876 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 379 out of 592 Training Loss: 0.5007731916671055 Test Loss: 0.01906800622677882\n",
      "Epoch: 1 Batch: 380 out of 592 Training Loss: 0.0008621278389315176 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 381 out of 592 Training Loss: 0.014048081560335593 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 382 out of 592 Training Loss: 0.03674394145449205 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 383 out of 592 Training Loss: 0.060584067104659516 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 384 out of 592 Training Loss: 0.07130065161576077 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 385 out of 592 Training Loss: 0.08733752529909417 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 386 out of 592 Training Loss: 0.13340897645642563 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 387 out of 592 Training Loss: 0.14222025379379793 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 388 out of 592 Training Loss: 0.18000383555730387 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 389 out of 592 Training Loss: 0.22472081959088847 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 390 out of 592 Training Loss: 0.27869704618891283 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 391 out of 592 Training Loss: 0.29270132697959944 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 392 out of 592 Training Loss: 0.3034393561243038 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 393 out of 592 Training Loss: 0.30987526869242593 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 394 out of 592 Training Loss: 0.35943893363540574 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 395 out of 592 Training Loss: 0.3918164784259121 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 396 out of 592 Training Loss: 0.4242291229016106 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 397 out of 592 Training Loss: 0.4318246466278057 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 398 out of 592 Training Loss: 0.45441889364113613 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 399 out of 592 Training Loss: 0.4832276755182962 Test Loss: 0.019037154478472516\n",
      "Epoch: 1 Batch: 400 out of 592 Training Loss: 0.0008332511638516074 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 401 out of 592 Training Loss: 0.00828142336059748 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 402 out of 592 Training Loss: 0.029710373491536962 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 403 out of 592 Training Loss: 0.045248075038682806 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 404 out of 592 Training Loss: 0.05466999544252096 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 405 out of 592 Training Loss: 0.07756535208274065 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 406 out of 592 Training Loss: 0.09617957530964552 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 407 out of 592 Training Loss: 0.13613181991923987 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 408 out of 592 Training Loss: 0.17943584232081114 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 409 out of 592 Training Loss: 0.22916927246798216 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 410 out of 592 Training Loss: 0.26826541556466754 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 411 out of 592 Training Loss: 0.2808894772310966 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 412 out of 592 Training Loss: 0.28778996609572705 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 413 out of 592 Training Loss: 0.3086177847576552 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 414 out of 592 Training Loss: 0.34862748079423245 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 415 out of 592 Training Loss: 0.34979415335648145 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 416 out of 592 Training Loss: 0.3635456080078372 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 417 out of 592 Training Loss: 0.37938788370184984 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 418 out of 592 Training Loss: 0.4010247851251849 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 419 out of 592 Training Loss: 0.4301951958327779 Test Loss: 0.019093472752226233\n",
      "Epoch: 1 Batch: 420 out of 592 Training Loss: 0.00083430093513236 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 421 out of 592 Training Loss: 0.01870650593922839 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 422 out of 592 Training Loss: 0.026740190394086694 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 423 out of 592 Training Loss: 0.032838547423345896 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 424 out of 592 Training Loss: 0.06714468617723689 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 425 out of 592 Training Loss: 0.088651304885609 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 426 out of 592 Training Loss: 0.1019908765404056 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 427 out of 592 Training Loss: 0.11760456469641432 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 428 out of 592 Training Loss: 0.1437466031013559 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 429 out of 592 Training Loss: 0.15510856249169813 Test Loss: 0.019031024239886778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 430 out of 592 Training Loss: 0.16768836864070402 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 431 out of 592 Training Loss: 0.20207237341241346 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 432 out of 592 Training Loss: 0.21314105094210611 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 433 out of 592 Training Loss: 0.24468651116625773 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 434 out of 592 Training Loss: 0.2574418865217398 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 435 out of 592 Training Loss: 0.29077428282038675 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 436 out of 592 Training Loss: 0.2995990922747682 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 437 out of 592 Training Loss: 0.3382521120904516 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 438 out of 592 Training Loss: 0.3511738300396632 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 439 out of 592 Training Loss: 0.35645053983466135 Test Loss: 0.019031024239886778\n",
      "Epoch: 1 Batch: 440 out of 592 Training Loss: 0.0006253057427047594 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 441 out of 592 Training Loss: 0.030493282646620164 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 442 out of 592 Training Loss: 0.07240537640520323 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 443 out of 592 Training Loss: 0.1027274308062433 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 444 out of 592 Training Loss: 0.15217652072080362 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 445 out of 592 Training Loss: 0.20580117304691065 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 446 out of 592 Training Loss: 0.221012554996395 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 447 out of 592 Training Loss: 0.23167290718176353 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 448 out of 592 Training Loss: 0.2896089236519574 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 449 out of 592 Training Loss: 0.29829949040778864 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 450 out of 592 Training Loss: 0.3148350836045383 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 451 out of 592 Training Loss: 0.3244684542796849 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 452 out of 592 Training Loss: 0.34530703157999504 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 453 out of 592 Training Loss: 0.36516786546328056 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 454 out of 592 Training Loss: 0.3822731784812449 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 455 out of 592 Training Loss: 0.4032533764175414 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 456 out of 592 Training Loss: 0.41836056572266805 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 457 out of 592 Training Loss: 0.47792027604647863 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 458 out of 592 Training Loss: 0.5140117340988755 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 459 out of 592 Training Loss: 0.5331188989139437 Test Loss: 0.019078635488933123\n",
      "Epoch: 1 Batch: 460 out of 592 Training Loss: 0.0009369854200642856 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 461 out of 592 Training Loss: 0.006167193347589329 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 462 out of 592 Training Loss: 0.023659178460017995 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 463 out of 592 Training Loss: 0.033572253243998845 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 464 out of 592 Training Loss: 0.05765024054236825 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 465 out of 592 Training Loss: 0.08424301299639161 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 466 out of 592 Training Loss: 0.1101959314513963 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 467 out of 592 Training Loss: 0.16456969495304044 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 468 out of 592 Training Loss: 0.1749459667447966 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 469 out of 592 Training Loss: 0.18709695286490138 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 470 out of 592 Training Loss: 0.20386978931166347 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 471 out of 592 Training Loss: 0.2189693706426781 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 472 out of 592 Training Loss: 0.23783060483075794 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 473 out of 592 Training Loss: 0.2936944434735935 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 474 out of 592 Training Loss: 0.3048393152970832 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 475 out of 592 Training Loss: 0.33480060327239447 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 476 out of 592 Training Loss: 0.36915304365940504 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 477 out of 592 Training Loss: 0.37431875842996054 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 478 out of 592 Training Loss: 0.4031656649280351 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 479 out of 592 Training Loss: 0.4112189993578952 Test Loss: 0.01898636845042788\n",
      "Epoch: 1 Batch: 480 out of 592 Training Loss: 0.0007051737715295107 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 481 out of 592 Training Loss: 0.03120165371492417 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 482 out of 592 Training Loss: 0.11282845639780076 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 483 out of 592 Training Loss: 0.11952170503482135 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 484 out of 592 Training Loss: 0.14919775498256 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 485 out of 592 Training Loss: 0.15902768687501462 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 486 out of 592 Training Loss: 0.166578957412093 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 487 out of 592 Training Loss: 0.17241952016174705 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 488 out of 592 Training Loss: 0.1807530075979224 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 489 out of 592 Training Loss: 0.18579913030534895 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 490 out of 592 Training Loss: 0.18903035095117898 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 491 out of 592 Training Loss: 0.20154956991009326 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 492 out of 592 Training Loss: 0.2395743828468761 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 493 out of 592 Training Loss: 0.24706916215054603 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 494 out of 592 Training Loss: 0.2709625967048845 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 495 out of 592 Training Loss: 0.28187402693711133 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 496 out of 592 Training Loss: 0.31792303158126684 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 497 out of 592 Training Loss: 0.3822640998490891 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 498 out of 592 Training Loss: 0.40020015260361047 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 499 out of 592 Training Loss: 0.43581398373626085 Test Loss: 0.018813208997898706\n",
      "Epoch: 1 Batch: 500 out of 592 Training Loss: 0.0007850029754692774 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 501 out of 592 Training Loss: 0.0357004427599961 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 502 out of 592 Training Loss: 0.054682240380650206 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 503 out of 592 Training Loss: 0.05922304701686446 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 504 out of 592 Training Loss: 0.06610173175737802 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 505 out of 592 Training Loss: 0.07677790338799897 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 506 out of 592 Training Loss: 0.09093240371600333 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 507 out of 592 Training Loss: 0.12088519802943412 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 508 out of 592 Training Loss: 0.1516989087934846 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 509 out of 592 Training Loss: 0.171642069729721 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 510 out of 592 Training Loss: 0.19710760659233276 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 511 out of 592 Training Loss: 0.21003245751068536 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 512 out of 592 Training Loss: 0.22991014274463598 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 513 out of 592 Training Loss: 0.24077472465739194 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 514 out of 592 Training Loss: 0.2572383564214462 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 515 out of 592 Training Loss: 0.2706935499112362 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 516 out of 592 Training Loss: 0.3246981594960445 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 517 out of 592 Training Loss: 0.4163355026643032 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 518 out of 592 Training Loss: 0.43600767853364886 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 519 out of 592 Training Loss: 0.5094940662632936 Test Loss: 0.018784243410636642\n",
      "Epoch: 1 Batch: 520 out of 592 Training Loss: 0.0009038341263528199 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 521 out of 592 Training Loss: 0.01692488650178865 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 522 out of 592 Training Loss: 0.01915337518176392 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 523 out of 592 Training Loss: 0.050078917297929085 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 524 out of 592 Training Loss: 0.05848275777271346 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 525 out of 592 Training Loss: 0.0716855749519761 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 526 out of 592 Training Loss: 0.10515094176253632 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 527 out of 592 Training Loss: 0.10903548705710427 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 528 out of 592 Training Loss: 0.16448067012561812 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 529 out of 592 Training Loss: 0.20722192409767165 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 530 out of 592 Training Loss: 0.2182296214191462 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 531 out of 592 Training Loss: 0.24856127216799512 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 532 out of 592 Training Loss: 0.25806288770718827 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 533 out of 592 Training Loss: 0.26608594763411536 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 534 out of 592 Training Loss: 0.28498685802651896 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 535 out of 592 Training Loss: 0.28888014424911096 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 536 out of 592 Training Loss: 0.299542349766403 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 537 out of 592 Training Loss: 0.31060330383798435 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 538 out of 592 Training Loss: 0.3631294984402354 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 539 out of 592 Training Loss: 0.37696463894417837 Test Loss: 0.019571642155025267\n",
      "Epoch: 1 Batch: 540 out of 592 Training Loss: 0.000696789943681425 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 541 out of 592 Training Loss: 0.012643411294625467 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 542 out of 592 Training Loss: 0.032195889861391255 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 543 out of 592 Training Loss: 0.03958245879511661 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 544 out of 592 Training Loss: 0.04652723856742806 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 545 out of 592 Training Loss: 0.05671081971849627 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 546 out of 592 Training Loss: 0.0688848998444838 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 547 out of 592 Training Loss: 0.12511228211696573 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 548 out of 592 Training Loss: 0.1435447726445956 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 549 out of 592 Training Loss: 0.16518757120247313 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 550 out of 592 Training Loss: 0.1822394467937512 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 551 out of 592 Training Loss: 0.2863543517934842 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 552 out of 592 Training Loss: 0.30965434387798735 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 553 out of 592 Training Loss: 0.4020796008455319 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 554 out of 592 Training Loss: 0.4371063038754029 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 555 out of 592 Training Loss: 0.45593391918118426 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 556 out of 592 Training Loss: 0.4671229431452913 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 557 out of 592 Training Loss: 0.4809857085394425 Test Loss: 0.01888911812001783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 558 out of 592 Training Loss: 0.4993304722058339 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 559 out of 592 Training Loss: 0.539681416009621 Test Loss: 0.01888911812001783\n",
      "Epoch: 1 Batch: 560 out of 592 Training Loss: 0.0009406326193996046 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 561 out of 592 Training Loss: 0.06892752329303699 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 562 out of 592 Training Loss: 0.08824312182559448 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 563 out of 592 Training Loss: 0.10062895210875945 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 564 out of 592 Training Loss: 0.10633849593295532 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 565 out of 592 Training Loss: 0.124223540150065 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 566 out of 592 Training Loss: 0.13701512718989328 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 567 out of 592 Training Loss: 0.1490764856193848 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 568 out of 592 Training Loss: 0.1725251205120869 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 569 out of 592 Training Loss: 0.18212518709196523 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 570 out of 592 Training Loss: 0.19111089183433727 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 571 out of 592 Training Loss: 0.22665777950032429 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 572 out of 592 Training Loss: 0.2788176812086888 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 573 out of 592 Training Loss: 0.300474866837699 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 574 out of 592 Training Loss: 0.3149437686283775 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 575 out of 592 Training Loss: 0.32093823848708347 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 576 out of 592 Training Loss: 0.4053084194515534 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 577 out of 592 Training Loss: 0.4163732720394321 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 578 out of 592 Training Loss: 0.4262322220806666 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 579 out of 592 Training Loss: 0.44784141665740684 Test Loss: 0.018907830239957988\n",
      "Epoch: 1 Batch: 580 out of 592 Training Loss: 0.0008433589302514164 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 581 out of 592 Training Loss: 0.018734775256416616 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 582 out of 592 Training Loss: 0.060763712745210946 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 583 out of 592 Training Loss: 0.074398449394903 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 584 out of 592 Training Loss: 0.10250883433600408 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 585 out of 592 Training Loss: 0.12276401068647844 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 586 out of 592 Training Loss: 0.15299760061880094 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 587 out of 592 Training Loss: 0.1774767830558632 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 588 out of 592 Training Loss: 0.21668645809968454 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 589 out of 592 Training Loss: 0.23611819814523202 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 590 out of 592 Training Loss: 0.2643164012171839 Test Loss: 0.01919802574962942\n",
      "Epoch: 1 Batch: 591 out of 592 Training Loss: 0.27355825412651047 Test Loss: 0.01919802574962942\n",
      "Epoch: 2 Batch: 0 out of 592 Training Loss: 8.589807092337994e-05 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 1 out of 592 Training Loss: 0.03578791800080924 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 2 out of 592 Training Loss: 0.05172090451358943 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 3 out of 592 Training Loss: 0.0716856460565248 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 4 out of 592 Training Loss: 0.08227867200165181 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 5 out of 592 Training Loss: 0.11928631677418142 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 6 out of 592 Training Loss: 0.13647702648430257 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 7 out of 592 Training Loss: 0.16282232604115396 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 8 out of 592 Training Loss: 0.17039104118137746 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 9 out of 592 Training Loss: 0.19176519229202657 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 10 out of 592 Training Loss: 0.20445093452124982 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 11 out of 592 Training Loss: 0.2119324820594409 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 12 out of 592 Training Loss: 0.2271892641217926 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 13 out of 592 Training Loss: 0.2677459296317318 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 14 out of 592 Training Loss: 0.3005191673071602 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 15 out of 592 Training Loss: 0.31674138722135814 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 16 out of 592 Training Loss: 0.37428149473979266 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 17 out of 592 Training Loss: 0.3963363029540995 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 18 out of 592 Training Loss: 0.449528422999523 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 19 out of 592 Training Loss: 0.4715761475743273 Test Loss: 0.018912551772893603\n",
      "Epoch: 2 Batch: 20 out of 592 Training Loss: 0.0008105912205715201 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 21 out of 592 Training Loss: 0.05579663225979996 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 22 out of 592 Training Loss: 0.07378266909690094 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 23 out of 592 Training Loss: 0.09860024759144974 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 24 out of 592 Training Loss: 0.14472801023693277 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 25 out of 592 Training Loss: 0.188717061370327 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 26 out of 592 Training Loss: 0.20348047623009874 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 27 out of 592 Training Loss: 0.23870014557213975 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 28 out of 592 Training Loss: 0.28225360596151544 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 29 out of 592 Training Loss: 0.2982511498925228 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 30 out of 592 Training Loss: 0.36419160449476434 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 31 out of 592 Training Loss: 0.37539004413010074 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 32 out of 592 Training Loss: 0.4223928290379186 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 33 out of 592 Training Loss: 0.44368253899337246 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 34 out of 592 Training Loss: 0.5101087192964693 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 35 out of 592 Training Loss: 0.540518640257134 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 36 out of 592 Training Loss: 0.5636901955080171 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 37 out of 592 Training Loss: 0.5849292899441858 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 38 out of 592 Training Loss: 0.6130000694614788 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 39 out of 592 Training Loss: 0.6432569070679088 Test Loss: 0.018361693077057487\n",
      "Epoch: 2 Batch: 40 out of 592 Training Loss: 0.0010983229923069813 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 41 out of 592 Training Loss: 0.02755181056736115 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 42 out of 592 Training Loss: 0.03445630924133662 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 43 out of 592 Training Loss: 0.059956623551529684 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 44 out of 592 Training Loss: 0.07126669523594979 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 45 out of 592 Training Loss: 0.07951191750643853 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 46 out of 592 Training Loss: 0.1023648300516427 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 47 out of 592 Training Loss: 0.14709064674852496 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 48 out of 592 Training Loss: 0.16077828065781002 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 49 out of 592 Training Loss: 0.18438177028028374 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 50 out of 592 Training Loss: 0.19941881981967097 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 51 out of 592 Training Loss: 0.26437983599780207 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 52 out of 592 Training Loss: 0.28621184391258364 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 53 out of 592 Training Loss: 0.2931049404042781 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 54 out of 592 Training Loss: 0.3053566397476018 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 55 out of 592 Training Loss: 0.39060231608627444 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 56 out of 592 Training Loss: 0.4515218542265714 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 57 out of 592 Training Loss: 0.46889175412533884 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 58 out of 592 Training Loss: 0.5171420046436608 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 59 out of 592 Training Loss: 0.5385567763197243 Test Loss: 0.018357591202595468\n",
      "Epoch: 2 Batch: 60 out of 592 Training Loss: 0.0009425549719663588 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 61 out of 592 Training Loss: 0.014439429803583237 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 62 out of 592 Training Loss: 0.02412640833744726 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 63 out of 592 Training Loss: 0.05589176872262678 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 64 out of 592 Training Loss: 0.06810572063067874 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 65 out of 592 Training Loss: 0.09714321111777743 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 66 out of 592 Training Loss: 0.1296616799149843 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 67 out of 592 Training Loss: 0.1357727387871774 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 68 out of 592 Training Loss: 0.15876089613402208 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 69 out of 592 Training Loss: 0.1740889308925899 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 70 out of 592 Training Loss: 0.17888867733517608 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 71 out of 592 Training Loss: 0.18404195801803788 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 72 out of 592 Training Loss: 0.19824442052731236 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 73 out of 592 Training Loss: 0.22571554379055223 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 74 out of 592 Training Loss: 0.23929533427098712 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 75 out of 592 Training Loss: 0.28625971143582785 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 76 out of 592 Training Loss: 0.2945320417124959 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 77 out of 592 Training Loss: 0.3020501934874984 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 78 out of 592 Training Loss: 0.3065469457719656 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 79 out of 592 Training Loss: 0.3542602776501509 Test Loss: 0.01827522813850506\n",
      "Epoch: 2 Batch: 80 out of 592 Training Loss: 0.0006489555167292576 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 81 out of 592 Training Loss: 0.011448679194066428 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 82 out of 592 Training Loss: 0.07632747821531048 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 83 out of 592 Training Loss: 0.12497398041209927 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 84 out of 592 Training Loss: 0.1448113077136015 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 85 out of 592 Training Loss: 0.16472825378200762 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 86 out of 592 Training Loss: 0.17671328939518682 Test Loss: 0.018681558099518897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 87 out of 592 Training Loss: 0.23316086240133993 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 88 out of 592 Training Loss: 0.2715880700202917 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 89 out of 592 Training Loss: 0.2831365863608455 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 90 out of 592 Training Loss: 0.29332751378438227 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 91 out of 592 Training Loss: 0.30856949992499105 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 92 out of 592 Training Loss: 0.32445587001762144 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 93 out of 592 Training Loss: 0.3467055332662796 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 94 out of 592 Training Loss: 0.3749046117814516 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 95 out of 592 Training Loss: 0.3978485815974211 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 96 out of 592 Training Loss: 0.41998998791000597 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 97 out of 592 Training Loss: 0.45219095766565554 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 98 out of 592 Training Loss: 0.47873539820334665 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 99 out of 592 Training Loss: 0.4848914320396518 Test Loss: 0.018681558099518897\n",
      "Epoch: 2 Batch: 100 out of 592 Training Loss: 0.0008850310077718141 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 101 out of 592 Training Loss: 0.0195870053103021 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 102 out of 592 Training Loss: 0.03751955629194292 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 103 out of 592 Training Loss: 0.0414678336201063 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 104 out of 592 Training Loss: 0.0513056985182515 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 105 out of 592 Training Loss: 0.08736887044141206 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 106 out of 592 Training Loss: 0.11520154430398855 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 107 out of 592 Training Loss: 0.13860980263779077 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 108 out of 592 Training Loss: 0.14818045060018215 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 109 out of 592 Training Loss: 0.16699946793595466 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 110 out of 592 Training Loss: 0.1757537222547284 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 111 out of 592 Training Loss: 0.1964876580131999 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 112 out of 592 Training Loss: 0.21358075021634015 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 113 out of 592 Training Loss: 0.2764197143805972 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 114 out of 592 Training Loss: 0.3112841027868739 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 115 out of 592 Training Loss: 0.3237931405274025 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 116 out of 592 Training Loss: 0.33299960411021623 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 117 out of 592 Training Loss: 0.34249499562064323 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 118 out of 592 Training Loss: 0.35112581244537744 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 119 out of 592 Training Loss: 0.3592075391752473 Test Loss: 0.01811746338551695\n",
      "Epoch: 2 Batch: 120 out of 592 Training Loss: 0.0006408891708385285 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 121 out of 592 Training Loss: 0.005702321424407893 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 122 out of 592 Training Loss: 0.014646129689438278 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 123 out of 592 Training Loss: 0.038969274617059166 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 124 out of 592 Training Loss: 0.04585113330490702 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 125 out of 592 Training Loss: 0.14737912221558208 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 126 out of 592 Training Loss: 0.15439507039941663 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 127 out of 592 Training Loss: 0.1749829058603179 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 128 out of 592 Training Loss: 0.17815097189254875 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 129 out of 592 Training Loss: 0.1886319746331107 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 130 out of 592 Training Loss: 0.19871710567481632 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 131 out of 592 Training Loss: 0.21453720844097252 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 132 out of 592 Training Loss: 0.27084423742003555 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 133 out of 592 Training Loss: 0.29217325477607364 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 134 out of 592 Training Loss: 0.33102700440890903 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 135 out of 592 Training Loss: 0.3536126664907467 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 136 out of 592 Training Loss: 0.36813376984305496 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 137 out of 592 Training Loss: 0.3824138366685044 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 138 out of 592 Training Loss: 0.39268744039095754 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 139 out of 592 Training Loss: 0.40155445924706573 Test Loss: 0.01810768161103073\n",
      "Epoch: 2 Batch: 140 out of 592 Training Loss: 0.0006963498914112129 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 141 out of 592 Training Loss: 0.006622268355028776 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 142 out of 592 Training Loss: 0.011072627271102814 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 143 out of 592 Training Loss: 0.01595209213210836 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 144 out of 592 Training Loss: 0.06660426082326665 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 145 out of 592 Training Loss: 0.0882684816630055 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 146 out of 592 Training Loss: 0.09737877687676444 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 147 out of 592 Training Loss: 0.1375717422948648 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 148 out of 592 Training Loss: 0.14838506700708165 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 149 out of 592 Training Loss: 0.17625124054266705 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 150 out of 592 Training Loss: 0.19557142863287225 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 151 out of 592 Training Loss: 0.21996992500736012 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 152 out of 592 Training Loss: 0.2424497031630446 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 153 out of 592 Training Loss: 0.2781826318265368 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 154 out of 592 Training Loss: 0.28685713885589137 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 155 out of 592 Training Loss: 0.29319724487377896 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 156 out of 592 Training Loss: 0.3166645751358439 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 157 out of 592 Training Loss: 0.3420008891285826 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 158 out of 592 Training Loss: 0.3532480332092811 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 159 out of 592 Training Loss: 0.3572994451993574 Test Loss: 0.0183166287244989\n",
      "Epoch: 2 Batch: 160 out of 592 Training Loss: 0.000621623004085891 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 161 out of 592 Training Loss: 0.013947616907067173 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 162 out of 592 Training Loss: 0.024072396213419313 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 163 out of 592 Training Loss: 0.031576975358880634 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 164 out of 592 Training Loss: 0.042716854362047786 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 165 out of 592 Training Loss: 0.06674205429688798 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 166 out of 592 Training Loss: 0.06871959950611697 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 167 out of 592 Training Loss: 0.10099655028269397 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 168 out of 592 Training Loss: 0.13183273758576022 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 169 out of 592 Training Loss: 0.15092661056504356 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 170 out of 592 Training Loss: 0.17266828205273257 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 171 out of 592 Training Loss: 0.2078414150683652 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 172 out of 592 Training Loss: 0.22055979177192556 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 173 out of 592 Training Loss: 0.23641602240339624 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 174 out of 592 Training Loss: 0.26426471687690123 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 175 out of 592 Training Loss: 0.26703316333540483 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 176 out of 592 Training Loss: 0.28608758176513716 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 177 out of 592 Training Loss: 0.2883791312279622 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 178 out of 592 Training Loss: 0.3378764711024682 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 179 out of 592 Training Loss: 0.3485228394793789 Test Loss: 0.018168106099244942\n",
      "Epoch: 2 Batch: 180 out of 592 Training Loss: 0.0006051353779373607 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 181 out of 592 Training Loss: 0.03471965637474637 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 182 out of 592 Training Loss: 0.09090854894786457 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 183 out of 592 Training Loss: 0.09686468510373811 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 184 out of 592 Training Loss: 0.10463003503575305 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 185 out of 592 Training Loss: 0.15515692221417407 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 186 out of 592 Training Loss: 0.16118274923830608 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 187 out of 592 Training Loss: 0.17284341823726276 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 188 out of 592 Training Loss: 0.22950037118464092 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 189 out of 592 Training Loss: 0.2829655326571999 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 190 out of 592 Training Loss: 0.28729819521724564 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 191 out of 592 Training Loss: 0.28826400495783744 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 192 out of 592 Training Loss: 0.3168030851306146 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 193 out of 592 Training Loss: 0.3266771579878157 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 194 out of 592 Training Loss: 0.33472549773470817 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 195 out of 592 Training Loss: 0.34746494747416434 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 196 out of 592 Training Loss: 0.38002528853432593 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 197 out of 592 Training Loss: 0.4052682840050882 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 198 out of 592 Training Loss: 0.45682880513088164 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 199 out of 592 Training Loss: 0.4838142660232013 Test Loss: 0.01799354001802082\n",
      "Epoch: 2 Batch: 200 out of 592 Training Loss: 0.0008584971270300156 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 201 out of 592 Training Loss: 0.018489059313271165 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 202 out of 592 Training Loss: 0.060135491444800016 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 203 out of 592 Training Loss: 0.07812627243909609 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 204 out of 592 Training Loss: 0.09866730521533262 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 205 out of 592 Training Loss: 0.12902705955419314 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 206 out of 592 Training Loss: 0.14560551780376207 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 207 out of 592 Training Loss: 0.17599509309146177 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 208 out of 592 Training Loss: 0.20461171145651114 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 209 out of 592 Training Loss: 0.23279434281501543 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 210 out of 592 Training Loss: 0.24645719143662226 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 211 out of 592 Training Loss: 0.24810645684700083 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 212 out of 592 Training Loss: 0.26553168475728106 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 213 out of 592 Training Loss: 0.27369663447241854 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 214 out of 592 Training Loss: 0.3121089819083698 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 215 out of 592 Training Loss: 0.3300286992649324 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 216 out of 592 Training Loss: 0.35472897395591807 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 217 out of 592 Training Loss: 0.37935423433880877 Test Loss: 0.01796143304972881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 218 out of 592 Training Loss: 0.4387726444015987 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 219 out of 592 Training Loss: 0.46201155848901343 Test Loss: 0.01796143304972881\n",
      "Epoch: 2 Batch: 220 out of 592 Training Loss: 0.0008129522643647899 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 221 out of 592 Training Loss: 0.03084750366133077 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 222 out of 592 Training Loss: 0.038607924568619556 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 223 out of 592 Training Loss: 0.05446760481890661 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 224 out of 592 Training Loss: 0.07408424592312796 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 225 out of 592 Training Loss: 0.1051676230013035 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 226 out of 592 Training Loss: 0.15362993939515573 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 227 out of 592 Training Loss: 0.16854846975501997 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 228 out of 592 Training Loss: 0.22119706994589788 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 229 out of 592 Training Loss: 0.23758791326281054 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 230 out of 592 Training Loss: 0.2517044770970605 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 231 out of 592 Training Loss: 0.2667797136559747 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 232 out of 592 Training Loss: 0.3253163214280866 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 233 out of 592 Training Loss: 0.33658420233723146 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 234 out of 592 Training Loss: 0.3661476479351185 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 235 out of 592 Training Loss: 0.3841606838136576 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 236 out of 592 Training Loss: 0.4035876834273003 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 237 out of 592 Training Loss: 0.41547755743112785 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 238 out of 592 Training Loss: 0.43138374606814606 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 239 out of 592 Training Loss: 0.4387923030621968 Test Loss: 0.01803376328880955\n",
      "Epoch: 2 Batch: 240 out of 592 Training Loss: 0.0007719287294000324 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 241 out of 592 Training Loss: 0.018180074321016662 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 242 out of 592 Training Loss: 0.02615728896288573 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 243 out of 592 Training Loss: 0.060078627580687397 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 244 out of 592 Training Loss: 0.06547708265660702 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 245 out of 592 Training Loss: 0.08042841792283952 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 246 out of 592 Training Loss: 0.13040042683659492 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 247 out of 592 Training Loss: 0.14180963315246997 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 248 out of 592 Training Loss: 0.17496391214607654 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 249 out of 592 Training Loss: 0.2174333503115028 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 250 out of 592 Training Loss: 0.22692304585306344 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 251 out of 592 Training Loss: 0.2629929493757933 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 252 out of 592 Training Loss: 0.274346688913092 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 253 out of 592 Training Loss: 0.2822225933539718 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 254 out of 592 Training Loss: 0.30676514111636577 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 255 out of 592 Training Loss: 0.33155970476745067 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 256 out of 592 Training Loss: 0.3699303550290912 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 257 out of 592 Training Loss: 0.41099674455998836 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 258 out of 592 Training Loss: 0.4323650469648689 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 259 out of 592 Training Loss: 0.4477005472051948 Test Loss: 0.017870333693942732\n",
      "Epoch: 2 Batch: 260 out of 592 Training Loss: 0.0008338448512272003 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 261 out of 592 Training Loss: 0.013716022569210923 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 262 out of 592 Training Loss: 0.017493019664184845 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 263 out of 592 Training Loss: 0.04835255894948701 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 264 out of 592 Training Loss: 0.06238346252729157 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 265 out of 592 Training Loss: 0.07621913202245692 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 266 out of 592 Training Loss: 0.08245795671035508 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 267 out of 592 Training Loss: 0.10013492483784417 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 268 out of 592 Training Loss: 0.10599340949346284 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 269 out of 592 Training Loss: 0.11995453000207404 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 270 out of 592 Training Loss: 0.1284675158946679 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 271 out of 592 Training Loss: 0.13408039914582828 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 272 out of 592 Training Loss: 0.15749970669065574 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 273 out of 592 Training Loss: 0.16362542923678616 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 274 out of 592 Training Loss: 0.18081382740427712 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 275 out of 592 Training Loss: 0.2015963502614663 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 276 out of 592 Training Loss: 0.20641626309742192 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 277 out of 592 Training Loss: 0.2635398608087466 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 278 out of 592 Training Loss: 0.31670715924491144 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 279 out of 592 Training Loss: 0.34342560622979856 Test Loss: 0.01789950788864217\n",
      "Epoch: 2 Batch: 280 out of 592 Training Loss: 0.0006620855889356898 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 281 out of 592 Training Loss: 0.07134951370573409 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 282 out of 592 Training Loss: 0.09186218957221873 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 283 out of 592 Training Loss: 0.10766971762097247 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 284 out of 592 Training Loss: 0.13486341479039557 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 285 out of 592 Training Loss: 0.16977514418578513 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 286 out of 592 Training Loss: 0.1910452144107855 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 287 out of 592 Training Loss: 0.2106914026550091 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 288 out of 592 Training Loss: 0.26812917272365455 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 289 out of 592 Training Loss: 0.276725331310097 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 290 out of 592 Training Loss: 0.2912717237955964 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 291 out of 592 Training Loss: 0.31888210586077337 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 292 out of 592 Training Loss: 0.3214567997380233 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 293 out of 592 Training Loss: 0.32801584665468697 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 294 out of 592 Training Loss: 0.3315932992733157 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 295 out of 592 Training Loss: 0.34815318881339197 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 296 out of 592 Training Loss: 0.3807318769282735 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 297 out of 592 Training Loss: 0.38952191359168653 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 298 out of 592 Training Loss: 0.4211948159969485 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 299 out of 592 Training Loss: 0.4426886644101775 Test Loss: 0.017930817352890066\n",
      "Epoch: 2 Batch: 300 out of 592 Training Loss: 0.0007699296929104365 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 301 out of 592 Training Loss: 0.04188953872101141 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 302 out of 592 Training Loss: 0.05555041666405035 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 303 out of 592 Training Loss: 0.0694012276016338 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 304 out of 592 Training Loss: 0.07260377887898815 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 305 out of 592 Training Loss: 0.0848955409255041 Test Loss: 0.01776737697373116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 306 out of 592 Training Loss: 0.12168813213968409 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 307 out of 592 Training Loss: 0.13241107944661512 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 308 out of 592 Training Loss: 0.16013325374835863 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 309 out of 592 Training Loss: 0.17346182555222645 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 310 out of 592 Training Loss: 0.18542459138000145 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 311 out of 592 Training Loss: 0.2063845665779127 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 312 out of 592 Training Loss: 0.22045317475015058 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 313 out of 592 Training Loss: 0.24268017146998777 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 314 out of 592 Training Loss: 0.2554217874657525 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 315 out of 592 Training Loss: 0.26848074510955944 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 316 out of 592 Training Loss: 0.30266133040452137 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 317 out of 592 Training Loss: 0.3106090565797104 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 318 out of 592 Training Loss: 0.32325665742421283 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 319 out of 592 Training Loss: 0.3348697777491225 Test Loss: 0.01776737697373116\n",
      "Epoch: 2 Batch: 320 out of 592 Training Loss: 0.0005996753649855609 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 321 out of 592 Training Loss: 0.015924131050899123 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 322 out of 592 Training Loss: 0.023272983282997702 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 323 out of 592 Training Loss: 0.04167070753891716 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 324 out of 592 Training Loss: 0.06595624193568478 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 325 out of 592 Training Loss: 0.08618999786694298 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 326 out of 592 Training Loss: 0.10942000292260895 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 327 out of 592 Training Loss: 0.1258728387953783 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 328 out of 592 Training Loss: 0.14140457429011116 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 329 out of 592 Training Loss: 0.15112474981714735 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 330 out of 592 Training Loss: 0.15936806473900328 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 331 out of 592 Training Loss: 0.16656645342726956 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 332 out of 592 Training Loss: 0.18127470046301136 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 333 out of 592 Training Loss: 0.2256614486875082 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 334 out of 592 Training Loss: 0.24069294940699587 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 335 out of 592 Training Loss: 0.26502099309135924 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 336 out of 592 Training Loss: 0.282783281542554 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 337 out of 592 Training Loss: 0.31952345792223463 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 338 out of 592 Training Loss: 0.34188317369689 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 339 out of 592 Training Loss: 0.3535425798224355 Test Loss: 0.01810763457835878\n",
      "Epoch: 2 Batch: 340 out of 592 Training Loss: 0.0006604251629877576 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 341 out of 592 Training Loss: 0.014769969618992854 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 342 out of 592 Training Loss: 0.0334567857152987 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 343 out of 592 Training Loss: 0.0673458260662127 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 344 out of 592 Training Loss: 0.09051717456348424 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 345 out of 592 Training Loss: 0.10242103703238253 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 346 out of 592 Training Loss: 0.11052350990392451 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 347 out of 592 Training Loss: 0.11544248845346455 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 348 out of 592 Training Loss: 0.15455254178412442 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 349 out of 592 Training Loss: 0.17834295529790406 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 350 out of 592 Training Loss: 0.19696582119830136 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 351 out of 592 Training Loss: 0.21391545619733815 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 352 out of 592 Training Loss: 0.2200269933036733 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 353 out of 592 Training Loss: 0.24985769681550746 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 354 out of 592 Training Loss: 0.2600603905684519 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 355 out of 592 Training Loss: 0.27439549576409344 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 356 out of 592 Training Loss: 0.29882065217860226 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 357 out of 592 Training Loss: 0.3148551398134995 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 358 out of 592 Training Loss: 0.33140109490521436 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 359 out of 592 Training Loss: 0.3648419753826428 Test Loss: 0.017855967190398864\n",
      "Epoch: 2 Batch: 360 out of 592 Training Loss: 0.000634706790142293 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 361 out of 592 Training Loss: 0.009994404601447815 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 362 out of 592 Training Loss: 0.014644897288226122 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 363 out of 592 Training Loss: 0.034839532083415024 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 364 out of 592 Training Loss: 0.06040518426015281 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 365 out of 592 Training Loss: 0.06829132393612766 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 366 out of 592 Training Loss: 0.0748859552913835 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 367 out of 592 Training Loss: 0.12573738231002593 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 368 out of 592 Training Loss: 0.1470647791647842 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 369 out of 592 Training Loss: 0.16415444410084987 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 370 out of 592 Training Loss: 0.16971061725779676 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 371 out of 592 Training Loss: 0.1721316382536223 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 372 out of 592 Training Loss: 0.2534038976082137 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 373 out of 592 Training Loss: 0.2633222120129993 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 374 out of 592 Training Loss: 0.2650071963216668 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 375 out of 592 Training Loss: 0.2865357774789935 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 376 out of 592 Training Loss: 0.30576077508884725 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 377 out of 592 Training Loss: 0.3425778973396426 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 378 out of 592 Training Loss: 0.3549056820120221 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 379 out of 592 Training Loss: 0.366717375211001 Test Loss: 0.01885347368343818\n",
      "Epoch: 2 Batch: 380 out of 592 Training Loss: 0.0006925270165108355 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 381 out of 592 Training Loss: 0.039780649563183654 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 382 out of 592 Training Loss: 0.06415411943744646 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 383 out of 592 Training Loss: 0.07186089523311483 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 384 out of 592 Training Loss: 0.09752424873586522 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 385 out of 592 Training Loss: 0.11846212737437116 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 386 out of 592 Training Loss: 0.1425113349425398 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 387 out of 592 Training Loss: 0.1604763164388739 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 388 out of 592 Training Loss: 0.16675150554802048 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 389 out of 592 Training Loss: 0.17817658718850243 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 390 out of 592 Training Loss: 0.2011481614055835 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 391 out of 592 Training Loss: 0.23560383703734505 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 392 out of 592 Training Loss: 0.24273771487351287 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 393 out of 592 Training Loss: 0.26347386345442164 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 394 out of 592 Training Loss: 0.2654729559755676 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 395 out of 592 Training Loss: 0.27382799421611953 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 396 out of 592 Training Loss: 0.29258489792410064 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 397 out of 592 Training Loss: 0.32014373128477264 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 398 out of 592 Training Loss: 0.3288473216554873 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 399 out of 592 Training Loss: 0.3479345263651364 Test Loss: 0.017671886217695745\n",
      "Epoch: 2 Batch: 400 out of 592 Training Loss: 0.0006069351849333044 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 401 out of 592 Training Loss: 0.025901168298460602 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 402 out of 592 Training Loss: 0.0684165392932669 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 403 out of 592 Training Loss: 0.08238319558326161 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 404 out of 592 Training Loss: 0.08727312757868326 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 405 out of 592 Training Loss: 0.1144707307149843 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 406 out of 592 Training Loss: 0.1510484807123617 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 407 out of 592 Training Loss: 0.1862619399954752 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 408 out of 592 Training Loss: 0.2015635373001174 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 409 out of 592 Training Loss: 0.20529851987261333 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 410 out of 592 Training Loss: 0.2325134903122858 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 411 out of 592 Training Loss: 0.2818153791059927 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 412 out of 592 Training Loss: 0.30271951406497516 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 413 out of 592 Training Loss: 0.3137137360801176 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 414 out of 592 Training Loss: 0.3277771232580022 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 415 out of 592 Training Loss: 0.3432908199657873 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 416 out of 592 Training Loss: 0.35854785441655673 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 417 out of 592 Training Loss: 0.37877833022970714 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 418 out of 592 Training Loss: 0.397670719020553 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 419 out of 592 Training Loss: 0.40637964866358794 Test Loss: 0.017774064094710135\n",
      "Epoch: 2 Batch: 420 out of 592 Training Loss: 0.0007230401580179791 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 421 out of 592 Training Loss: 0.010813470425173149 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 422 out of 592 Training Loss: 0.02472292022602211 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 423 out of 592 Training Loss: 0.03421311588780533 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 424 out of 592 Training Loss: 0.03755774647751402 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 425 out of 592 Training Loss: 0.05043272041538309 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 426 out of 592 Training Loss: 0.08028333187857221 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 427 out of 592 Training Loss: 0.11851919368663381 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 428 out of 592 Training Loss: 0.14406188876190734 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 429 out of 592 Training Loss: 0.17418727763751102 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 430 out of 592 Training Loss: 0.19927819558241916 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 431 out of 592 Training Loss: 0.20752431005993438 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 432 out of 592 Training Loss: 0.2120530840084846 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 433 out of 592 Training Loss: 0.23390606158235622 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 434 out of 592 Training Loss: 0.25578597114244056 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 435 out of 592 Training Loss: 0.28217801936724735 Test Loss: 0.017600079001788276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 436 out of 592 Training Loss: 0.30465199165800166 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 437 out of 592 Training Loss: 0.3218512211070354 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 438 out of 592 Training Loss: 0.3896540526376064 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 439 out of 592 Training Loss: 0.46293000616887164 Test Loss: 0.017600079001788276\n",
      "Epoch: 2 Batch: 440 out of 592 Training Loss: 0.0007979143382731213 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 441 out of 592 Training Loss: 0.013190640133780553 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 442 out of 592 Training Loss: 0.03651803731636436 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 443 out of 592 Training Loss: 0.04429812403113873 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 444 out of 592 Training Loss: 0.07494891481192142 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 445 out of 592 Training Loss: 0.08814516616703302 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 446 out of 592 Training Loss: 0.10292162047417194 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 447 out of 592 Training Loss: 0.11782899452091486 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 448 out of 592 Training Loss: 0.1227313713142398 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 449 out of 592 Training Loss: 0.12677192268611223 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 450 out of 592 Training Loss: 0.13641020329387218 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 451 out of 592 Training Loss: 0.1546745584318164 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 452 out of 592 Training Loss: 0.16494927694739134 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 453 out of 592 Training Loss: 0.1907376847201708 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 454 out of 592 Training Loss: 0.19729631812961132 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 455 out of 592 Training Loss: 0.2229353995034221 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 456 out of 592 Training Loss: 0.24024931573958427 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 457 out of 592 Training Loss: 0.262569593728495 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 458 out of 592 Training Loss: 0.2857158915111545 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 459 out of 592 Training Loss: 0.31401561246902976 Test Loss: 0.01775645217011598\n",
      "Epoch: 2 Batch: 460 out of 592 Training Loss: 0.0005456494722732021 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 461 out of 592 Training Loss: 0.023030114043153138 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 462 out of 592 Training Loss: 0.036258645970023484 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 463 out of 592 Training Loss: 0.0452986468184956 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 464 out of 592 Training Loss: 0.05297744149307093 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 465 out of 592 Training Loss: 0.07030220949987254 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 466 out of 592 Training Loss: 0.09248439284006438 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 467 out of 592 Training Loss: 0.09995822856167635 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 468 out of 592 Training Loss: 0.10473651919478855 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 469 out of 592 Training Loss: 0.1301363400738903 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 470 out of 592 Training Loss: 0.1482579402249523 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 471 out of 592 Training Loss: 0.18792302083188972 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 472 out of 592 Training Loss: 0.19571425175259194 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 473 out of 592 Training Loss: 0.21494141561875424 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 474 out of 592 Training Loss: 0.22357300916651568 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 475 out of 592 Training Loss: 0.23951649168471179 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 476 out of 592 Training Loss: 0.3020101884935268 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 477 out of 592 Training Loss: 0.323892780262626 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 478 out of 592 Training Loss: 0.36186386333743414 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 479 out of 592 Training Loss: 0.3705859786454805 Test Loss: 0.017571676813531667\n",
      "Epoch: 2 Batch: 480 out of 592 Training Loss: 0.0006489152102284752 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 481 out of 592 Training Loss: 0.01287872661040357 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 482 out of 592 Training Loss: 0.03119820673153928 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 483 out of 592 Training Loss: 0.08684810195253423 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 484 out of 592 Training Loss: 0.09054955062591663 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 485 out of 592 Training Loss: 0.1192344461115562 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 486 out of 592 Training Loss: 0.13649911959790817 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 487 out of 592 Training Loss: 0.15852393736266723 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 488 out of 592 Training Loss: 0.1837896943571531 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 489 out of 592 Training Loss: 0.20308310506069294 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 490 out of 592 Training Loss: 0.2129826561095678 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 491 out of 592 Training Loss: 0.22359783635609976 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 492 out of 592 Training Loss: 0.23183789351158968 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 493 out of 592 Training Loss: 0.26044680983894697 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 494 out of 592 Training Loss: 0.31302352682822576 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 495 out of 592 Training Loss: 0.3366528401630484 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 496 out of 592 Training Loss: 0.35575557002001634 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 497 out of 592 Training Loss: 0.37003586222552887 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 498 out of 592 Training Loss: 0.39457673174464336 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 499 out of 592 Training Loss: 0.3982849376289659 Test Loss: 0.017633023187623952\n",
      "Epoch: 2 Batch: 500 out of 592 Training Loss: 0.0007004796398348189 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 501 out of 592 Training Loss: 0.009327683864302097 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 502 out of 592 Training Loss: 0.012285522684595642 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 503 out of 592 Training Loss: 0.020232688203594744 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 504 out of 592 Training Loss: 0.02350862048207885 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 505 out of 592 Training Loss: 0.03146053388356334 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 506 out of 592 Training Loss: 0.03474992260909742 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 507 out of 592 Training Loss: 0.03919600535757011 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 508 out of 592 Training Loss: 0.07131515283710427 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 509 out of 592 Training Loss: 0.09921170984036393 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 510 out of 592 Training Loss: 0.10600762701234884 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 511 out of 592 Training Loss: 0.12593909053704805 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 512 out of 592 Training Loss: 0.16851737423441476 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 513 out of 592 Training Loss: 0.20431091337106294 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 514 out of 592 Training Loss: 0.23034596143863267 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 515 out of 592 Training Loss: 0.27215922697088785 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 516 out of 592 Training Loss: 0.2896783806254632 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 517 out of 592 Training Loss: 0.2930416383651561 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 518 out of 592 Training Loss: 0.3107387044338054 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 519 out of 592 Training Loss: 0.32011633486128516 Test Loss: 0.01758953712873556\n",
      "Epoch: 2 Batch: 520 out of 592 Training Loss: 0.0005657883991609263 Test Loss: 0.017805416447420914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 521 out of 592 Training Loss: 0.011516288544715468 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 522 out of 592 Training Loss: 0.023092410217703405 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 523 out of 592 Training Loss: 0.02907557056072456 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 524 out of 592 Training Loss: 0.059171116135747256 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 525 out of 592 Training Loss: 0.06801229079040987 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 526 out of 592 Training Loss: 0.08301020336124403 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 527 out of 592 Training Loss: 0.10615074743303758 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 528 out of 592 Training Loss: 0.1251229729202698 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 529 out of 592 Training Loss: 0.15803947408470614 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 530 out of 592 Training Loss: 0.18438067954751952 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 531 out of 592 Training Loss: 0.19714368243488772 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 532 out of 592 Training Loss: 0.227938418432803 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 533 out of 592 Training Loss: 0.2762161764946888 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 534 out of 592 Training Loss: 0.29085002575788005 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 535 out of 592 Training Loss: 0.3147523823824833 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 536 out of 592 Training Loss: 0.32133371538269623 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 537 out of 592 Training Loss: 0.3500932899003039 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 538 out of 592 Training Loss: 0.3750376307104836 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 539 out of 592 Training Loss: 0.39080207577693565 Test Loss: 0.017805416447420914\n",
      "Epoch: 2 Batch: 540 out of 592 Training Loss: 0.0006883105547351629 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 541 out of 592 Training Loss: 0.006753653130046824 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 542 out of 592 Training Loss: 0.029869464999548893 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 543 out of 592 Training Loss: 0.04960693961989115 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 544 out of 592 Training Loss: 0.06782197191428374 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 545 out of 592 Training Loss: 0.07785409300636481 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 546 out of 592 Training Loss: 0.09252876772593688 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 547 out of 592 Training Loss: 0.11782516895841788 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 548 out of 592 Training Loss: 0.15055298044394683 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 549 out of 592 Training Loss: 0.18211690320681762 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 550 out of 592 Training Loss: 0.21148227698218058 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 551 out of 592 Training Loss: 0.21678395646672677 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 552 out of 592 Training Loss: 0.23280169705908726 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 553 out of 592 Training Loss: 0.24458212948452662 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 554 out of 592 Training Loss: 0.26427231713246535 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 555 out of 592 Training Loss: 0.27280413064222764 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 556 out of 592 Training Loss: 0.28480823974173497 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 557 out of 592 Training Loss: 0.2909456187212093 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 558 out of 592 Training Loss: 0.30267449126627277 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 559 out of 592 Training Loss: 0.32778333344545196 Test Loss: 0.01750400229745234\n",
      "Epoch: 2 Batch: 560 out of 592 Training Loss: 0.0005843796413809032 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 561 out of 592 Training Loss: 0.029258395416703817 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 562 out of 592 Training Loss: 0.03961426472481787 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 563 out of 592 Training Loss: 0.07574059149440825 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 564 out of 592 Training Loss: 0.09842455653664171 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 565 out of 592 Training Loss: 0.10475408133309781 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 566 out of 592 Training Loss: 0.1262020509336561 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 567 out of 592 Training Loss: 0.13481029622447194 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 568 out of 592 Training Loss: 0.1606885654498309 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 569 out of 592 Training Loss: 0.1801620790381402 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 570 out of 592 Training Loss: 0.20332520209443272 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 571 out of 592 Training Loss: 0.22900584839474858 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 572 out of 592 Training Loss: 0.2398241892103523 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 573 out of 592 Training Loss: 0.26455137353580893 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 574 out of 592 Training Loss: 0.27145465590250195 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 575 out of 592 Training Loss: 0.3117178840209216 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 576 out of 592 Training Loss: 0.3541143027712077 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 577 out of 592 Training Loss: 0.3952304063011378 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 578 out of 592 Training Loss: 0.42482721001100243 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 579 out of 592 Training Loss: 0.46476130873155297 Test Loss: 0.017625681398909143\n",
      "Epoch: 2 Batch: 580 out of 592 Training Loss: 0.0008332895834937239 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 581 out of 592 Training Loss: 0.02302546415226604 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 582 out of 592 Training Loss: 0.033626722988952654 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 583 out of 592 Training Loss: 0.05046187609182741 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 584 out of 592 Training Loss: 0.05974346108483221 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 585 out of 592 Training Loss: 0.06179553561592545 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 586 out of 592 Training Loss: 0.07652264007115807 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 587 out of 592 Training Loss: 0.09017864719713177 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 588 out of 592 Training Loss: 0.09522919368678774 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 589 out of 592 Training Loss: 0.11026272655213322 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 590 out of 592 Training Loss: 0.14940092072570765 Test Loss: 0.017572353528741973\n",
      "Epoch: 2 Batch: 591 out of 592 Training Loss: 0.19211780012333834 Test Loss: 0.017572353528741973\n",
      "Epoch: 3 Batch: 0 out of 592 Training Loss: 1.7223840365438042e-05 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 1 out of 592 Training Loss: 0.019189285075078038 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 2 out of 592 Training Loss: 0.030273240951607255 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 3 out of 592 Training Loss: 0.05293530314953092 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 4 out of 592 Training Loss: 0.09479197203905347 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 5 out of 592 Training Loss: 0.11869979366452459 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 6 out of 592 Training Loss: 0.12999428245962624 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 7 out of 592 Training Loss: 0.16835514638365273 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 8 out of 592 Training Loss: 0.1923473532849646 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 9 out of 592 Training Loss: 0.22250705394686227 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 10 out of 592 Training Loss: 0.23318690523297553 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 11 out of 592 Training Loss: 0.2557306540736652 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 12 out of 592 Training Loss: 0.26485992684752707 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 13 out of 592 Training Loss: 0.27480197693944935 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 14 out of 592 Training Loss: 0.2815133398005462 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 15 out of 592 Training Loss: 0.2866134997063971 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 16 out of 592 Training Loss: 0.3317093271368838 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 17 out of 592 Training Loss: 0.3335090316438771 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 18 out of 592 Training Loss: 0.3530444781833983 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 19 out of 592 Training Loss: 0.36473562098831896 Test Loss: 0.01793887181856641\n",
      "Epoch: 3 Batch: 20 out of 592 Training Loss: 0.0007086993946432222 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 21 out of 592 Training Loss: 0.003932976099356924 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 22 out of 592 Training Loss: 0.01449843396495274 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 23 out of 592 Training Loss: 0.04741557215403012 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 24 out of 592 Training Loss: 0.06407239747296264 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 25 out of 592 Training Loss: 0.07184529158349326 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 26 out of 592 Training Loss: 0.10863988327856353 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 27 out of 592 Training Loss: 0.13955556455130866 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 28 out of 592 Training Loss: 0.1787848159956484 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 29 out of 592 Training Loss: 0.18853799386943867 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 30 out of 592 Training Loss: 0.23180051281179478 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 31 out of 592 Training Loss: 0.24497479147340348 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 32 out of 592 Training Loss: 0.2566763455333858 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 33 out of 592 Training Loss: 0.28114314324285083 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 34 out of 592 Training Loss: 0.2878928933578282 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 35 out of 592 Training Loss: 0.2903512273269444 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 36 out of 592 Training Loss: 0.3527149115043431 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 37 out of 592 Training Loss: 0.35871853626395755 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 38 out of 592 Training Loss: 0.36608701239402586 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 39 out of 592 Training Loss: 0.3790767355176359 Test Loss: 0.017689371936615895\n",
      "Epoch: 3 Batch: 40 out of 592 Training Loss: 0.0006491291009976301 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 41 out of 592 Training Loss: 0.011495946035642806 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 42 out of 592 Training Loss: 0.02527523233081836 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 43 out of 592 Training Loss: 0.0525646377959444 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 44 out of 592 Training Loss: 0.071871213675399 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 45 out of 592 Training Loss: 0.0867287396126582 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 46 out of 592 Training Loss: 0.10049401415969866 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 47 out of 592 Training Loss: 0.1257728916266157 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 48 out of 592 Training Loss: 0.13328163424964684 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 49 out of 592 Training Loss: 0.14065726526539463 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 50 out of 592 Training Loss: 0.1625520256952538 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 51 out of 592 Training Loss: 0.18903394468586582 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 52 out of 592 Training Loss: 0.21844512276809352 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 53 out of 592 Training Loss: 0.23217576142858404 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 54 out of 592 Training Loss: 0.2773688003094091 Test Loss: 0.017382712599073508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 55 out of 592 Training Loss: 0.3221455305488004 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 56 out of 592 Training Loss: 0.33201111626337904 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 57 out of 592 Training Loss: 0.3388162986979975 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 58 out of 592 Training Loss: 0.3416873713162883 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 59 out of 592 Training Loss: 0.3483047027279778 Test Loss: 0.017382712599073508\n",
      "Epoch: 3 Batch: 60 out of 592 Training Loss: 0.0006072729731817375 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 61 out of 592 Training Loss: 0.02586899734091951 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 62 out of 592 Training Loss: 0.04469164124679757 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 63 out of 592 Training Loss: 0.05756056717586709 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 64 out of 592 Training Loss: 0.07763519174409104 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 65 out of 592 Training Loss: 0.08551528993052436 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 66 out of 592 Training Loss: 0.11616661156517459 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 67 out of 592 Training Loss: 0.12108831656155182 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 68 out of 592 Training Loss: 0.14976239939210012 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 69 out of 592 Training Loss: 0.15337465786364868 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 70 out of 592 Training Loss: 0.18052696519521072 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 71 out of 592 Training Loss: 0.19153430861410692 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 72 out of 592 Training Loss: 0.20879268077191904 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 73 out of 592 Training Loss: 0.2818765657859762 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 74 out of 592 Training Loss: 0.3041745069342573 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 75 out of 592 Training Loss: 0.37151795801696375 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 76 out of 592 Training Loss: 0.38384775620874956 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 77 out of 592 Training Loss: 0.41384038318869665 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 78 out of 592 Training Loss: 0.4246871023344715 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 79 out of 592 Training Loss: 0.45406590764400556 Test Loss: 0.017365159415710492\n",
      "Epoch: 3 Batch: 80 out of 592 Training Loss: 0.0008757324396737858 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 81 out of 592 Training Loss: 0.01643301876924122 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 82 out of 592 Training Loss: 0.02780684414050663 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 83 out of 592 Training Loss: 0.05965141537330235 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 84 out of 592 Training Loss: 0.07121626059255684 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 85 out of 592 Training Loss: 0.08768873805842484 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 86 out of 592 Training Loss: 0.10109645458600605 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 87 out of 592 Training Loss: 0.1320967390640744 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 88 out of 592 Training Loss: 0.15687633077344978 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 89 out of 592 Training Loss: 0.166216821919299 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 90 out of 592 Training Loss: 0.18407971928558434 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 91 out of 592 Training Loss: 0.1856717357152247 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 92 out of 592 Training Loss: 0.2123623771333241 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 93 out of 592 Training Loss: 0.2609262225889706 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 94 out of 592 Training Loss: 0.31165059113140336 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 95 out of 592 Training Loss: 0.3408113384359621 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 96 out of 592 Training Loss: 0.361020039927318 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 97 out of 592 Training Loss: 0.4034599141889357 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 98 out of 592 Training Loss: 0.4129795815535926 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 99 out of 592 Training Loss: 0.41969770721356503 Test Loss: 0.01787165060963933\n",
      "Epoch: 3 Batch: 100 out of 592 Training Loss: 0.000729216361292687 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 101 out of 592 Training Loss: 0.011215925315269317 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 102 out of 592 Training Loss: 0.027212930941828577 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 103 out of 592 Training Loss: 0.05147416340733418 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 104 out of 592 Training Loss: 0.0623353422674025 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 105 out of 592 Training Loss: 0.10095790410960565 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 106 out of 592 Training Loss: 0.11666085208381066 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 107 out of 592 Training Loss: 0.13167005880350718 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 108 out of 592 Training Loss: 0.1481099157648767 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 109 out of 592 Training Loss: 0.15848474493796477 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 110 out of 592 Training Loss: 0.17431938751215587 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 111 out of 592 Training Loss: 0.17818953083442995 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 112 out of 592 Training Loss: 0.2043905908762152 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 113 out of 592 Training Loss: 0.2316398861406977 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 114 out of 592 Training Loss: 0.2484970948247845 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 115 out of 592 Training Loss: 0.2715583125649626 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 116 out of 592 Training Loss: 0.28775831671046564 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 117 out of 592 Training Loss: 0.2929020937121148 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 118 out of 592 Training Loss: 0.30289731673511217 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 119 out of 592 Training Loss: 0.32896507843943784 Test Loss: 0.017414072143958147\n",
      "Epoch: 3 Batch: 120 out of 592 Training Loss: 0.0005717981200147109 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 121 out of 592 Training Loss: 0.024964363832347437 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 122 out of 592 Training Loss: 0.04360279213296656 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 123 out of 592 Training Loss: 0.10278417687522654 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 124 out of 592 Training Loss: 0.1082391418110061 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 125 out of 592 Training Loss: 0.12876864295112375 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 126 out of 592 Training Loss: 0.1385355904291797 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 127 out of 592 Training Loss: 0.15773738693105463 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 128 out of 592 Training Loss: 0.18902202095495943 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 129 out of 592 Training Loss: 0.208842876885407 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 130 out of 592 Training Loss: 0.21508466224776987 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 131 out of 592 Training Loss: 0.24919456015931848 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 132 out of 592 Training Loss: 0.271618021238916 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 133 out of 592 Training Loss: 0.28072433132337815 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 134 out of 592 Training Loss: 0.2961490131508041 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 135 out of 592 Training Loss: 0.3005262935127545 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 136 out of 592 Training Loss: 0.31911069020288474 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 137 out of 592 Training Loss: 0.34592822245913035 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 138 out of 592 Training Loss: 0.369444652047329 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 139 out of 592 Training Loss: 0.38087611511009223 Test Loss: 0.017355518621942876\n",
      "Epoch: 3 Batch: 140 out of 592 Training Loss: 0.000652784398354729 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 141 out of 592 Training Loss: 0.013491121037461003 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 142 out of 592 Training Loss: 0.03438721309468909 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 143 out of 592 Training Loss: 0.04307407691196366 Test Loss: 0.017432668491506553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 144 out of 592 Training Loss: 0.059791281736053664 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 145 out of 592 Training Loss: 0.06941584303096696 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 146 out of 592 Training Loss: 0.08780767015234395 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 147 out of 592 Training Loss: 0.10889629266277714 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 148 out of 592 Training Loss: 0.14533789820090695 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 149 out of 592 Training Loss: 0.15311491667345925 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 150 out of 592 Training Loss: 0.1892171110124962 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 151 out of 592 Training Loss: 0.19800649109349414 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 152 out of 592 Training Loss: 0.22970848801598712 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 153 out of 592 Training Loss: 0.24717761288449927 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 154 out of 592 Training Loss: 0.27272568355367344 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 155 out of 592 Training Loss: 0.295234457506396 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 156 out of 592 Training Loss: 0.3052805735828058 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 157 out of 592 Training Loss: 0.3150476663471834 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 158 out of 592 Training Loss: 0.32937111138419073 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 159 out of 592 Training Loss: 0.37149112104491155 Test Loss: 0.017432668491506553\n",
      "Epoch: 3 Batch: 160 out of 592 Training Loss: 0.00064406238916432 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 161 out of 592 Training Loss: 0.021949843246965757 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 162 out of 592 Training Loss: 0.032533671450345865 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 163 out of 592 Training Loss: 0.08346319918009554 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 164 out of 592 Training Loss: 0.09600359734567915 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 165 out of 592 Training Loss: 0.10260458730581318 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 166 out of 592 Training Loss: 0.1111053868425182 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 167 out of 592 Training Loss: 0.11205675264309262 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 168 out of 592 Training Loss: 0.1463091540844283 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 169 out of 592 Training Loss: 0.1626155934990487 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 170 out of 592 Training Loss: 0.21243074161420678 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 171 out of 592 Training Loss: 0.24450938773761605 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 172 out of 592 Training Loss: 0.24970666270191882 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 173 out of 592 Training Loss: 0.2739938306384823 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 174 out of 592 Training Loss: 0.2954117632233641 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 175 out of 592 Training Loss: 0.3107288717054865 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 176 out of 592 Training Loss: 0.3517815558933756 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 177 out of 592 Training Loss: 0.361102390544965 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 178 out of 592 Training Loss: 0.38468391026790355 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 179 out of 592 Training Loss: 0.4026260464214823 Test Loss: 0.017380494402128865\n",
      "Epoch: 3 Batch: 180 out of 592 Training Loss: 0.0008015251187898859 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 181 out of 592 Training Loss: 0.009068850467941973 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 182 out of 592 Training Loss: 0.06806085262729905 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 183 out of 592 Training Loss: 0.09238995988204263 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 184 out of 592 Training Loss: 0.11363124292626164 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 185 out of 592 Training Loss: 0.12240075137748502 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 186 out of 592 Training Loss: 0.15795393105878613 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 187 out of 592 Training Loss: 0.16575040579124473 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 188 out of 592 Training Loss: 0.17826925128980659 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 189 out of 592 Training Loss: 0.20784166179879687 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 190 out of 592 Training Loss: 0.24173278458698771 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 191 out of 592 Training Loss: 0.2578764958707907 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 192 out of 592 Training Loss: 0.26674504582121156 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 193 out of 592 Training Loss: 0.2931090984894254 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 194 out of 592 Training Loss: 0.2990869409805455 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 195 out of 592 Training Loss: 0.322731941512815 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 196 out of 592 Training Loss: 0.37844738321661137 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 197 out of 592 Training Loss: 0.38507192958428643 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 198 out of 592 Training Loss: 0.38845543567373536 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 199 out of 592 Training Loss: 0.4165452551497438 Test Loss: 0.01736047625485243\n",
      "Epoch: 3 Batch: 200 out of 592 Training Loss: 0.000726677181669623 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 201 out of 592 Training Loss: 0.027804381849714666 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 202 out of 592 Training Loss: 0.045020337852188494 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 203 out of 592 Training Loss: 0.06096008949215069 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 204 out of 592 Training Loss: 0.10286212869221821 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 205 out of 592 Training Loss: 0.11666477721417799 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 206 out of 592 Training Loss: 0.13272761617089168 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 207 out of 592 Training Loss: 0.13891588727245585 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 208 out of 592 Training Loss: 0.15417188516119973 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 209 out of 592 Training Loss: 0.18422899877885834 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 210 out of 592 Training Loss: 0.20540731324116246 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 211 out of 592 Training Loss: 0.21803748065839068 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 212 out of 592 Training Loss: 0.2366620229394223 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 213 out of 592 Training Loss: 0.2821837926120545 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 214 out of 592 Training Loss: 0.2970605838880922 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 215 out of 592 Training Loss: 0.31106806563446776 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 216 out of 592 Training Loss: 0.3371556402952458 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 217 out of 592 Training Loss: 0.3479766176850583 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 218 out of 592 Training Loss: 0.41347660141299025 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 219 out of 592 Training Loss: 0.44248415360460536 Test Loss: 0.017676810836278353\n",
      "Epoch: 3 Batch: 220 out of 592 Training Loss: 0.0007946916286105283 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 221 out of 592 Training Loss: 0.01681805927196685 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 222 out of 592 Training Loss: 0.06091685000697318 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 223 out of 592 Training Loss: 0.06870013302082363 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 224 out of 592 Training Loss: 0.0830736822810227 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 225 out of 592 Training Loss: 0.1296861163702065 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 226 out of 592 Training Loss: 0.15829623410368504 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 227 out of 592 Training Loss: 0.17720588775241913 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 228 out of 592 Training Loss: 0.20896615194047036 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 229 out of 592 Training Loss: 0.23323650950456204 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 230 out of 592 Training Loss: 0.2505346938397938 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 231 out of 592 Training Loss: 0.27606024326646866 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 232 out of 592 Training Loss: 0.2967907219555432 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 233 out of 592 Training Loss: 0.30991372572148385 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 234 out of 592 Training Loss: 0.315975501796996 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 235 out of 592 Training Loss: 0.3177772046894038 Test Loss: 0.01720755296992138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 236 out of 592 Training Loss: 0.3310420442268455 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 237 out of 592 Training Loss: 0.3618643324718082 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 238 out of 592 Training Loss: 0.3878296822205389 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 239 out of 592 Training Loss: 0.3946244240634346 Test Loss: 0.01720755296992138\n",
      "Epoch: 3 Batch: 240 out of 592 Training Loss: 0.0006913661489376788 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 241 out of 592 Training Loss: 0.011068070074368453 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 242 out of 592 Training Loss: 0.02137667662446592 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 243 out of 592 Training Loss: 0.026184873422432878 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 244 out of 592 Training Loss: 0.04346322431211565 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 245 out of 592 Training Loss: 0.05374421062861774 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 246 out of 592 Training Loss: 0.05835111648758147 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 247 out of 592 Training Loss: 0.09076599241216872 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 248 out of 592 Training Loss: 0.10477798455377314 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 249 out of 592 Training Loss: 0.12351865717192385 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 250 out of 592 Training Loss: 0.13967409753223153 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 251 out of 592 Training Loss: 0.16545202606340143 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 252 out of 592 Training Loss: 0.18326074280758592 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 253 out of 592 Training Loss: 0.19695973047037812 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 254 out of 592 Training Loss: 0.22530723572691175 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 255 out of 592 Training Loss: 0.2407187356146166 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 256 out of 592 Training Loss: 0.2925674228461573 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 257 out of 592 Training Loss: 0.311141528421838 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 258 out of 592 Training Loss: 0.3235999841706941 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 259 out of 592 Training Loss: 0.33508111373503896 Test Loss: 0.017539807344843266\n",
      "Epoch: 3 Batch: 260 out of 592 Training Loss: 0.0006265009574847161 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 261 out of 592 Training Loss: 0.025082903244371884 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 262 out of 592 Training Loss: 0.04369833719062376 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 263 out of 592 Training Loss: 0.06875613730477857 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 264 out of 592 Training Loss: 0.07945913940745163 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 265 out of 592 Training Loss: 0.10545314036386776 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 266 out of 592 Training Loss: 0.14647225223558713 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 267 out of 592 Training Loss: 0.15995474744665195 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 268 out of 592 Training Loss: 0.17547389052885343 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 269 out of 592 Training Loss: 0.2680884965516882 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 270 out of 592 Training Loss: 0.28238286942320634 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 271 out of 592 Training Loss: 0.2944556812577205 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 272 out of 592 Training Loss: 0.30920110669064094 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 273 out of 592 Training Loss: 0.3394081539295869 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 274 out of 592 Training Loss: 0.3838177903376775 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 275 out of 592 Training Loss: 0.4175249061368661 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 276 out of 592 Training Loss: 0.4452143537633376 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 277 out of 592 Training Loss: 0.4541861585162835 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 278 out of 592 Training Loss: 0.4666947669171052 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 279 out of 592 Training Loss: 0.4790926820419984 Test Loss: 0.017827474224649257\n",
      "Epoch: 3 Batch: 280 out of 592 Training Loss: 0.0008135623369076897 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 281 out of 592 Training Loss: 0.036877944979415246 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 282 out of 592 Training Loss: 0.04307941059355528 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 283 out of 592 Training Loss: 0.04637321821187527 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 284 out of 592 Training Loss: 0.09144835165475398 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 285 out of 592 Training Loss: 0.1277724727127412 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 286 out of 592 Training Loss: 0.1564610208455184 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 287 out of 592 Training Loss: 0.1650927885506013 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 288 out of 592 Training Loss: 0.18093777811859638 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 289 out of 592 Training Loss: 0.2132781370613435 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 290 out of 592 Training Loss: 0.22106271507923395 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 291 out of 592 Training Loss: 0.24515990066115648 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 292 out of 592 Training Loss: 0.26625437932793883 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 293 out of 592 Training Loss: 0.31033353405778197 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 294 out of 592 Training Loss: 0.33594011599902895 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 295 out of 592 Training Loss: 0.38936136002425936 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 296 out of 592 Training Loss: 0.41418829458539275 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 297 out of 592 Training Loss: 0.43353886055533675 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 298 out of 592 Training Loss: 0.46451743031625536 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 299 out of 592 Training Loss: 0.46884952476953057 Test Loss: 0.01718534679843508\n",
      "Epoch: 3 Batch: 300 out of 592 Training Loss: 0.0008165644373347796 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 301 out of 592 Training Loss: 0.031891317468469384 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 302 out of 592 Training Loss: 0.050877488579933884 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 303 out of 592 Training Loss: 0.07819278667527937 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 304 out of 592 Training Loss: 0.08352017205986881 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 305 out of 592 Training Loss: 0.09868477926436567 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 306 out of 592 Training Loss: 0.1010228250067284 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 307 out of 592 Training Loss: 0.11520890089530253 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 308 out of 592 Training Loss: 0.12950975111532712 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 309 out of 592 Training Loss: 0.14625248721647763 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 310 out of 592 Training Loss: 0.2123316120247891 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 311 out of 592 Training Loss: 0.23009589909422898 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 312 out of 592 Training Loss: 0.24634129776466393 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 313 out of 592 Training Loss: 0.27107338084686305 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 314 out of 592 Training Loss: 0.28351691122013833 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 315 out of 592 Training Loss: 0.2931646941121271 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 316 out of 592 Training Loss: 0.32503606880862024 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 317 out of 592 Training Loss: 0.36601656804639604 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 318 out of 592 Training Loss: 0.4018069853897741 Test Loss: 0.017233975984819346\n",
      "Epoch: 3 Batch: 319 out of 592 Training Loss: 0.4151179114248207 Test Loss: 0.017233975984819346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 320 out of 592 Training Loss: 0.0007500005184137221 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 321 out of 592 Training Loss: 0.004879110422583759 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 322 out of 592 Training Loss: 0.018824919056745708 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 323 out of 592 Training Loss: 0.057961082947107494 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 324 out of 592 Training Loss: 0.06946098518058891 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 325 out of 592 Training Loss: 0.08894319918439025 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 326 out of 592 Training Loss: 0.09572548458785052 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 327 out of 592 Training Loss: 0.10585490188062424 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 328 out of 592 Training Loss: 0.12470961494803662 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 329 out of 592 Training Loss: 0.1359547306081676 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 330 out of 592 Training Loss: 0.1614028349748277 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 331 out of 592 Training Loss: 0.18310591618031496 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 332 out of 592 Training Loss: 0.2006343309065723 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 333 out of 592 Training Loss: 0.23302183339804644 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 334 out of 592 Training Loss: 0.24047109011993045 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 335 out of 592 Training Loss: 0.259298221137973 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 336 out of 592 Training Loss: 0.2666801157636964 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 337 out of 592 Training Loss: 0.27780968409463996 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 338 out of 592 Training Loss: 0.285140013252052 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 339 out of 592 Training Loss: 0.29004522685393924 Test Loss: 0.01711093500489369\n",
      "Epoch: 3 Batch: 340 out of 592 Training Loss: 0.0005299265045317786 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 341 out of 592 Training Loss: 0.018742028858104623 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 342 out of 592 Training Loss: 0.023743201747128642 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 343 out of 592 Training Loss: 0.06701731006760327 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 344 out of 592 Training Loss: 0.11758161033649174 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 345 out of 592 Training Loss: 0.12482821871672002 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 346 out of 592 Training Loss: 0.13877392397407617 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 347 out of 592 Training Loss: 0.17021551192883577 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 348 out of 592 Training Loss: 0.18605766655091371 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 349 out of 592 Training Loss: 0.19839522731635895 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 350 out of 592 Training Loss: 0.20342890056926932 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 351 out of 592 Training Loss: 0.22395860101241793 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 352 out of 592 Training Loss: 0.23551366548199382 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 353 out of 592 Training Loss: 0.25542833539802756 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 354 out of 592 Training Loss: 0.28960026311833587 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 355 out of 592 Training Loss: 0.30176651920337405 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 356 out of 592 Training Loss: 0.32385659660358157 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 357 out of 592 Training Loss: 0.33679768233705487 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 358 out of 592 Training Loss: 0.34582026838470425 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 359 out of 592 Training Loss: 0.3828607925694081 Test Loss: 0.017087521105581385\n",
      "Epoch: 3 Batch: 360 out of 592 Training Loss: 0.0006942985437023232 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 361 out of 592 Training Loss: 0.009471795879970749 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 362 out of 592 Training Loss: 0.028888457813035208 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 363 out of 592 Training Loss: 0.08885317424751396 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 364 out of 592 Training Loss: 0.1018289917937624 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 365 out of 592 Training Loss: 0.11402688676930542 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 366 out of 592 Training Loss: 0.13076501841164703 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 367 out of 592 Training Loss: 0.15777408460594292 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 368 out of 592 Training Loss: 0.1842287419221508 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 369 out of 592 Training Loss: 0.22248845676399345 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 370 out of 592 Training Loss: 0.23332276324249382 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 371 out of 592 Training Loss: 0.259057483994137 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 372 out of 592 Training Loss: 0.2671265764019596 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 373 out of 592 Training Loss: 0.2852660151056158 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 374 out of 592 Training Loss: 0.31684448885239236 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 375 out of 592 Training Loss: 0.3538426415852415 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 376 out of 592 Training Loss: 0.3651793882108437 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 377 out of 592 Training Loss: 0.3752045569799053 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 378 out of 592 Training Loss: 0.39143888595021836 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 379 out of 592 Training Loss: 0.40561778972364537 Test Loss: 0.017348616440413578\n",
      "Epoch: 3 Batch: 380 out of 592 Training Loss: 0.0007466668523985167 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 381 out of 592 Training Loss: 0.02581403909997523 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 382 out of 592 Training Loss: 0.04055503143744052 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 383 out of 592 Training Loss: 0.05642356484012187 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 384 out of 592 Training Loss: 0.06927382892744124 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 385 out of 592 Training Loss: 0.08940473145620406 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 386 out of 592 Training Loss: 0.09126371196751923 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 387 out of 592 Training Loss: 0.11256059683208794 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 388 out of 592 Training Loss: 0.13201214424615235 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 389 out of 592 Training Loss: 0.1496137702400002 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 390 out of 592 Training Loss: 0.15626284717862934 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 391 out of 592 Training Loss: 0.17734857886379093 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 392 out of 592 Training Loss: 0.19051635503118367 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 393 out of 592 Training Loss: 0.2148214397514376 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 394 out of 592 Training Loss: 0.22044238313382 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 395 out of 592 Training Loss: 0.2844007150763783 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 396 out of 592 Training Loss: 0.3082978283548865 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 397 out of 592 Training Loss: 0.31478909687419865 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 398 out of 592 Training Loss: 0.3399440954001758 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 399 out of 592 Training Loss: 0.3520937711494181 Test Loss: 0.017282824435934537\n",
      "Epoch: 3 Batch: 400 out of 592 Training Loss: 0.0006355768372509483 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 401 out of 592 Training Loss: 0.02853157132167554 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 402 out of 592 Training Loss: 0.050791244425365686 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 403 out of 592 Training Loss: 0.05671369836900568 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 404 out of 592 Training Loss: 0.06554147874240017 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 405 out of 592 Training Loss: 0.07714644440684652 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 406 out of 592 Training Loss: 0.08415131264601088 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 407 out of 592 Training Loss: 0.09686275379035807 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 408 out of 592 Training Loss: 0.11892584332499838 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 409 out of 592 Training Loss: 0.12171209248017704 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 410 out of 592 Training Loss: 0.13389751332115568 Test Loss: 0.017029891085147745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 411 out of 592 Training Loss: 0.1608218123973552 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 412 out of 592 Training Loss: 0.18458637798439897 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 413 out of 592 Training Loss: 0.20038855837892927 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 414 out of 592 Training Loss: 0.22613435069333948 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 415 out of 592 Training Loss: 0.2365272028208915 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 416 out of 592 Training Loss: 0.2553111428232137 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 417 out of 592 Training Loss: 0.2644971536309663 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 418 out of 592 Training Loss: 0.2806907949806396 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 419 out of 592 Training Loss: 0.301839769056553 Test Loss: 0.017029891085147745\n",
      "Epoch: 3 Batch: 420 out of 592 Training Loss: 0.0005713546387198714 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 421 out of 592 Training Loss: 0.018607265662336663 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 422 out of 592 Training Loss: 0.029870536524737194 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 423 out of 592 Training Loss: 0.03989304450520308 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 424 out of 592 Training Loss: 0.05612222512432368 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 425 out of 592 Training Loss: 0.06304445571445855 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 426 out of 592 Training Loss: 0.10381006903194817 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 427 out of 592 Training Loss: 0.15647671452903184 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 428 out of 592 Training Loss: 0.16582294541590842 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 429 out of 592 Training Loss: 0.18116321775310668 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 430 out of 592 Training Loss: 0.18338952806957992 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 431 out of 592 Training Loss: 0.1902227912768069 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 432 out of 592 Training Loss: 0.22056744701408776 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 433 out of 592 Training Loss: 0.2625517965032998 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 434 out of 592 Training Loss: 0.2668126815668586 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 435 out of 592 Training Loss: 0.2756559479720238 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 436 out of 592 Training Loss: 0.29441950877376827 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 437 out of 592 Training Loss: 0.2995644648297969 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 438 out of 592 Training Loss: 0.30785728140913876 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 439 out of 592 Training Loss: 0.3109207603819016 Test Loss: 0.01705256780736487\n",
      "Epoch: 3 Batch: 440 out of 592 Training Loss: 0.0005582534695055643 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 441 out of 592 Training Loss: 0.01603895621243979 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 442 out of 592 Training Loss: 0.023850109519996898 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 443 out of 592 Training Loss: 0.033148986379244585 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 444 out of 592 Training Loss: 0.05762983774636056 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 445 out of 592 Training Loss: 0.08280772818122174 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 446 out of 592 Training Loss: 0.1017324953326967 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 447 out of 592 Training Loss: 0.11242490457240607 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 448 out of 592 Training Loss: 0.11974999690298106 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 449 out of 592 Training Loss: 0.12329279431011643 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 450 out of 592 Training Loss: 0.18016370930578673 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 451 out of 592 Training Loss: 0.18450227132868136 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 452 out of 592 Training Loss: 0.21487096263896788 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 453 out of 592 Training Loss: 0.2424817824257549 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 454 out of 592 Training Loss: 0.2442707702038821 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 455 out of 592 Training Loss: 0.2536231282619771 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 456 out of 592 Training Loss: 0.2683165011448797 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 457 out of 592 Training Loss: 0.3282702577753004 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 458 out of 592 Training Loss: 0.36995738125275934 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 459 out of 592 Training Loss: 0.3788998838035401 Test Loss: 0.017018687059383163\n",
      "Epoch: 3 Batch: 460 out of 592 Training Loss: 0.0006645635732370951 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 461 out of 592 Training Loss: 0.016776982418533003 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 462 out of 592 Training Loss: 0.02778351465546671 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 463 out of 592 Training Loss: 0.04484582851016108 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 464 out of 592 Training Loss: 0.05357374968313757 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 465 out of 592 Training Loss: 0.13600296499514167 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 466 out of 592 Training Loss: 0.14896636801147048 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 467 out of 592 Training Loss: 0.15640024673500244 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 468 out of 592 Training Loss: 0.16746719666131918 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 469 out of 592 Training Loss: 0.1776053311333913 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 470 out of 592 Training Loss: 0.1984931418702859 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 471 out of 592 Training Loss: 0.23471638951757137 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 472 out of 592 Training Loss: 0.26017842460730256 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 473 out of 592 Training Loss: 0.27178494092383565 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 474 out of 592 Training Loss: 0.27737842286312164 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 475 out of 592 Training Loss: 0.285652722887035 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 476 out of 592 Training Loss: 0.29140282720633925 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 477 out of 592 Training Loss: 0.2937742259384054 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 478 out of 592 Training Loss: 0.37885545306214274 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 479 out of 592 Training Loss: 0.4345043461800474 Test Loss: 0.016996130936383975\n",
      "Epoch: 3 Batch: 480 out of 592 Training Loss: 0.0007670744832002463 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 481 out of 592 Training Loss: 0.01328316233561867 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 482 out of 592 Training Loss: 0.02430890682594412 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 483 out of 592 Training Loss: 0.07793301525966757 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 484 out of 592 Training Loss: 0.10303978058950537 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 485 out of 592 Training Loss: 0.11242114934579962 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 486 out of 592 Training Loss: 0.12680660090850226 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 487 out of 592 Training Loss: 0.16156208417819373 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 488 out of 592 Training Loss: 0.19755149535344474 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 489 out of 592 Training Loss: 0.2056539728815996 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 490 out of 592 Training Loss: 0.21685325070725314 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 491 out of 592 Training Loss: 0.25181278302775256 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 492 out of 592 Training Loss: 0.26632210142420165 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 493 out of 592 Training Loss: 0.27851617797718875 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 494 out of 592 Training Loss: 0.27993022588057004 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 495 out of 592 Training Loss: 0.31740483206672154 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 496 out of 592 Training Loss: 0.3834442080275484 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 497 out of 592 Training Loss: 0.3889931797326871 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 498 out of 592 Training Loss: 0.39435872093749724 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 499 out of 592 Training Loss: 0.4058545160384246 Test Loss: 0.016972188061724108\n",
      "Epoch: 3 Batch: 500 out of 592 Training Loss: 0.0007084368991290107 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 501 out of 592 Training Loss: 0.06359510713094817 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 502 out of 592 Training Loss: 0.07697496593767748 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 503 out of 592 Training Loss: 0.1049094016926919 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 504 out of 592 Training Loss: 0.11009729963565217 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 505 out of 592 Training Loss: 0.13453739818954813 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 506 out of 592 Training Loss: 0.1886143976154362 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 507 out of 592 Training Loss: 0.2004540696072017 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 508 out of 592 Training Loss: 0.2289242579864894 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 509 out of 592 Training Loss: 0.24974471174949753 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 510 out of 592 Training Loss: 0.2678815986292993 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 511 out of 592 Training Loss: 0.30022888661200153 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 512 out of 592 Training Loss: 0.3386409546511804 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 513 out of 592 Training Loss: 0.4067798297184144 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 514 out of 592 Training Loss: 0.425484391235057 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 515 out of 592 Training Loss: 0.4491185083317195 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 516 out of 592 Training Loss: 0.4627626112925445 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 517 out of 592 Training Loss: 0.47742073920393335 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 518 out of 592 Training Loss: 0.5177270801069771 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 519 out of 592 Training Loss: 0.5395742257388865 Test Loss: 0.017698310068667386\n",
      "Epoch: 3 Batch: 520 out of 592 Training Loss: 0.000922046987717826 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 521 out of 592 Training Loss: 0.03456845173091241 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 522 out of 592 Training Loss: 0.04140708401278206 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 523 out of 592 Training Loss: 0.06765109851435372 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 524 out of 592 Training Loss: 0.08916012189761349 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 525 out of 592 Training Loss: 0.1552362590231056 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 526 out of 592 Training Loss: 0.18715694911256978 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 527 out of 592 Training Loss: 0.22808489195481488 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 528 out of 592 Training Loss: 0.25152440547422117 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 529 out of 592 Training Loss: 0.2538190071139539 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 530 out of 592 Training Loss: 0.2646928221587146 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 531 out of 592 Training Loss: 0.27470231397487765 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 532 out of 592 Training Loss: 0.28306151895262843 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 533 out of 592 Training Loss: 0.29931004500426894 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 534 out of 592 Training Loss: 0.31730653314330226 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 535 out of 592 Training Loss: 0.3358374439779723 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 536 out of 592 Training Loss: 0.35721084257998115 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 537 out of 592 Training Loss: 0.37784554964341766 Test Loss: 0.01699497662907974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 538 out of 592 Training Loss: 0.39350438831784373 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 539 out of 592 Training Loss: 0.42817185787894374 Test Loss: 0.01699497662907974\n",
      "Epoch: 3 Batch: 540 out of 592 Training Loss: 0.0007610283849666591 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 541 out of 592 Training Loss: 0.012544721370739746 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 542 out of 592 Training Loss: 0.04539793357829552 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 543 out of 592 Training Loss: 0.05613501184265118 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 544 out of 592 Training Loss: 0.07385355979780656 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 545 out of 592 Training Loss: 0.09623462707380753 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 546 out of 592 Training Loss: 0.10287900036777359 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 547 out of 592 Training Loss: 0.14064897364581924 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 548 out of 592 Training Loss: 0.15151131107117038 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 549 out of 592 Training Loss: 0.20087141497637134 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 550 out of 592 Training Loss: 0.21258335048909288 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 551 out of 592 Training Loss: 0.22197087621892791 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 552 out of 592 Training Loss: 0.23805443710292679 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 553 out of 592 Training Loss: 0.27106027072872024 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 554 out of 592 Training Loss: 0.293159225495709 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 555 out of 592 Training Loss: 0.3139821714779268 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 556 out of 592 Training Loss: 0.3444438236823211 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 557 out of 592 Training Loss: 0.3904934595396648 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 558 out of 592 Training Loss: 0.4135372093936334 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 559 out of 592 Training Loss: 0.4225639156123529 Test Loss: 0.016914272982500155\n",
      "Epoch: 3 Batch: 560 out of 592 Training Loss: 0.0007426593967219278 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 561 out of 592 Training Loss: 0.009346141903891174 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 562 out of 592 Training Loss: 0.014570366083874313 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 563 out of 592 Training Loss: 0.030678916215195265 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 564 out of 592 Training Loss: 0.03552127671916684 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 565 out of 592 Training Loss: 0.03905479326401552 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 566 out of 592 Training Loss: 0.0832852283075889 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 567 out of 592 Training Loss: 0.09157433449779352 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 568 out of 592 Training Loss: 0.09829817475308379 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 569 out of 592 Training Loss: 0.10836303090234002 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 570 out of 592 Training Loss: 0.1145989307716926 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 571 out of 592 Training Loss: 0.1297949412420829 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 572 out of 592 Training Loss: 0.1352615910165449 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 573 out of 592 Training Loss: 0.1436673714362522 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 574 out of 592 Training Loss: 0.14806901705939968 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 575 out of 592 Training Loss: 0.1512186484925886 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 576 out of 592 Training Loss: 0.1604991028599878 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 577 out of 592 Training Loss: 0.18206112036038835 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 578 out of 592 Training Loss: 0.19384693676163156 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 579 out of 592 Training Loss: 0.19960185694996913 Test Loss: 0.017215984705906136\n",
      "Epoch: 3 Batch: 580 out of 592 Training Loss: 0.0003557618819983357 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 581 out of 592 Training Loss: 0.026612699415615355 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 582 out of 592 Training Loss: 0.03924070150785759 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 583 out of 592 Training Loss: 0.05770488308005646 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 584 out of 592 Training Loss: 0.10030742214255646 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 585 out of 592 Training Loss: 0.10975391176723555 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 586 out of 592 Training Loss: 0.11125417255059496 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 587 out of 592 Training Loss: 0.13701275006130947 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 588 out of 592 Training Loss: 0.16731656916633858 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 589 out of 592 Training Loss: 0.17752453420505537 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 590 out of 592 Training Loss: 0.18783624299064888 Test Loss: 0.01712979842859292\n",
      "Epoch: 3 Batch: 591 out of 592 Training Loss: 0.20848959699467434 Test Loss: 0.01712979842859292\n",
      "Epoch: 4 Batch: 0 out of 592 Training Loss: 2.012579431253913e-05 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 1 out of 592 Training Loss: 0.0167049801974507 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 2 out of 592 Training Loss: 0.039123330178758686 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 3 out of 592 Training Loss: 0.07133744648387152 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 4 out of 592 Training Loss: 0.09479883118262011 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 5 out of 592 Training Loss: 0.12774013160457331 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 6 out of 592 Training Loss: 0.15274962960471827 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 7 out of 592 Training Loss: 0.18095646976222712 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 8 out of 592 Training Loss: 0.21082543759336192 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 9 out of 592 Training Loss: 0.23969805604507644 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 10 out of 592 Training Loss: 0.2506979935868593 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 11 out of 592 Training Loss: 0.2625778875379773 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 12 out of 592 Training Loss: 0.2810165396957608 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 13 out of 592 Training Loss: 0.3005031253277274 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 14 out of 592 Training Loss: 0.310506476934156 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 15 out of 592 Training Loss: 0.33404621566375214 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 16 out of 592 Training Loss: 0.35108037377437074 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 17 out of 592 Training Loss: 0.3587127741492303 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 18 out of 592 Training Loss: 0.3707317508823188 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 19 out of 592 Training Loss: 0.43970428631370384 Test Loss: 0.016873236645055426\n",
      "Epoch: 4 Batch: 20 out of 592 Training Loss: 0.0007732127629634147 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 21 out of 592 Training Loss: 0.02432937629748547 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 22 out of 592 Training Loss: 0.033076683881175156 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 23 out of 592 Training Loss: 0.04864620499540531 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 24 out of 592 Training Loss: 0.06459241256345474 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 25 out of 592 Training Loss: 0.07881997421611511 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 26 out of 592 Training Loss: 0.09125880327303373 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 27 out of 592 Training Loss: 0.1128615810372349 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 28 out of 592 Training Loss: 0.11644135893550181 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 29 out of 592 Training Loss: 0.12826859870221877 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 30 out of 592 Training Loss: 0.2110670796897676 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 31 out of 592 Training Loss: 0.24466602050926947 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 32 out of 592 Training Loss: 0.2848594478633668 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 33 out of 592 Training Loss: 0.3096194557216432 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 34 out of 592 Training Loss: 0.3336707848426368 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 35 out of 592 Training Loss: 0.3492190056782629 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 36 out of 592 Training Loss: 0.37595451263007185 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 37 out of 592 Training Loss: 0.3938276787590649 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 38 out of 592 Training Loss: 0.4079721049707796 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 39 out of 592 Training Loss: 0.4290687331300642 Test Loss: 0.01701103138850268\n",
      "Epoch: 4 Batch: 40 out of 592 Training Loss: 0.0007531200663533407 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 41 out of 592 Training Loss: 0.02118621814412739 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 42 out of 592 Training Loss: 0.03753495264215138 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 43 out of 592 Training Loss: 0.04560001894248392 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 44 out of 592 Training Loss: 0.05888060621333983 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 45 out of 592 Training Loss: 0.0706126787827697 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 46 out of 592 Training Loss: 0.08397825482649471 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 47 out of 592 Training Loss: 0.1801190865354028 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 48 out of 592 Training Loss: 0.18290287164387967 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 49 out of 592 Training Loss: 0.2041822625452068 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 50 out of 592 Training Loss: 0.2152215391525414 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 51 out of 592 Training Loss: 0.21893918562097217 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 52 out of 592 Training Loss: 0.23149066931916143 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 53 out of 592 Training Loss: 0.25496143988681697 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 54 out of 592 Training Loss: 0.27487919806850813 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 55 out of 592 Training Loss: 0.2813677615614023 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 56 out of 592 Training Loss: 0.2876676162832763 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 57 out of 592 Training Loss: 0.29191360065130256 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 58 out of 592 Training Loss: 0.30862746575264 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 59 out of 592 Training Loss: 0.31713533402768634 Test Loss: 0.016885417351273423\n",
      "Epoch: 4 Batch: 60 out of 592 Training Loss: 0.0005541769253004084 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 61 out of 592 Training Loss: 0.004065770602403928 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 62 out of 592 Training Loss: 0.03144596325474863 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 63 out of 592 Training Loss: 0.07849794807153826 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 64 out of 592 Training Loss: 0.09474662127214556 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 65 out of 592 Training Loss: 0.09722182591687029 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 66 out of 592 Training Loss: 0.1025258411584336 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 67 out of 592 Training Loss: 0.11853445665220802 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 68 out of 592 Training Loss: 0.14130613737980907 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 69 out of 592 Training Loss: 0.16239447863082473 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 70 out of 592 Training Loss: 0.17939859972576683 Test Loss: 0.016876706419597296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 71 out of 592 Training Loss: 0.18518533389325326 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 72 out of 592 Training Loss: 0.19009297359044736 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 73 out of 592 Training Loss: 0.22211804631407445 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 74 out of 592 Training Loss: 0.24015866796727364 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 75 out of 592 Training Loss: 0.2565771454876203 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 76 out of 592 Training Loss: 0.28963217157180493 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 77 out of 592 Training Loss: 0.2963681269368071 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 78 out of 592 Training Loss: 0.32703846990372126 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 79 out of 592 Training Loss: 0.3437721181591887 Test Loss: 0.016876706419597296\n",
      "Epoch: 4 Batch: 80 out of 592 Training Loss: 0.0005975273327558861 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 81 out of 592 Training Loss: 0.019401246763917766 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 82 out of 592 Training Loss: 0.034950945608469806 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 83 out of 592 Training Loss: 0.06139279723927673 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 84 out of 592 Training Loss: 0.0902716108487576 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 85 out of 592 Training Loss: 0.10624199555084404 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 86 out of 592 Training Loss: 0.13577791870400605 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 87 out of 592 Training Loss: 0.14237541992977557 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 88 out of 592 Training Loss: 0.1719530197681516 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 89 out of 592 Training Loss: 0.2131730849744409 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 90 out of 592 Training Loss: 0.22494766590014395 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 91 out of 592 Training Loss: 0.22881488230839667 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 92 out of 592 Training Loss: 0.23653073058292565 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 93 out of 592 Training Loss: 0.2562657427863806 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 94 out of 592 Training Loss: 0.2600465806556522 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 95 out of 592 Training Loss: 0.2671631424111902 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 96 out of 592 Training Loss: 0.2868968116474926 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 97 out of 592 Training Loss: 0.301266664488393 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 98 out of 592 Training Loss: 0.31254083555020684 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 99 out of 592 Training Loss: 0.3212900523451745 Test Loss: 0.018627309427370852\n",
      "Epoch: 4 Batch: 100 out of 592 Training Loss: 0.0005612534930825131 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 101 out of 592 Training Loss: 0.01598199861998557 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 102 out of 592 Training Loss: 0.037497977120232576 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 103 out of 592 Training Loss: 0.05230966894830465 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 104 out of 592 Training Loss: 0.09148025452102422 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 105 out of 592 Training Loss: 0.10860950439180135 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 106 out of 592 Training Loss: 0.11982212993259429 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 107 out of 592 Training Loss: 0.13141658469046352 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 108 out of 592 Training Loss: 0.17227587043250797 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 109 out of 592 Training Loss: 0.19466325393821 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 110 out of 592 Training Loss: 0.212677224864614 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 111 out of 592 Training Loss: 0.22766613154615162 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 112 out of 592 Training Loss: 0.2638614246150374 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 113 out of 592 Training Loss: 0.3077091568609595 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 114 out of 592 Training Loss: 0.31533279768134354 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 115 out of 592 Training Loss: 0.34831746048237083 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 116 out of 592 Training Loss: 0.3647620448371291 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 117 out of 592 Training Loss: 0.37959391190613506 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 118 out of 592 Training Loss: 0.38878722893948553 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 119 out of 592 Training Loss: 0.45366561817879675 Test Loss: 0.017036002512075796\n",
      "Epoch: 4 Batch: 120 out of 592 Training Loss: 0.0007729788522408922 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 121 out of 592 Training Loss: 0.011650274249462935 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 122 out of 592 Training Loss: 0.02597380869343743 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 123 out of 592 Training Loss: 0.03284625590100751 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 124 out of 592 Training Loss: 0.09555955857291684 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 125 out of 592 Training Loss: 0.12880314395084844 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 126 out of 592 Training Loss: 0.1465796890880154 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 127 out of 592 Training Loss: 0.16620769046500192 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 128 out of 592 Training Loss: 0.18339228138580785 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 129 out of 592 Training Loss: 0.1897008476990448 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 130 out of 592 Training Loss: 0.1995062979149805 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 131 out of 592 Training Loss: 0.20884409288197622 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 132 out of 592 Training Loss: 0.2473972972694026 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 133 out of 592 Training Loss: 0.268970283887087 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 134 out of 592 Training Loss: 0.29018504286438096 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 135 out of 592 Training Loss: 0.3179882217410194 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 136 out of 592 Training Loss: 0.34294866325586904 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 137 out of 592 Training Loss: 0.35183066429823506 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 138 out of 592 Training Loss: 0.3774720646652566 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 139 out of 592 Training Loss: 0.42247260662048924 Test Loss: 0.016787622476730383\n",
      "Epoch: 4 Batch: 140 out of 592 Training Loss: 0.0007418207119802947 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 141 out of 592 Training Loss: 0.009937944751570654 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 142 out of 592 Training Loss: 0.03470617618818088 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 143 out of 592 Training Loss: 0.050653777014801935 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 144 out of 592 Training Loss: 0.0674652801122169 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 145 out of 592 Training Loss: 0.07362510350007816 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 146 out of 592 Training Loss: 0.0798059240210454 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 147 out of 592 Training Loss: 0.1392878800857465 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 148 out of 592 Training Loss: 0.1600394811708133 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 149 out of 592 Training Loss: 0.20584377685968086 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 150 out of 592 Training Loss: 0.23734183649007484 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 151 out of 592 Training Loss: 0.24622418677840635 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 152 out of 592 Training Loss: 0.2514480873237948 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 153 out of 592 Training Loss: 0.26309757486392066 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 154 out of 592 Training Loss: 0.2724606440629463 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 155 out of 592 Training Loss: 0.3038501152064304 Test Loss: 0.01681149281406154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 156 out of 592 Training Loss: 0.32992308859188363 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 157 out of 592 Training Loss: 0.36068064619262025 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 158 out of 592 Training Loss: 0.37455656186152503 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 159 out of 592 Training Loss: 0.40592668384719893 Test Loss: 0.01681149281406154\n",
      "Epoch: 4 Batch: 160 out of 592 Training Loss: 0.0007341740702070666 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 161 out of 592 Training Loss: 0.02115587275966782 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 162 out of 592 Training Loss: 0.04167974684878964 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 163 out of 592 Training Loss: 0.048174360213128514 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 164 out of 592 Training Loss: 0.05205430989645202 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 165 out of 592 Training Loss: 0.0656252183534928 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 166 out of 592 Training Loss: 0.07892598552189548 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 167 out of 592 Training Loss: 0.08397572921149213 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 168 out of 592 Training Loss: 0.090029778569264 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 169 out of 592 Training Loss: 0.09718701537258703 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 170 out of 592 Training Loss: 0.11175869233466465 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 171 out of 592 Training Loss: 0.14546048096872646 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 172 out of 592 Training Loss: 0.14964072607017834 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 173 out of 592 Training Loss: 0.1677735616967805 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 174 out of 592 Training Loss: 0.20607477999783833 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 175 out of 592 Training Loss: 0.23166059158302624 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 176 out of 592 Training Loss: 0.24237143262780983 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 177 out of 592 Training Loss: 0.25170886996693687 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 178 out of 592 Training Loss: 0.27894211109287814 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 179 out of 592 Training Loss: 0.31550867404348926 Test Loss: 0.016786807329591476\n",
      "Epoch: 4 Batch: 180 out of 592 Training Loss: 0.0005872044132671256 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 181 out of 592 Training Loss: 0.007061426935496082 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 182 out of 592 Training Loss: 0.028677489084901562 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 183 out of 592 Training Loss: 0.044117590820195424 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 184 out of 592 Training Loss: 0.052401557972433316 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 185 out of 592 Training Loss: 0.08429069857835983 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 186 out of 592 Training Loss: 0.08800415432803546 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 187 out of 592 Training Loss: 0.09889452735685025 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 188 out of 592 Training Loss: 0.11101097444795285 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 189 out of 592 Training Loss: 0.12604308611504947 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 190 out of 592 Training Loss: 0.13089738431178247 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 191 out of 592 Training Loss: 0.14930915477477227 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 192 out of 592 Training Loss: 0.1633511727561823 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 193 out of 592 Training Loss: 0.1851087381949297 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 194 out of 592 Training Loss: 0.20096465403937017 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 195 out of 592 Training Loss: 0.21074295527093326 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 196 out of 592 Training Loss: 0.22864896169655238 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 197 out of 592 Training Loss: 0.23718041664712344 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 198 out of 592 Training Loss: 0.2686472508942357 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 199 out of 592 Training Loss: 0.29400681256108196 Test Loss: 0.016748727343369726\n",
      "Epoch: 4 Batch: 200 out of 592 Training Loss: 0.00050908995025779 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 201 out of 592 Training Loss: 0.012208060521418106 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 202 out of 592 Training Loss: 0.050673981357151524 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 203 out of 592 Training Loss: 0.05392393136561836 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 204 out of 592 Training Loss: 0.08756237516344513 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 205 out of 592 Training Loss: 0.09905459957362141 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 206 out of 592 Training Loss: 0.12658840643406832 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 207 out of 592 Training Loss: 0.14009424245149815 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 208 out of 592 Training Loss: 0.15193062966854298 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 209 out of 592 Training Loss: 0.17167081257493222 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 210 out of 592 Training Loss: 0.17683902887792313 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 211 out of 592 Training Loss: 0.1876123002001735 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 212 out of 592 Training Loss: 0.19899660201908076 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 213 out of 592 Training Loss: 0.22088877523482764 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 214 out of 592 Training Loss: 0.28045613357962096 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 215 out of 592 Training Loss: 0.2872259816357585 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 216 out of 592 Training Loss: 0.3108226521143409 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 217 out of 592 Training Loss: 0.3231709594109746 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 218 out of 592 Training Loss: 0.3261233655938598 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 219 out of 592 Training Loss: 0.3474367087909671 Test Loss: 0.016992651322983544\n",
      "Epoch: 4 Batch: 220 out of 592 Training Loss: 0.0006199760001016987 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 221 out of 592 Training Loss: 0.03391193799784912 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 222 out of 592 Training Loss: 0.051031895131976736 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 223 out of 592 Training Loss: 0.056417028569789064 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 224 out of 592 Training Loss: 0.07328500706812871 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 225 out of 592 Training Loss: 0.09283491890987886 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 226 out of 592 Training Loss: 0.10624767948052419 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 227 out of 592 Training Loss: 0.12772358503303063 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 228 out of 592 Training Loss: 0.1653542510803653 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 229 out of 592 Training Loss: 0.17120677061489356 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 230 out of 592 Training Loss: 0.18999455884209407 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 231 out of 592 Training Loss: 0.21228439562133086 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 232 out of 592 Training Loss: 0.2275594288121773 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 233 out of 592 Training Loss: 0.24653128735831512 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 234 out of 592 Training Loss: 0.266918625832708 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 235 out of 592 Training Loss: 0.27377418490237126 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 236 out of 592 Training Loss: 0.2859981289703263 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 237 out of 592 Training Loss: 0.30664886595791707 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 238 out of 592 Training Loss: 0.33011354940122495 Test Loss: 0.01672814438599303\n",
      "Epoch: 4 Batch: 239 out of 592 Training Loss: 0.33393906211590896 Test Loss: 0.01672814438599303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 240 out of 592 Training Loss: 0.0005741505280495517 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 241 out of 592 Training Loss: 0.011042270283436857 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 242 out of 592 Training Loss: 0.020997584271466814 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 243 out of 592 Training Loss: 0.07837471778992422 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 244 out of 592 Training Loss: 0.11390107165578611 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 245 out of 592 Training Loss: 0.13770703639391668 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 246 out of 592 Training Loss: 0.1604369957328201 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 247 out of 592 Training Loss: 0.17848398576978453 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 248 out of 592 Training Loss: 0.22258575301173933 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 249 out of 592 Training Loss: 0.2380585655898214 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 250 out of 592 Training Loss: 0.24960546422306307 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 251 out of 592 Training Loss: 0.2718347817987562 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 252 out of 592 Training Loss: 0.2891202051520706 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 253 out of 592 Training Loss: 0.2905695390051589 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 254 out of 592 Training Loss: 0.2984984533956633 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 255 out of 592 Training Loss: 0.3230744542649374 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 256 out of 592 Training Loss: 0.3436563925912962 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 257 out of 592 Training Loss: 0.3682328647097931 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 258 out of 592 Training Loss: 0.39419303444236375 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 259 out of 592 Training Loss: 0.44684049841970064 Test Loss: 0.017011684459055577\n",
      "Epoch: 4 Batch: 260 out of 592 Training Loss: 0.0007849068856153714 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 261 out of 592 Training Loss: 0.001680723427357905 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 262 out of 592 Training Loss: 0.08060033529860043 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 263 out of 592 Training Loss: 0.08871414609116578 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 264 out of 592 Training Loss: 0.10869696460527443 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 265 out of 592 Training Loss: 0.11858660358798766 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 266 out of 592 Training Loss: 0.12803778707844282 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 267 out of 592 Training Loss: 0.14902395292979742 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 268 out of 592 Training Loss: 0.1526838760250535 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 269 out of 592 Training Loss: 0.16605481834945346 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 270 out of 592 Training Loss: 0.1880672029578414 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 271 out of 592 Training Loss: 0.20315846571561957 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 272 out of 592 Training Loss: 0.2304017763012376 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 273 out of 592 Training Loss: 0.23480527991292144 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 274 out of 592 Training Loss: 0.24563324874130155 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 275 out of 592 Training Loss: 0.266461907456192 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 276 out of 592 Training Loss: 0.2745459410720792 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 277 out of 592 Training Loss: 0.2828168464252081 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 278 out of 592 Training Loss: 0.30009829489377404 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 279 out of 592 Training Loss: 0.32101328534868623 Test Loss: 0.01720969096552863\n",
      "Epoch: 4 Batch: 280 out of 592 Training Loss: 0.0005492037571817681 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 281 out of 592 Training Loss: 0.03813557237757743 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 282 out of 592 Training Loss: 0.06538751073552668 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 283 out of 592 Training Loss: 0.11225215815378725 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 284 out of 592 Training Loss: 0.1399835246093422 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 285 out of 592 Training Loss: 0.14790781405194103 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 286 out of 592 Training Loss: 0.15695004914326965 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 287 out of 592 Training Loss: 0.19787102196736633 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 288 out of 592 Training Loss: 0.19898890999051183 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 289 out of 592 Training Loss: 0.21784965430291742 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 290 out of 592 Training Loss: 0.24337746125551313 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 291 out of 592 Training Loss: 0.2615878965416774 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 292 out of 592 Training Loss: 0.31706540395411104 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 293 out of 592 Training Loss: 0.3461564603904113 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 294 out of 592 Training Loss: 0.35873507910313457 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 295 out of 592 Training Loss: 0.373931965016745 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 296 out of 592 Training Loss: 0.3944984731906876 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 297 out of 592 Training Loss: 0.43284485318603366 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 298 out of 592 Training Loss: 0.4752760326200947 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 299 out of 592 Training Loss: 0.5140407850676998 Test Loss: 0.017150984517877187\n",
      "Epoch: 4 Batch: 300 out of 592 Training Loss: 0.0009069285937096998 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 301 out of 592 Training Loss: 0.027856708655550712 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 302 out of 592 Training Loss: 0.04671292674024367 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 303 out of 592 Training Loss: 0.051486039219841 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 304 out of 592 Training Loss: 0.10953476807330512 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 305 out of 592 Training Loss: 0.11518475009356402 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 306 out of 592 Training Loss: 0.13062618362013959 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 307 out of 592 Training Loss: 0.14673136795167588 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 308 out of 592 Training Loss: 0.1641495699644532 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 309 out of 592 Training Loss: 0.1726622978806939 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 310 out of 592 Training Loss: 0.19988642508392 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 311 out of 592 Training Loss: 0.21348865950708054 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 312 out of 592 Training Loss: 0.2287381968111243 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 313 out of 592 Training Loss: 0.2497694163233485 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 314 out of 592 Training Loss: 0.2610192956984486 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 315 out of 592 Training Loss: 0.28359406875674387 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 316 out of 592 Training Loss: 0.3064262761384692 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 317 out of 592 Training Loss: 0.312321589945301 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 318 out of 592 Training Loss: 0.3219594142049994 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 319 out of 592 Training Loss: 0.33071715725813766 Test Loss: 0.016840623786926946\n",
      "Epoch: 4 Batch: 320 out of 592 Training Loss: 0.0005794149815030137 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 321 out of 592 Training Loss: 0.00795029811476541 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 322 out of 592 Training Loss: 0.020185284874487643 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 323 out of 592 Training Loss: 0.03688271958296371 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 324 out of 592 Training Loss: 0.04149498566900564 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 325 out of 592 Training Loss: 0.060867560654032954 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 326 out of 592 Training Loss: 0.06849656030451132 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 327 out of 592 Training Loss: 0.07214885982600225 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 328 out of 592 Training Loss: 0.07896104580638183 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 329 out of 592 Training Loss: 0.10067188865420593 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 330 out of 592 Training Loss: 0.11392495254901422 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 331 out of 592 Training Loss: 0.16070401887324823 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 332 out of 592 Training Loss: 0.25562724689868466 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 333 out of 592 Training Loss: 0.27029569758949057 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 334 out of 592 Training Loss: 0.29113619527694956 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 335 out of 592 Training Loss: 0.3029978101598956 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 336 out of 592 Training Loss: 0.32617765418169276 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 337 out of 592 Training Loss: 0.34735478772697226 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 338 out of 592 Training Loss: 0.36511254436251894 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 339 out of 592 Training Loss: 0.4071900745379664 Test Loss: 0.01670606388484664\n",
      "Epoch: 4 Batch: 340 out of 592 Training Loss: 0.0007308020932126249 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 341 out of 592 Training Loss: 0.02252544720894577 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 342 out of 592 Training Loss: 0.07005718236095192 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 343 out of 592 Training Loss: 0.08652727005309345 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 344 out of 592 Training Loss: 0.09669698194838049 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 345 out of 592 Training Loss: 0.11834123709476473 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 346 out of 592 Training Loss: 0.12008625422376128 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 347 out of 592 Training Loss: 0.12469981671082707 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 348 out of 592 Training Loss: 0.1386659374903414 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 349 out of 592 Training Loss: 0.15166212678658697 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 350 out of 592 Training Loss: 0.17729769334900114 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 351 out of 592 Training Loss: 0.18911117561864588 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 352 out of 592 Training Loss: 0.22583983712601396 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 353 out of 592 Training Loss: 0.23910410296874973 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 354 out of 592 Training Loss: 0.25414766804086897 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 355 out of 592 Training Loss: 0.28146599665987226 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 356 out of 592 Training Loss: 0.2903198857303235 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 357 out of 592 Training Loss: 0.2999763308535788 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 358 out of 592 Training Loss: 0.3096018961142275 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 359 out of 592 Training Loss: 0.3149580421369049 Test Loss: 0.017029369038275698\n",
      "Epoch: 4 Batch: 360 out of 592 Training Loss: 0.0005442329521120255 Test Loss: 0.016972458369282045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 361 out of 592 Training Loss: 0.011609418022149867 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 362 out of 592 Training Loss: 0.05658103309821493 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 363 out of 592 Training Loss: 0.06836178124200709 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 364 out of 592 Training Loss: 0.0773768487225116 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 365 out of 592 Training Loss: 0.09749028895300038 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 366 out of 592 Training Loss: 0.11447198938828118 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 367 out of 592 Training Loss: 0.14700242009263642 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 368 out of 592 Training Loss: 0.16412846621375687 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 369 out of 592 Training Loss: 0.18026672180991776 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 370 out of 592 Training Loss: 0.2120884868155063 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 371 out of 592 Training Loss: 0.23181751247745164 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 372 out of 592 Training Loss: 0.2389675576078535 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 373 out of 592 Training Loss: 0.26768604589875944 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 374 out of 592 Training Loss: 0.2751952648932875 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 375 out of 592 Training Loss: 0.28192828146990545 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 376 out of 592 Training Loss: 0.3147536918359638 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 377 out of 592 Training Loss: 0.33648860378440626 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 378 out of 592 Training Loss: 0.36296452133234747 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 379 out of 592 Training Loss: 0.4158356703656555 Test Loss: 0.016972458369282045\n",
      "Epoch: 4 Batch: 380 out of 592 Training Loss: 0.0007145629523716567 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 381 out of 592 Training Loss: 0.006666889998915016 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 382 out of 592 Training Loss: 0.057318105015756904 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 383 out of 592 Training Loss: 0.07015144976586324 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 384 out of 592 Training Loss: 0.08169059606999379 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 385 out of 592 Training Loss: 0.0932508806026004 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 386 out of 592 Training Loss: 0.10430715943485719 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 387 out of 592 Training Loss: 0.14089037443310243 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 388 out of 592 Training Loss: 0.14915580081552487 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 389 out of 592 Training Loss: 0.1639332013463996 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 390 out of 592 Training Loss: 0.177842026229801 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 391 out of 592 Training Loss: 0.1962215878328703 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 392 out of 592 Training Loss: 0.21446238423496705 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 393 out of 592 Training Loss: 0.22358134261131507 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 394 out of 592 Training Loss: 0.24766676745176536 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 395 out of 592 Training Loss: 0.3013705337679408 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 396 out of 592 Training Loss: 0.30892363295689446 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 397 out of 592 Training Loss: 0.33316392370298725 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 398 out of 592 Training Loss: 0.3426203496082149 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 399 out of 592 Training Loss: 0.34615727293104154 Test Loss: 0.01671861572487449\n",
      "Epoch: 4 Batch: 400 out of 592 Training Loss: 0.0006061321719534531 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 401 out of 592 Training Loss: 0.018820179127062096 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 402 out of 592 Training Loss: 0.05538691344102694 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 403 out of 592 Training Loss: 0.09916637601693942 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 404 out of 592 Training Loss: 0.1145785827784784 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 405 out of 592 Training Loss: 0.130967490807081 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 406 out of 592 Training Loss: 0.1397442374973543 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 407 out of 592 Training Loss: 0.17491339562823846 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 408 out of 592 Training Loss: 0.18233720908304527 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 409 out of 592 Training Loss: 0.20991015011807754 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 410 out of 592 Training Loss: 0.23082048768540694 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 411 out of 592 Training Loss: 0.24092164026936366 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 412 out of 592 Training Loss: 0.2668414613827117 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 413 out of 592 Training Loss: 0.2726881135850199 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 414 out of 592 Training Loss: 0.28631084362408 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 415 out of 592 Training Loss: 0.29204574270536976 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 416 out of 592 Training Loss: 0.30410079980632143 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 417 out of 592 Training Loss: 0.30933650114005284 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 418 out of 592 Training Loss: 0.3441804041749783 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 419 out of 592 Training Loss: 0.3651270316577502 Test Loss: 0.01666133535433222\n",
      "Epoch: 4 Batch: 420 out of 592 Training Loss: 0.0007145734821079664 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 421 out of 592 Training Loss: 0.015792281771135182 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 422 out of 592 Training Loss: 0.032111936653089375 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 423 out of 592 Training Loss: 0.07302763446612343 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 424 out of 592 Training Loss: 0.09072795830113872 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 425 out of 592 Training Loss: 0.11688006616575702 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 426 out of 592 Training Loss: 0.13228386215312465 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 427 out of 592 Training Loss: 0.14090661838097557 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 428 out of 592 Training Loss: 0.17169856227381214 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 429 out of 592 Training Loss: 0.18317641544533952 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 430 out of 592 Training Loss: 0.1973699922059296 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 431 out of 592 Training Loss: 0.2174360374305962 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 432 out of 592 Training Loss: 0.22374958352563962 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 433 out of 592 Training Loss: 0.23784632136492118 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 434 out of 592 Training Loss: 0.26816559111146315 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 435 out of 592 Training Loss: 0.28461887960700377 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 436 out of 592 Training Loss: 0.2965761375930725 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 437 out of 592 Training Loss: 0.30082520417315944 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 438 out of 592 Training Loss: 0.3253849702273129 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 439 out of 592 Training Loss: 0.3502275398318289 Test Loss: 0.01661388831086118\n",
      "Epoch: 4 Batch: 440 out of 592 Training Loss: 0.0006330433934581841 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 441 out of 592 Training Loss: 0.010858556412125882 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 442 out of 592 Training Loss: 0.041707677923108394 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 443 out of 592 Training Loss: 0.06253388167169219 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 444 out of 592 Training Loss: 0.07947180994596606 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 445 out of 592 Training Loss: 0.11004937083270674 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 446 out of 592 Training Loss: 0.1328147595449937 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 447 out of 592 Training Loss: 0.1643793938383115 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 448 out of 592 Training Loss: 0.18878254682567244 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 449 out of 592 Training Loss: 0.19852149967845803 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 450 out of 592 Training Loss: 0.21141166814413195 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 451 out of 592 Training Loss: 0.2186763758472753 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 452 out of 592 Training Loss: 0.2294350337870074 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 453 out of 592 Training Loss: 0.2488254249728871 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 454 out of 592 Training Loss: 0.25813254869353297 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 455 out of 592 Training Loss: 0.26838719720762494 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 456 out of 592 Training Loss: 0.2869531965516162 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 457 out of 592 Training Loss: 0.2976016967259002 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 458 out of 592 Training Loss: 0.30480167099084976 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 459 out of 592 Training Loss: 0.3392053527459157 Test Loss: 0.016899361303358368\n",
      "Epoch: 4 Batch: 460 out of 592 Training Loss: 0.0006362505203137547 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 461 out of 592 Training Loss: 0.01845651752706863 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 462 out of 592 Training Loss: 0.025218167150581852 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 463 out of 592 Training Loss: 0.03983918465134002 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 464 out of 592 Training Loss: 0.05741565375980236 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 465 out of 592 Training Loss: 0.11200223192032673 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 466 out of 592 Training Loss: 0.14710194318946695 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 467 out of 592 Training Loss: 0.15277780154180026 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 468 out of 592 Training Loss: 0.16458600973170495 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 469 out of 592 Training Loss: 0.18599273664336896 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 470 out of 592 Training Loss: 0.19262483743410802 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 471 out of 592 Training Loss: 0.19871867801454163 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 472 out of 592 Training Loss: 0.23306393231656647 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 473 out of 592 Training Loss: 0.24437863081153727 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 474 out of 592 Training Loss: 0.2588827012052784 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 475 out of 592 Training Loss: 0.2817884588560114 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 476 out of 592 Training Loss: 0.3071748537888775 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 477 out of 592 Training Loss: 0.3275896923562536 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 478 out of 592 Training Loss: 0.3437747003456602 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 479 out of 592 Training Loss: 0.37479889559950924 Test Loss: 0.01665865840517323\n",
      "Epoch: 4 Batch: 480 out of 592 Training Loss: 0.0007038693588664419 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 481 out of 592 Training Loss: 0.020605375961630572 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 482 out of 592 Training Loss: 0.04467772875842356 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 483 out of 592 Training Loss: 0.06096992638823294 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 484 out of 592 Training Loss: 0.0653235148352077 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 485 out of 592 Training Loss: 0.07528119438257717 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 486 out of 592 Training Loss: 0.0791255624212762 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 487 out of 592 Training Loss: 0.08160322412778341 Test Loss: 0.016633195619127066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Batch: 488 out of 592 Training Loss: 0.10015437632729017 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 489 out of 592 Training Loss: 0.115327907464697 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 490 out of 592 Training Loss: 0.12729668777365888 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 491 out of 592 Training Loss: 0.1434931193657657 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 492 out of 592 Training Loss: 0.1545959121726891 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 493 out of 592 Training Loss: 0.1659569849439403 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 494 out of 592 Training Loss: 0.17797250647027696 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 495 out of 592 Training Loss: 0.24124740410526002 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 496 out of 592 Training Loss: 0.24700695948605145 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 497 out of 592 Training Loss: 0.2681660924286088 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 498 out of 592 Training Loss: 0.28478081637923325 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 499 out of 592 Training Loss: 0.32953287774626816 Test Loss: 0.016633195619127066\n",
      "Epoch: 4 Batch: 500 out of 592 Training Loss: 0.0006429578536618737 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 501 out of 592 Training Loss: 0.008029500244693854 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 502 out of 592 Training Loss: 0.023324822297828295 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 503 out of 592 Training Loss: 0.03411606907709132 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 504 out of 592 Training Loss: 0.04845939319445382 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 505 out of 592 Training Loss: 0.06434495676173459 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 506 out of 592 Training Loss: 0.11433898959173451 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 507 out of 592 Training Loss: 0.12437829024954329 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 508 out of 592 Training Loss: 0.13884163111312875 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 509 out of 592 Training Loss: 0.17447595879300126 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 510 out of 592 Training Loss: 0.2070962434993554 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 511 out of 592 Training Loss: 0.2778137198077012 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 512 out of 592 Training Loss: 0.30106227867169866 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 513 out of 592 Training Loss: 0.33545177824659833 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 514 out of 592 Training Loss: 0.34728117681904325 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 515 out of 592 Training Loss: 0.3707810977832843 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 516 out of 592 Training Loss: 0.3860096291066576 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 517 out of 592 Training Loss: 0.39601027522100696 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 518 out of 592 Training Loss: 0.4153671034039427 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 519 out of 592 Training Loss: 0.44819699901952037 Test Loss: 0.01674477924649237\n",
      "Epoch: 4 Batch: 520 out of 592 Training Loss: 0.0007778539552607328 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 521 out of 592 Training Loss: 0.046792278368703345 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 522 out of 592 Training Loss: 0.058634077501229266 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 523 out of 592 Training Loss: 0.07262909070097587 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 524 out of 592 Training Loss: 0.08819581505351445 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 525 out of 592 Training Loss: 0.09914814059131524 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 526 out of 592 Training Loss: 0.10942313850366256 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 527 out of 592 Training Loss: 0.12046138413690707 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 528 out of 592 Training Loss: 0.13550667494975707 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 529 out of 592 Training Loss: 0.1625600478842177 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 530 out of 592 Training Loss: 0.24010659970783374 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 531 out of 592 Training Loss: 0.24794167124175212 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 532 out of 592 Training Loss: 0.2636673509373584 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 533 out of 592 Training Loss: 0.32345473982834005 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 534 out of 592 Training Loss: 0.3332223187431016 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 535 out of 592 Training Loss: 0.34988977231525564 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 536 out of 592 Training Loss: 0.35433422618054533 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 537 out of 592 Training Loss: 0.3727860286130824 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 538 out of 592 Training Loss: 0.39157245897196913 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 539 out of 592 Training Loss: 0.4217848333760419 Test Loss: 0.016670477901105627\n",
      "Epoch: 4 Batch: 540 out of 592 Training Loss: 0.0007408844419149797 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 541 out of 592 Training Loss: 0.048588438529553836 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 542 out of 592 Training Loss: 0.06289328226077837 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 543 out of 592 Training Loss: 0.0853823587410478 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 544 out of 592 Training Loss: 0.10069205017018598 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 545 out of 592 Training Loss: 0.15825377554822248 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 546 out of 592 Training Loss: 0.18132346646118444 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 547 out of 592 Training Loss: 0.2114302400611667 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 548 out of 592 Training Loss: 0.26274986528981015 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 549 out of 592 Training Loss: 0.26755877175557896 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 550 out of 592 Training Loss: 0.2898921497278242 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 551 out of 592 Training Loss: 0.31600996256995484 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 552 out of 592 Training Loss: 0.33097112980860754 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 553 out of 592 Training Loss: 0.37303041351217314 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 554 out of 592 Training Loss: 0.37499669274504766 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 555 out of 592 Training Loss: 0.426382669119197 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 556 out of 592 Training Loss: 0.4548624817859708 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 557 out of 592 Training Loss: 0.472249840257722 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 558 out of 592 Training Loss: 0.48799666283841714 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 559 out of 592 Training Loss: 0.5147623148393212 Test Loss: 0.01656871165191247\n",
      "Epoch: 4 Batch: 560 out of 592 Training Loss: 0.0008970546656685199 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 561 out of 592 Training Loss: 0.04801263391314319 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 562 out of 592 Training Loss: 0.05875327169237903 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 563 out of 592 Training Loss: 0.07009754899231962 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 564 out of 592 Training Loss: 0.07870223104296498 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 565 out of 592 Training Loss: 0.08593078480376176 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 566 out of 592 Training Loss: 0.12910368071211747 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 567 out of 592 Training Loss: 0.14171253760619335 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 568 out of 592 Training Loss: 0.1476007216730552 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 569 out of 592 Training Loss: 0.15741416449932627 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 570 out of 592 Training Loss: 0.17913105535594992 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 571 out of 592 Training Loss: 0.18987283214269451 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 572 out of 592 Training Loss: 0.2011015146159964 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 573 out of 592 Training Loss: 0.2175604059481459 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 574 out of 592 Training Loss: 0.2305982616850572 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 575 out of 592 Training Loss: 0.24071256920038037 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 576 out of 592 Training Loss: 0.289698519849165 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 577 out of 592 Training Loss: 0.3210524964266854 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 578 out of 592 Training Loss: 0.34538319996832184 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 579 out of 592 Training Loss: 0.3761809851163464 Test Loss: 0.016677958245219834\n",
      "Epoch: 4 Batch: 580 out of 592 Training Loss: 0.0006464375066774549 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 581 out of 592 Training Loss: 0.02585254329025919 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 582 out of 592 Training Loss: 0.03969408291161234 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 583 out of 592 Training Loss: 0.04383932984203274 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 584 out of 592 Training Loss: 0.07757132312983447 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 585 out of 592 Training Loss: 0.1073066517594474 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 586 out of 592 Training Loss: 0.12108358739644462 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 587 out of 592 Training Loss: 0.13617485108554536 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 588 out of 592 Training Loss: 0.17444168868243867 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 589 out of 592 Training Loss: 0.2041186308372038 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 590 out of 592 Training Loss: 0.2123330142232793 Test Loss: 0.016581506235525012\n",
      "Epoch: 4 Batch: 591 out of 592 Training Loss: 0.21312441984193572 Test Loss: 0.016581506235525012\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "train_losses, testlosses = model.fit(trainloader = trainloader,\n",
    "                                     validationloader = valloader,\n",
    "                                     loss = MSELoss,\n",
    "                                     optim = SGD,\n",
    "                                     lr=0.01,\n",
    "                                     epochs = 5,\n",
    "                                     val_per_batch = 20,\n",
    "                                     cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22584ea0608>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(testlosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, expected = model.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x225872abe88>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(result)\n",
    "plt.plot(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
