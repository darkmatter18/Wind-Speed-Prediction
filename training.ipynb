{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Copyright 2020 Arkadip Bhattacharya\n",
    "\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import Normalize_df, WindSpeedDataset, WindSpeedDatasetTimeSeries, ComposeTransform, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>air_temperature_mean</th>\n",
       "      <th>pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370203</td>\n",
       "      <td>0.103164</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.322799</td>\n",
       "      <td>0.268912</td>\n",
       "      <td>0.838440</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.302483</td>\n",
       "      <td>0.709078</td>\n",
       "      <td>0.988858</td>\n",
       "      <td>0.260417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.246050</td>\n",
       "      <td>0.850758</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>0.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.827372</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  air_temperature_mean  pressure  wind_direction  wind_speed\n",
       "0  0.000000              0.370203  0.103164        0.732591    0.625000\n",
       "1  0.000011              0.322799  0.268912        0.838440    0.354167\n",
       "2  0.000022              0.302483  0.709078        0.988858    0.260417\n",
       "3  0.000033              0.246050  0.850758        0.239554    0.093750\n",
       "4  0.000044              0.194131  0.827372        0.345404    0.291667"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Normalize_df(pd.read_csv('./dataset-daily.csv'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(dataset, test_size = 0.1)\n",
    "trainset, valset = train_test_split(trainset, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                    0.225184\n",
       "air_temperature_mean    0.668172\n",
       "pressure                0.303301\n",
       "wind_direction          0.562674\n",
       "wind_speed              0.343750\n",
       "Name: 903, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = WindSpeedDatasetTimeSeries(trainset,transform=ComposeTransform([ToTensor()]))\n",
    "test_dataset = WindSpeedDatasetTimeSeries(testset, transform=ComposeTransform([ToTensor()]))\n",
    "val_dataset = WindSpeedDatasetTimeSeries(valset, transform=ComposeTransform([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6682, 0.3033, 0.5627],\n",
       "         [0.8397, 0.6018, 0.3733],\n",
       "         [0.4402, 0.2105, 0.5181],\n",
       "         [0.5779, 0.5454, 0.7242],\n",
       "         [0.5282, 0.2840, 0.6741],\n",
       "         [0.7630, 0.2799, 0.5543]], dtype=torch.float64), tensor([[0.3438],\n",
       "         [0.2396],\n",
       "         [0.2812],\n",
       "         [0.4271],\n",
       "         [0.6354],\n",
       "         [0.2812]], dtype=torch.float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "trainloader = dataloader.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "valloader = dataloader.DataLoader(val_dataset, batch_size, shuffle = False)\n",
    "testloader = dataloader.DataLoader(test_dataset, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, l = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Count: 1\n",
      "Device: Tesla K80\n",
      "Device Capability: (3, 7)\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if cuda else torch.device('cpu')\n",
    "if cuda:\n",
    "    print(\"Device Count:\", torch.cuda.device_count())\n",
    "    print(\"Device:\", torch.cuda.get_device_name())\n",
    "    print(\"Device Capability:\", torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm1): LSTM(3, 100, batch_first=True)\n",
      "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "model = Model(3, 100, 1, cuda=cuda)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 0 out of 93 Training Loss: 9.50202586189393e-05 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 1 out of 93 Training Loss: 0.007422369649453509 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 2 out of 93 Training Loss: 0.016287465324445118 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 3 out of 93 Training Loss: 0.026763507840974678 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 4 out of 93 Training Loss: 0.03475979410652672 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 5 out of 93 Training Loss: 0.043769169343455185 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 6 out of 93 Training Loss: 0.050407962152554144 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 7 out of 93 Training Loss: 0.05812406048719441 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 8 out of 93 Training Loss: 0.07029329028521333 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 9 out of 93 Training Loss: 0.0792357965612844 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 10 out of 93 Training Loss: 0.0890738561058477 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 11 out of 93 Training Loss: 0.09924878154490743 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 12 out of 93 Training Loss: 0.10746432815287862 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 13 out of 93 Training Loss: 0.11802766074035917 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 14 out of 93 Training Loss: 0.12635587130282674 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 15 out of 93 Training Loss: 0.13306520248372708 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 16 out of 93 Training Loss: 0.1409037767218486 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 17 out of 93 Training Loss: 0.149629631290032 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 18 out of 93 Training Loss: 0.15857271871138964 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 19 out of 93 Training Loss: 0.16518967060913 Test Loss: 0.008060482203621756\n",
      "Epoch: 1 Batch: 20 out of 93 Training Loss: 0.001859648876471049 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 21 out of 93 Training Loss: 0.009400787555260188 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 22 out of 93 Training Loss: 0.01748049052117539 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 23 out of 93 Training Loss: 0.02531622422544432 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 24 out of 93 Training Loss: 0.034129678994996075 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 25 out of 93 Training Loss: 0.04315915031043959 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 26 out of 93 Training Loss: 0.050873979252410655 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 27 out of 93 Training Loss: 0.05898783381713701 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 28 out of 93 Training Loss: 0.06791047405136896 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 29 out of 93 Training Loss: 0.07626107174588515 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 30 out of 93 Training Loss: 0.08557854525430275 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 31 out of 93 Training Loss: 0.09430217023624254 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 32 out of 93 Training Loss: 0.10306929729564263 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 33 out of 93 Training Loss: 0.11171478058828188 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 34 out of 93 Training Loss: 0.11982209168059899 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 35 out of 93 Training Loss: 0.12803496446145846 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 36 out of 93 Training Loss: 0.13645346395356728 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 37 out of 93 Training Loss: 0.14434993851615263 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 38 out of 93 Training Loss: 0.15272456526441647 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 39 out of 93 Training Loss: 0.16095717277063204 Test Loss: 0.007943895687772469\n",
      "Epoch: 1 Batch: 40 out of 93 Training Loss: 0.0018164961356131057 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 41 out of 93 Training Loss: 0.009134493185153386 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 42 out of 93 Training Loss: 0.018033181665411374 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 43 out of 93 Training Loss: 0.025383595259806296 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 44 out of 93 Training Loss: 0.032555347191115996 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 45 out of 93 Training Loss: 0.0433529856372086 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 46 out of 93 Training Loss: 0.05065842845844307 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 47 out of 93 Training Loss: 0.05850278620975294 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 48 out of 93 Training Loss: 0.06649421935336866 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 49 out of 93 Training Loss: 0.07562802580552377 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 50 out of 93 Training Loss: 0.08281079109193959 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 51 out of 93 Training Loss: 0.0914698176774589 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 52 out of 93 Training Loss: 0.10022425826074757 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 53 out of 93 Training Loss: 0.10801533551322498 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 54 out of 93 Training Loss: 0.11761527915852346 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 55 out of 93 Training Loss: 0.1240119280982224 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 56 out of 93 Training Loss: 0.1320912939253656 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 57 out of 93 Training Loss: 0.14094856183411755 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 58 out of 93 Training Loss: 0.14825918280886688 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 59 out of 93 Training Loss: 0.1568607514481453 Test Loss: 0.00795113676312295\n",
      "Epoch: 1 Batch: 60 out of 93 Training Loss: 0.0017746226518979933 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 61 out of 93 Training Loss: 0.01154966611709508 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 62 out of 93 Training Loss: 0.020426329749785987 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 63 out of 93 Training Loss: 0.0284133018582693 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 64 out of 93 Training Loss: 0.037493068318032355 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 65 out of 93 Training Loss: 0.04795946047004375 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 66 out of 93 Training Loss: 0.057165257225894064 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 67 out of 93 Training Loss: 0.0652687256812683 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 68 out of 93 Training Loss: 0.07308472745414886 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 69 out of 93 Training Loss: 0.0815884466871253 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 70 out of 93 Training Loss: 0.09180248581167373 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 71 out of 93 Training Loss: 0.10221453901674661 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 72 out of 93 Training Loss: 0.10912355413954768 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 73 out of 93 Training Loss: 0.11657571658786449 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 74 out of 93 Training Loss: 0.12638101499822052 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 75 out of 93 Training Loss: 0.1347310036077729 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 76 out of 93 Training Loss: 0.14253850054290207 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 77 out of 93 Training Loss: 0.15171818673905046 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 78 out of 93 Training Loss: 0.15979349106844576 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 79 out of 93 Training Loss: 0.16804020066108616 Test Loss: 0.007918919436633587\n",
      "Epoch: 1 Batch: 80 out of 93 Training Loss: 0.0018884501957040225 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 81 out of 93 Training Loss: 0.01105545270970873 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 82 out of 93 Training Loss: 0.019745826681767026 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 83 out of 93 Training Loss: 0.02834413103542141 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 84 out of 93 Training Loss: 0.037984708284650365 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 85 out of 93 Training Loss: 0.046143547592375794 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 86 out of 93 Training Loss: 0.054493009073350945 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 87 out of 93 Training Loss: 0.061004852370981494 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 88 out of 93 Training Loss: 0.07047010814553908 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 89 out of 93 Training Loss: 0.07766181412867121 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 90 out of 93 Training Loss: 0.08596228711089901 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 91 out of 93 Training Loss: 0.09627598427018932 Test Loss: 0.007965051061050459\n",
      "Epoch: 1 Batch: 92 out of 93 Training Loss: 0.10394180934659056 Test Loss: 0.007965051061050459\n",
      "Epoch: 2 Batch: 0 out of 93 Training Loss: 8.495197060608095e-05 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 1 out of 93 Training Loss: 0.00815948237094187 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 2 out of 93 Training Loss: 0.016788911525039904 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 3 out of 93 Training Loss: 0.023811718433975212 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 4 out of 93 Training Loss: 0.03295117335754537 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 5 out of 93 Training Loss: 0.04139122737932109 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 6 out of 93 Training Loss: 0.05102913825201892 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 7 out of 93 Training Loss: 0.05876550309720539 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 8 out of 93 Training Loss: 0.06791408311936163 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 9 out of 93 Training Loss: 0.0765000167152574 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 10 out of 93 Training Loss: 0.08591846049192452 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 11 out of 93 Training Loss: 0.09466465980175041 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 12 out of 93 Training Loss: 0.10344554994615816 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 13 out of 93 Training Loss: 0.11267067164543175 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 14 out of 93 Training Loss: 0.12087098367872738 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 15 out of 93 Training Loss: 0.1291610034963777 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 16 out of 93 Training Loss: 0.13729217321041132 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 17 out of 93 Training Loss: 0.14548691500338817 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 18 out of 93 Training Loss: 0.15228485221403743 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 19 out of 93 Training Loss: 0.16075533533829356 Test Loss: 0.007967607456852089\n",
      "Epoch: 2 Batch: 20 out of 93 Training Loss: 0.0018344668930243319 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 21 out of 93 Training Loss: 0.010226424623079187 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 22 out of 93 Training Loss: 0.01787408192435611 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 23 out of 93 Training Loss: 0.0269799105150472 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 24 out of 93 Training Loss: 0.034545287795163276 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 25 out of 93 Training Loss: 0.04210682476664293 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 26 out of 93 Training Loss: 0.04922531855086196 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 27 out of 93 Training Loss: 0.05726804707924235 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 28 out of 93 Training Loss: 0.06679712266454804 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 29 out of 93 Training Loss: 0.07496806357780803 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 30 out of 93 Training Loss: 0.08263248841071832 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 31 out of 93 Training Loss: 0.08975880483890283 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 32 out of 93 Training Loss: 0.09731067794168699 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 33 out of 93 Training Loss: 0.10635809584314335 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 34 out of 93 Training Loss: 0.1140575710011493 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 35 out of 93 Training Loss: 0.1235055535359155 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 36 out of 93 Training Loss: 0.13174248796681273 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 37 out of 93 Training Loss: 0.13920964424291957 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 38 out of 93 Training Loss: 0.14766511910388577 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 39 out of 93 Training Loss: 0.15601779644171107 Test Loss: 0.008128184994513338\n",
      "Epoch: 2 Batch: 40 out of 93 Training Loss: 0.0017606065521215354 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 41 out of 93 Training Loss: 0.010674861157891647 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 42 out of 93 Training Loss: 0.018376185456869497 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 43 out of 93 Training Loss: 0.026428996483442678 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 44 out of 93 Training Loss: 0.03409121268486728 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 45 out of 93 Training Loss: 0.04152630585303654 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 46 out of 93 Training Loss: 0.050742097562757625 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 47 out of 93 Training Loss: 0.05991488079598297 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 48 out of 93 Training Loss: 0.06793839334568133 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 49 out of 93 Training Loss: 0.07592901065072169 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 50 out of 93 Training Loss: 0.08476209304353347 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 51 out of 93 Training Loss: 0.0935863129707252 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 52 out of 93 Training Loss: 0.10133534147462 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 53 out of 93 Training Loss: 0.10926053895672908 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 54 out of 93 Training Loss: 0.11771454698464026 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 55 out of 93 Training Loss: 0.1260643940331613 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 56 out of 93 Training Loss: 0.13472169022133698 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 57 out of 93 Training Loss: 0.1419679670507287 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 58 out of 93 Training Loss: 0.1482522286387359 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 59 out of 93 Training Loss: 0.15700618679693093 Test Loss: 0.007851459212939848\n",
      "Epoch: 2 Batch: 60 out of 93 Training Loss: 0.0017838075555803047 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 61 out of 93 Training Loss: 0.010822634006285832 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 62 out of 93 Training Loss: 0.01902386152967231 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 63 out of 93 Training Loss: 0.0291750326551439 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 64 out of 93 Training Loss: 0.03755376094802634 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 65 out of 93 Training Loss: 0.04664880389198081 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 66 out of 93 Training Loss: 0.05471878012492673 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 67 out of 93 Training Loss: 0.06310486493482606 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 68 out of 93 Training Loss: 0.07135132493301646 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 69 out of 93 Training Loss: 0.08120417853667752 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 70 out of 93 Training Loss: 0.09083614477738158 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 71 out of 93 Training Loss: 0.09794055277332084 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 72 out of 93 Training Loss: 0.10612967496588246 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 73 out of 93 Training Loss: 0.11289717841743843 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 74 out of 93 Training Loss: 0.12096573717742579 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 75 out of 93 Training Loss: 0.13073471959530727 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 76 out of 93 Training Loss: 0.13833006311058418 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 77 out of 93 Training Loss: 0.14635701410322802 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 78 out of 93 Training Loss: 0.15694508168696777 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 79 out of 93 Training Loss: 0.16710658769904987 Test Loss: 0.007901028348979626\n",
      "Epoch: 2 Batch: 80 out of 93 Training Loss: 0.0018814659194599575 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 81 out of 93 Training Loss: 0.010008107543135782 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 82 out of 93 Training Loss: 0.017923022292698522 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 83 out of 93 Training Loss: 0.027905493647004266 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 84 out of 93 Training Loss: 0.038102969199563165 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 85 out of 93 Training Loss: 0.04597681262358536 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 86 out of 93 Training Loss: 0.05385186117901911 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 87 out of 93 Training Loss: 0.06292229392393936 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 88 out of 93 Training Loss: 0.0720367353858005 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 89 out of 93 Training Loss: 0.08021582257762064 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 90 out of 93 Training Loss: 0.08863105126484741 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 91 out of 93 Training Loss: 0.09583508050828685 Test Loss: 0.00799411873925816\n",
      "Epoch: 2 Batch: 92 out of 93 Training Loss: 0.10319085758938898 Test Loss: 0.00799411873925816\n",
      "Epoch: 3 Batch: 0 out of 93 Training Loss: 8.462035968419044e-05 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 1 out of 93 Training Loss: 0.007464627660210094 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 2 out of 93 Training Loss: 0.01669684603750225 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 3 out of 93 Training Loss: 0.02565130770146366 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 4 out of 93 Training Loss: 0.03414483360886093 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 5 out of 93 Training Loss: 0.042710583581919634 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 6 out of 93 Training Loss: 0.04982178650736328 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 7 out of 93 Training Loss: 0.057122184354211056 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 8 out of 93 Training Loss: 0.06522369237556573 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 9 out of 93 Training Loss: 0.07291328992634531 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 10 out of 93 Training Loss: 0.0820197350525808 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 11 out of 93 Training Loss: 0.09099606137662646 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 12 out of 93 Training Loss: 0.09813228195473071 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 13 out of 93 Training Loss: 0.10546334033771869 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 14 out of 93 Training Loss: 0.11345155017390367 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 15 out of 93 Training Loss: 0.12358481010362025 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 16 out of 93 Training Loss: 0.13115561699434636 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 17 out of 93 Training Loss: 0.13799439176856992 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 18 out of 93 Training Loss: 0.14775982212215183 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 19 out of 93 Training Loss: 0.15608033344089506 Test Loss: 0.00784140190278942\n",
      "Epoch: 3 Batch: 20 out of 93 Training Loss: 0.0017657543734772997 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 21 out of 93 Training Loss: 0.00924042828540643 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 22 out of 93 Training Loss: 0.018227509522543747 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 23 out of 93 Training Loss: 0.026482628958032925 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 24 out of 93 Training Loss: 0.03370926978226145 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 25 out of 93 Training Loss: 0.041646151637391646 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 26 out of 93 Training Loss: 0.05028410798098286 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 27 out of 93 Training Loss: 0.05817736296142777 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 28 out of 93 Training Loss: 0.06682089792724093 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 29 out of 93 Training Loss: 0.0761774311044379 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 30 out of 93 Training Loss: 0.08437621670003374 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 31 out of 93 Training Loss: 0.09226356802667816 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 32 out of 93 Training Loss: 0.09998470667909225 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 33 out of 93 Training Loss: 0.10685482232059677 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 34 out of 93 Training Loss: 0.11535444932218035 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 35 out of 93 Training Loss: 0.125020023269312 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 36 out of 93 Training Loss: 0.13375663275058708 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 37 out of 93 Training Loss: 0.1405123722010418 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 38 out of 93 Training Loss: 0.15014394635494194 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 39 out of 93 Training Loss: 0.15866224264885626 Test Loss: 0.007923235651105642\n",
      "Epoch: 3 Batch: 40 out of 93 Training Loss: 0.0017809389912324398 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 41 out of 93 Training Loss: 0.011019650549514814 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 42 out of 93 Training Loss: 0.02040225330822043 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 43 out of 93 Training Loss: 0.030028059781415983 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 44 out of 93 Training Loss: 0.03931540966115765 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 45 out of 93 Training Loss: 0.04689610719762616 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 46 out of 93 Training Loss: 0.05529060464434199 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 47 out of 93 Training Loss: 0.06434391036711506 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 48 out of 93 Training Loss: 0.07212129019282632 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 49 out of 93 Training Loss: 0.08036525052122645 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 50 out of 93 Training Loss: 0.08869925670407586 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 51 out of 93 Training Loss: 0.09638680301688485 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 52 out of 93 Training Loss: 0.10487707767717414 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 53 out of 93 Training Loss: 0.11335965775035195 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 54 out of 93 Training Loss: 0.12089566623873882 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 55 out of 93 Training Loss: 0.12910091707564764 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 56 out of 93 Training Loss: 0.13733506871469192 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 57 out of 93 Training Loss: 0.14506997836835556 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 58 out of 93 Training Loss: 0.1533395984963315 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 59 out of 93 Training Loss: 0.16072065074078612 Test Loss: 0.007893024012446404\n",
      "Epoch: 3 Batch: 60 out of 93 Training Loss: 0.0018178728246969105 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 61 out of 93 Training Loss: 0.009374131059823643 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 62 out of 93 Training Loss: 0.018632308191238057 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 63 out of 93 Training Loss: 0.026740096120713364 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 64 out of 93 Training Loss: 0.036300200580237516 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 65 out of 93 Training Loss: 0.0455790931196791 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 66 out of 93 Training Loss: 0.05578754524010242 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 67 out of 93 Training Loss: 0.06433396002518953 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 68 out of 93 Training Loss: 0.07366918174791158 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 69 out of 93 Training Loss: 0.08080218646916569 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 70 out of 93 Training Loss: 0.08896265372875155 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 71 out of 93 Training Loss: 0.09826064676287592 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 72 out of 93 Training Loss: 0.10701769598069609 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 73 out of 93 Training Loss: 0.11353233836832465 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 74 out of 93 Training Loss: 0.12127656357604087 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 75 out of 93 Training Loss: 0.12923523279148164 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 76 out of 93 Training Loss: 0.13719532227384867 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 77 out of 93 Training Loss: 0.1466452335776669 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 78 out of 93 Training Loss: 0.15495560093092742 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 79 out of 93 Training Loss: 0.16511329876500191 Test Loss: 0.007949867666783657\n",
      "Epoch: 3 Batch: 80 out of 93 Training Loss: 0.0018604891933760145 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 81 out of 93 Training Loss: 0.01097507221626849 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 82 out of 93 Training Loss: 0.022381825428482008 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 83 out of 93 Training Loss: 0.03137551626193137 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 84 out of 93 Training Loss: 0.04171655984598012 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 85 out of 93 Training Loss: 0.05118198512124629 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 86 out of 93 Training Loss: 0.05890562204646678 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 87 out of 93 Training Loss: 0.06700171256440492 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 88 out of 93 Training Loss: 0.07609938090669723 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 89 out of 93 Training Loss: 0.08460867689925046 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 90 out of 93 Training Loss: 0.09217537712308378 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 91 out of 93 Training Loss: 0.10050330456348867 Test Loss: 0.00797963984818621\n",
      "Epoch: 3 Batch: 92 out of 93 Training Loss: 0.10924526728692265 Test Loss: 0.00797963984818621\n",
      "Epoch: 4 Batch: 0 out of 93 Training Loss: 9.149283931780887e-05 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 1 out of 93 Training Loss: 0.0075343597570173846 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 2 out of 93 Training Loss: 0.017263155605804215 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 3 out of 93 Training Loss: 0.025378513325165997 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 4 out of 93 Training Loss: 0.034538514156054745 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 5 out of 93 Training Loss: 0.04282652790487934 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 6 out of 93 Training Loss: 0.051407612104129086 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 7 out of 93 Training Loss: 0.06065972632558275 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 8 out of 93 Training Loss: 0.06943547836334635 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 9 out of 93 Training Loss: 0.07694994163529206 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 10 out of 93 Training Loss: 0.08475963509471346 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 11 out of 93 Training Loss: 0.09255908540279795 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 12 out of 93 Training Loss: 0.10041823628188301 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 13 out of 93 Training Loss: 0.10920915304512907 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 14 out of 93 Training Loss: 0.11813210527242352 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 15 out of 93 Training Loss: 0.1258335860335939 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 16 out of 93 Training Loss: 0.13471778779120375 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 17 out of 93 Training Loss: 0.14210694952697683 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 18 out of 93 Training Loss: 0.14919094898567725 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 19 out of 93 Training Loss: 0.1588040280045681 Test Loss: 0.007953404025598005\n",
      "Epoch: 4 Batch: 20 out of 93 Training Loss: 0.0017993232023829338 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 21 out of 93 Training Loss: 0.008808135126989495 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 22 out of 93 Training Loss: 0.018089344796996726 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 23 out of 93 Training Loss: 0.026130201367002143 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 24 out of 93 Training Loss: 0.035638865155439986 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 25 out of 93 Training Loss: 0.04568906880662979 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 26 out of 93 Training Loss: 0.05559620734052004 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 27 out of 93 Training Loss: 0.06392638392494263 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 28 out of 93 Training Loss: 0.07326941478745283 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 29 out of 93 Training Loss: 0.07997360879303159 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 30 out of 93 Training Loss: 0.08785596636803092 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 31 out of 93 Training Loss: 0.09431329004214348 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 32 out of 93 Training Loss: 0.1030644998674208 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 33 out of 93 Training Loss: 0.11310734886423887 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 34 out of 93 Training Loss: 0.12144734810785593 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 35 out of 93 Training Loss: 0.1291277921547109 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 36 out of 93 Training Loss: 0.1372975586702043 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 37 out of 93 Training Loss: 0.14626478504137339 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 38 out of 93 Training Loss: 0.15333659814418496 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 39 out of 93 Training Loss: 0.16170402018101157 Test Loss: 0.007853561292656443\n",
      "Epoch: 4 Batch: 40 out of 93 Training Loss: 0.0018210056976654774 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 41 out of 93 Training Loss: 0.009899147766945626 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 42 out of 93 Training Loss: 0.01689045175847199 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 43 out of 93 Training Loss: 0.024711179062096144 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 44 out of 93 Training Loss: 0.03146656228166607 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 45 out of 93 Training Loss: 0.03957051502477911 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 46 out of 93 Training Loss: 0.046333056389657526 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 47 out of 93 Training Loss: 0.056137631162373094 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 48 out of 93 Training Loss: 0.06545935150173095 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 49 out of 93 Training Loss: 0.07603485833790687 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 50 out of 93 Training Loss: 0.08334847165208843 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 51 out of 93 Training Loss: 0.0919641377211788 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 52 out of 93 Training Loss: 0.10111395081442383 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 53 out of 93 Training Loss: 0.10854665607151177 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 54 out of 93 Training Loss: 0.11713896270778563 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 55 out of 93 Training Loss: 0.12577417164978172 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 56 out of 93 Training Loss: 0.1350057138868847 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 57 out of 93 Training Loss: 0.143419719747571 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 58 out of 93 Training Loss: 0.15078132551100637 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 59 out of 93 Training Loss: 0.1576908957020619 Test Loss: 0.007865114137530327\n",
      "Epoch: 4 Batch: 60 out of 93 Training Loss: 0.0017763765386528637 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 61 out of 93 Training Loss: 0.012021237783808198 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 62 out of 93 Training Loss: 0.02162386363812157 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 63 out of 93 Training Loss: 0.029187479321587766 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 64 out of 93 Training Loss: 0.037930581983853066 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 65 out of 93 Training Loss: 0.046292811897326196 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 66 out of 93 Training Loss: 0.05353671292283007 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 67 out of 93 Training Loss: 0.06316643188216159 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 68 out of 93 Training Loss: 0.07225279396839807 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 69 out of 93 Training Loss: 0.08121274019100139 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 70 out of 93 Training Loss: 0.08823425069891283 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 71 out of 93 Training Loss: 0.0974357344999201 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 72 out of 93 Training Loss: 0.10728878554396698 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 73 out of 93 Training Loss: 0.11581982791863749 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 74 out of 93 Training Loss: 0.12414831694655487 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 75 out of 93 Training Loss: 0.13321506537936995 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 76 out of 93 Training Loss: 0.14128871474855015 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 77 out of 93 Training Loss: 0.148951211743731 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 78 out of 93 Training Loss: 0.15800579430975387 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 79 out of 93 Training Loss: 0.1664399046308584 Test Loss: 0.00786874375560067\n",
      "Epoch: 4 Batch: 80 out of 93 Training Loss: 0.0018635984658866875 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 81 out of 93 Training Loss: 0.010982385478880977 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 82 out of 93 Training Loss: 0.01930608192082939 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 83 out of 93 Training Loss: 0.027653043732014274 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 84 out of 93 Training Loss: 0.03616750297692117 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 85 out of 93 Training Loss: 0.04490986244377193 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 86 out of 93 Training Loss: 0.053818516075578304 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 87 out of 93 Training Loss: 0.06102292948957977 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 88 out of 93 Training Loss: 0.06908227933194933 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 89 out of 93 Training Loss: 0.07701930367943344 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 90 out of 93 Training Loss: 0.08501052448150215 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 91 out of 93 Training Loss: 0.09319016301628646 Test Loss: 0.007812963459979404\n",
      "Epoch: 4 Batch: 92 out of 93 Training Loss: 0.0990475263595253 Test Loss: 0.007812963459979404\n",
      "Epoch: 5 Batch: 0 out of 93 Training Loss: 9.640493500296788e-05 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 1 out of 93 Training Loss: 0.009232659834206746 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 2 out of 93 Training Loss: 0.017290182421684907 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 3 out of 93 Training Loss: 0.02421000461164181 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 4 out of 93 Training Loss: 0.0319593604754216 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 5 out of 93 Training Loss: 0.04139020946878259 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 6 out of 93 Training Loss: 0.05091840495425527 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 7 out of 93 Training Loss: 0.05900966842728918 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 8 out of 93 Training Loss: 0.06710566208767955 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 9 out of 93 Training Loss: 0.07416258986178105 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 10 out of 93 Training Loss: 0.08051271458226507 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 11 out of 93 Training Loss: 0.08924584557372396 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 12 out of 93 Training Loss: 0.09895907250303095 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 13 out of 93 Training Loss: 0.10752203316497867 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 14 out of 93 Training Loss: 0.11475085421535437 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 15 out of 93 Training Loss: 0.12435614365223115 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 16 out of 93 Training Loss: 0.13281436591479245 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 17 out of 93 Training Loss: 0.13994978462415997 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 18 out of 93 Training Loss: 0.1489126840706474 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 19 out of 93 Training Loss: 0.1569015607337958 Test Loss: 0.007888916625895283\n",
      "Epoch: 5 Batch: 20 out of 93 Training Loss: 0.0017624770152027316 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 21 out of 93 Training Loss: 0.01179497715557167 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 22 out of 93 Training Loss: 0.022329658535366746 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 23 out of 93 Training Loss: 0.03160487886989662 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 24 out of 93 Training Loss: 0.03904448848689148 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 25 out of 93 Training Loss: 0.046198703148459884 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 26 out of 93 Training Loss: 0.05489140356430957 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 27 out of 93 Training Loss: 0.06232630674267361 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 28 out of 93 Training Loss: 0.07174684692645619 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 29 out of 93 Training Loss: 0.08106651742482732 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 30 out of 93 Training Loss: 0.09122840248519729 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 31 out of 93 Training Loss: 0.09947418448055337 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 32 out of 93 Training Loss: 0.10736777194315741 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 33 out of 93 Training Loss: 0.11611336775595496 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 34 out of 93 Training Loss: 0.12381767368415544 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 35 out of 93 Training Loss: 0.13219003359327267 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 36 out of 93 Training Loss: 0.13953705857047985 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 37 out of 93 Training Loss: 0.14725236714357684 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 38 out of 93 Training Loss: 0.15518401217186997 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 39 out of 93 Training Loss: 0.1626374142105336 Test Loss: 0.007754905107007785\n",
      "Epoch: 5 Batch: 40 out of 93 Training Loss: 0.0018437389883729665 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 41 out of 93 Training Loss: 0.0098107444721791 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 42 out of 93 Training Loss: 0.017221965980115814 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 43 out of 93 Training Loss: 0.026693868603888435 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 44 out of 93 Training Loss: 0.03536502100932924 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 45 out of 93 Training Loss: 0.04320095308530657 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 46 out of 93 Training Loss: 0.050616368804458065 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 47 out of 93 Training Loss: 0.05930813137847511 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 48 out of 93 Training Loss: 0.0662951014604005 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 49 out of 93 Training Loss: 0.07298010649565188 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 50 out of 93 Training Loss: 0.08034591066125361 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 51 out of 93 Training Loss: 0.08754548259485094 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 52 out of 93 Training Loss: 0.09513304910096852 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 53 out of 93 Training Loss: 0.10630455861273734 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 54 out of 93 Training Loss: 0.11558253286812036 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 55 out of 93 Training Loss: 0.12386411304059951 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 56 out of 93 Training Loss: 0.13222978046569556 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 57 out of 93 Training Loss: 0.14155600840631455 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 58 out of 93 Training Loss: 0.14880508944797843 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 59 out of 93 Training Loss: 0.15797328498620122 Test Loss: 0.00775626145133918\n",
      "Epoch: 5 Batch: 60 out of 93 Training Loss: 0.0017871778494844225 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 61 out of 93 Training Loss: 0.010595502894295195 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 62 out of 93 Training Loss: 0.020918069969786145 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 63 out of 93 Training Loss: 0.0281742439641783 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 64 out of 93 Training Loss: 0.036514000132603384 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 65 out of 93 Training Loss: 0.04405968599386308 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 66 out of 93 Training Loss: 0.05254705120242927 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 67 out of 93 Training Loss: 0.06080571804560516 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 68 out of 93 Training Loss: 0.06996880993611906 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 69 out of 93 Training Loss: 0.07808211222655867 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 70 out of 93 Training Loss: 0.08644010182834957 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 71 out of 93 Training Loss: 0.09728402793772314 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 72 out of 93 Training Loss: 0.10513914619006012 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 73 out of 93 Training Loss: 0.11254479186974262 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 74 out of 93 Training Loss: 0.12155147558443044 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 75 out of 93 Training Loss: 0.12930017626277898 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 76 out of 93 Training Loss: 0.13798308940969203 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 77 out of 93 Training Loss: 0.14555375147975777 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 78 out of 93 Training Loss: 0.15424577668674086 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 79 out of 93 Training Loss: 0.16261607464767788 Test Loss: 0.007762695832008665\n",
      "Epoch: 5 Batch: 80 out of 93 Training Loss: 0.0018290557898973366 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 81 out of 93 Training Loss: 0.010517083760390176 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 82 out of 93 Training Loss: 0.01846510516483535 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 83 out of 93 Training Loss: 0.028962261695454017 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 84 out of 93 Training Loss: 0.036890441272447005 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 85 out of 93 Training Loss: 0.044426005054304496 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 86 out of 93 Training Loss: 0.05203805463392486 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 87 out of 93 Training Loss: 0.059864476975092784 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 88 out of 93 Training Loss: 0.0685664915480112 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 89 out of 93 Training Loss: 0.07697465708483209 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 90 out of 93 Training Loss: 0.08566602198410977 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 91 out of 93 Training Loss: 0.09329060763124575 Test Loss: 0.007765625688162717\n",
      "Epoch: 5 Batch: 92 out of 93 Training Loss: 0.10196719337258686 Test Loss: 0.007765625688162717\n",
      "Epoch: 6 Batch: 0 out of 93 Training Loss: 8.348525772171635e-05 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 1 out of 93 Training Loss: 0.00822710489193278 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 2 out of 93 Training Loss: 0.017162169900632674 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 3 out of 93 Training Loss: 0.025082502153611953 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 4 out of 93 Training Loss: 0.03363365737060385 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 5 out of 93 Training Loss: 0.04216866345415192 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 6 out of 93 Training Loss: 0.050441228424108794 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 7 out of 93 Training Loss: 0.059159540210760406 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 8 out of 93 Training Loss: 0.06789240950057583 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 9 out of 93 Training Loss: 0.07620824185470419 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 10 out of 93 Training Loss: 0.08499489316055851 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 11 out of 93 Training Loss: 0.09356136211464482 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 12 out of 93 Training Loss: 0.10155452938089447 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 13 out of 93 Training Loss: 0.11007486713389235 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 14 out of 93 Training Loss: 0.11777942079389768 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 15 out of 93 Training Loss: 0.12505524802292067 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 16 out of 93 Training Loss: 0.13429074465567548 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 17 out of 93 Training Loss: 0.14205656537125189 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 18 out of 93 Training Loss: 0.14868362584421713 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 19 out of 93 Training Loss: 0.15638406771505553 Test Loss: 0.007897016338326714\n",
      "Epoch: 6 Batch: 20 out of 93 Training Loss: 0.0017899857665031912 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 21 out of 93 Training Loss: 0.009115718847009516 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 22 out of 93 Training Loss: 0.01801627462902961 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 23 out of 93 Training Loss: 0.025263459497931576 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 24 out of 93 Training Loss: 0.03258941190490422 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 25 out of 93 Training Loss: 0.04162029044875798 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 26 out of 93 Training Loss: 0.04907990999692974 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 27 out of 93 Training Loss: 0.05595401263037619 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 28 out of 93 Training Loss: 0.06484222652563748 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 29 out of 93 Training Loss: 0.07166768245572147 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 30 out of 93 Training Loss: 0.08024595126682815 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 31 out of 93 Training Loss: 0.08773681948134598 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 32 out of 93 Training Loss: 0.09554718163037595 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 33 out of 93 Training Loss: 0.10379890825772581 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 34 out of 93 Training Loss: 0.11105252716863451 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 35 out of 93 Training Loss: 0.11960581325167713 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 36 out of 93 Training Loss: 0.1279748474078804 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 37 out of 93 Training Loss: 0.137305888423565 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 38 out of 93 Training Loss: 0.14557684406811294 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 39 out of 93 Training Loss: 0.15460189160132704 Test Loss: 0.00788037237626585\n",
      "Epoch: 6 Batch: 40 out of 93 Training Loss: 0.0017526506063944868 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 41 out of 93 Training Loss: 0.010688859279216008 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 42 out of 93 Training Loss: 0.018621032367408946 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 43 out of 93 Training Loss: 0.026823598201335147 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 44 out of 93 Training Loss: 0.03495228128409453 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 45 out of 93 Training Loss: 0.043441902155936435 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 46 out of 93 Training Loss: 0.052061818781257346 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 47 out of 93 Training Loss: 0.05993658700859614 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 48 out of 93 Training Loss: 0.06722972938692637 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 49 out of 93 Training Loss: 0.07429156523099609 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 50 out of 93 Training Loss: 0.08338278137049146 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 51 out of 93 Training Loss: 0.09112670855498381 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 52 out of 93 Training Loss: 0.0982632178430862 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 53 out of 93 Training Loss: 0.10640242207697101 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 54 out of 93 Training Loss: 0.1144317814697928 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 55 out of 93 Training Loss: 0.12369784038415618 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 56 out of 93 Training Loss: 0.13106866577541895 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 57 out of 93 Training Loss: 0.1403858850327737 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 58 out of 93 Training Loss: 0.15004006145423718 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 59 out of 93 Training Loss: 0.15967883566296168 Test Loss: 0.007799825461750681\n",
      "Epoch: 6 Batch: 60 out of 93 Training Loss: 0.0018038418149890511 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 61 out of 93 Training Loss: 0.009018697217697104 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 62 out of 93 Training Loss: 0.017308253236705262 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 63 out of 93 Training Loss: 0.026192608662539918 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 64 out of 93 Training Loss: 0.03469412531762739 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 65 out of 93 Training Loss: 0.04249131996243616 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 66 out of 93 Training Loss: 0.05088642928808351 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 67 out of 93 Training Loss: 0.060956266150468785 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 68 out of 93 Training Loss: 0.07053759358882328 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 69 out of 93 Training Loss: 0.07872810717731138 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 70 out of 93 Training Loss: 0.0864664110360982 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 71 out of 93 Training Loss: 0.09514260375931283 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 72 out of 93 Training Loss: 0.10412734551414271 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 73 out of 93 Training Loss: 0.11110433600901981 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 74 out of 93 Training Loss: 0.11877410849615355 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 75 out of 93 Training Loss: 0.12725659357442876 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 76 out of 93 Training Loss: 0.13649737639411708 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 77 out of 93 Training Loss: 0.14440262960329076 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 78 out of 93 Training Loss: 0.1520366565330269 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 79 out of 93 Training Loss: 0.16083712214663048 Test Loss: 0.007899038451300426\n",
      "Epoch: 6 Batch: 80 out of 93 Training Loss: 0.0018333688228184199 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 81 out of 93 Training Loss: 0.010609789550190589 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 82 out of 93 Training Loss: 0.018479454129820487 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 83 out of 93 Training Loss: 0.026497574321811816 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 84 out of 93 Training Loss: 0.03486535398650708 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 85 out of 93 Training Loss: 0.0424587636104578 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 86 out of 93 Training Loss: 0.050021491743510386 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 87 out of 93 Training Loss: 0.05911347715545239 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 88 out of 93 Training Loss: 0.06738457063574138 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 89 out of 93 Training Loss: 0.07662491573381963 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 90 out of 93 Training Loss: 0.08534637818474117 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 91 out of 93 Training Loss: 0.09343857298034969 Test Loss: 0.007904819805513729\n",
      "Epoch: 6 Batch: 92 out of 93 Training Loss: 0.1012437714113647 Test Loss: 0.007904819805513729\n",
      "Epoch: 7 Batch: 0 out of 93 Training Loss: 9.113903688166731e-05 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 1 out of 93 Training Loss: 0.008421312827336531 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 2 out of 93 Training Loss: 0.016365850154030068 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 3 out of 93 Training Loss: 0.025298598963486893 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 4 out of 93 Training Loss: 0.03230134564982627 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 5 out of 93 Training Loss: 0.04104659687327121 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 6 out of 93 Training Loss: 0.04898727945582841 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 7 out of 93 Training Loss: 0.05804585273908351 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 8 out of 93 Training Loss: 0.06585315223382686 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 9 out of 93 Training Loss: 0.07405345774786447 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 10 out of 93 Training Loss: 0.08191684148805116 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 11 out of 93 Training Loss: 0.0923450903926966 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 12 out of 93 Training Loss: 0.09988095266343926 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 13 out of 93 Training Loss: 0.10747691969917987 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 14 out of 93 Training Loss: 0.11684795729415391 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 15 out of 93 Training Loss: 0.12443352518202637 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 16 out of 93 Training Loss: 0.13273760145129537 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 17 out of 93 Training Loss: 0.14060120218964195 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 18 out of 93 Training Loss: 0.14972464767815447 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 19 out of 93 Training Loss: 0.15885260076792812 Test Loss: 0.007823858029124412\n",
      "Epoch: 7 Batch: 20 out of 93 Training Loss: 0.0018161191804885197 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 21 out of 93 Training Loss: 0.009848207348453932 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 22 out of 93 Training Loss: 0.020672326089310578 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 23 out of 93 Training Loss: 0.02920039568232291 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 24 out of 93 Training Loss: 0.038638034017252854 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 25 out of 93 Training Loss: 0.04568884626166814 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 26 out of 93 Training Loss: 0.05486422415958636 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 27 out of 93 Training Loss: 0.06438710738109343 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 28 out of 93 Training Loss: 0.07395319558815949 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 29 out of 93 Training Loss: 0.08205419324474328 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 30 out of 93 Training Loss: 0.09076226700233214 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 31 out of 93 Training Loss: 0.09918311622129672 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 32 out of 93 Training Loss: 0.10766703367369168 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 33 out of 93 Training Loss: 0.11484509438412183 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 34 out of 93 Training Loss: 0.12188285460831516 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 35 out of 93 Training Loss: 0.13147659378262155 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 36 out of 93 Training Loss: 0.13923132337765687 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 37 out of 93 Training Loss: 0.1470282218618869 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 38 out of 93 Training Loss: 0.1568774162991523 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 39 out of 93 Training Loss: 0.16412637720020407 Test Loss: 0.007866365877403454\n",
      "Epoch: 7 Batch: 40 out of 93 Training Loss: 0.001842591341960045 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 41 out of 93 Training Loss: 0.009758901212739559 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 42 out of 93 Training Loss: 0.01695624262877068 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 43 out of 93 Training Loss: 0.025204323128240915 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 44 out of 93 Training Loss: 0.03280302732102833 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 45 out of 93 Training Loss: 0.04351715500317774 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 46 out of 93 Training Loss: 0.052171026718246555 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 47 out of 93 Training Loss: 0.060213738482820606 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 48 out of 93 Training Loss: 0.06947896293215713 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 49 out of 93 Training Loss: 0.07871214709095917 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 50 out of 93 Training Loss: 0.08678916505389175 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 51 out of 93 Training Loss: 0.09599001462422571 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 52 out of 93 Training Loss: 0.10440068635009489 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 53 out of 93 Training Loss: 0.11165920588039122 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 54 out of 93 Training Loss: 0.11941644166447482 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 55 out of 93 Training Loss: 0.1266765812303897 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 56 out of 93 Training Loss: 0.1351986968781825 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 57 out of 93 Training Loss: 0.14289802302457652 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 58 out of 93 Training Loss: 0.15210817464478574 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 59 out of 93 Training Loss: 0.16235905689038596 Test Loss: 0.0076748240231113\n",
      "Epoch: 7 Batch: 60 out of 93 Training Loss: 0.0018186642657004337 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 61 out of 93 Training Loss: 0.01036091790152416 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 62 out of 93 Training Loss: 0.017723270120003076 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 63 out of 93 Training Loss: 0.025095614498385044 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 64 out of 93 Training Loss: 0.033080719305106254 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 65 out of 93 Training Loss: 0.04170002312017307 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 66 out of 93 Training Loss: 0.048430009173133226 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 67 out of 93 Training Loss: 0.056605039523818346 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 68 out of 93 Training Loss: 0.06418998715085611 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 69 out of 93 Training Loss: 0.07265800227028951 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 70 out of 93 Training Loss: 0.08042694012252316 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 71 out of 93 Training Loss: 0.0876072131897293 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 72 out of 93 Training Loss: 0.0960636715273343 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 73 out of 93 Training Loss: 0.1042833234052144 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 74 out of 93 Training Loss: 0.11054307351438031 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 75 out of 93 Training Loss: 0.11995926255909428 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 76 out of 93 Training Loss: 0.12923024630723223 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 77 out of 93 Training Loss: 0.13791897649494633 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 78 out of 93 Training Loss: 0.14560574282509908 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 79 out of 93 Training Loss: 0.1520107499192439 Test Loss: 0.007727954028682275\n",
      "Epoch: 7 Batch: 80 out of 93 Training Loss: 0.0017237874557684426 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 81 out of 93 Training Loss: 0.01104525633803115 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 82 out of 93 Training Loss: 0.01909277652105794 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 83 out of 93 Training Loss: 0.02753287245234952 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 84 out of 93 Training Loss: 0.03575471014431224 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 85 out of 93 Training Loss: 0.04314298665797815 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 86 out of 93 Training Loss: 0.050378306942940806 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 87 out of 93 Training Loss: 0.05838026496297226 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 88 out of 93 Training Loss: 0.06791497225588665 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 89 out of 93 Training Loss: 0.07661893701827631 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 90 out of 93 Training Loss: 0.08324866526475654 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 91 out of 93 Training Loss: 0.09312345524034009 Test Loss: 0.007798845752735029\n",
      "Epoch: 7 Batch: 92 out of 93 Training Loss: 0.10474548459699855 Test Loss: 0.007798845752735029\n",
      "Epoch: 8 Batch: 0 out of 93 Training Loss: 7.962571677341257e-05 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 1 out of 93 Training Loss: 0.00769926371273174 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 2 out of 93 Training Loss: 0.016079746617344758 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 3 out of 93 Training Loss: 0.023729721980509898 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 4 out of 93 Training Loss: 0.0313911162897624 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 5 out of 93 Training Loss: 0.03965274008211269 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 6 out of 93 Training Loss: 0.04828536087867393 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 7 out of 93 Training Loss: 0.055256854242054364 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 8 out of 93 Training Loss: 0.06409546589699164 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 9 out of 93 Training Loss: 0.0707605825386621 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 10 out of 93 Training Loss: 0.07998792823624387 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 11 out of 93 Training Loss: 0.08855672642808929 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 12 out of 93 Training Loss: 0.09595232732814327 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 13 out of 93 Training Loss: 0.10517051439952627 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 14 out of 93 Training Loss: 0.11446057163399234 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 15 out of 93 Training Loss: 0.1224642246611096 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 16 out of 93 Training Loss: 0.130948522093115 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 17 out of 93 Training Loss: 0.14015541408431306 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 18 out of 93 Training Loss: 0.147745752672074 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 19 out of 93 Training Loss: 0.15540705012377873 Test Loss: 0.0078037275763397865\n",
      "Epoch: 8 Batch: 20 out of 93 Training Loss: 0.0017618927349803669 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 21 out of 93 Training Loss: 0.01070069234720447 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 22 out of 93 Training Loss: 0.01892217054865339 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 23 out of 93 Training Loss: 0.02823106098822334 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 24 out of 93 Training Loss: 0.038779530598735595 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 25 out of 93 Training Loss: 0.04714561711899021 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 26 out of 93 Training Loss: 0.05528041899106481 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 27 out of 93 Training Loss: 0.062151269911741996 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 28 out of 93 Training Loss: 0.0710248666991947 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 29 out of 93 Training Loss: 0.08048555899313667 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 30 out of 93 Training Loss: 0.08890976935527303 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 31 out of 93 Training Loss: 0.09669730944461205 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 32 out of 93 Training Loss: 0.10479502325779774 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 33 out of 93 Training Loss: 0.11296632179892638 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 34 out of 93 Training Loss: 0.1216132151139436 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 35 out of 93 Training Loss: 0.13030383216119387 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 36 out of 93 Training Loss: 0.13923563195294716 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 37 out of 93 Training Loss: 0.1466889203702567 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 38 out of 93 Training Loss: 0.1551335099329708 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 39 out of 93 Training Loss: 0.1633630235855935 Test Loss: 0.007826142521067099\n",
      "Epoch: 8 Batch: 40 out of 93 Training Loss: 0.0018453397329350931 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 41 out of 93 Training Loss: 0.010435685003866813 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 42 out of 93 Training Loss: 0.018744711252619884 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 43 out of 93 Training Loss: 0.028027333738972805 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 44 out of 93 Training Loss: 0.037074975157847065 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 45 out of 93 Training Loss: 0.04409269425155725 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 46 out of 93 Training Loss: 0.052457851311882395 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 47 out of 93 Training Loss: 0.06251403312267866 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 48 out of 93 Training Loss: 0.07187579753877725 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 49 out of 93 Training Loss: 0.08005237295361366 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 50 out of 93 Training Loss: 0.08778248070808257 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 51 out of 93 Training Loss: 0.09550858610378708 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 52 out of 93 Training Loss: 0.10382941500352826 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 53 out of 93 Training Loss: 0.11248967466013636 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 54 out of 93 Training Loss: 0.12074516103671994 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 55 out of 93 Training Loss: 0.1292472914333841 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 56 out of 93 Training Loss: 0.13723016479121652 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 57 out of 93 Training Loss: 0.14806155157344547 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 58 out of 93 Training Loss: 0.15622735158609358 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 59 out of 93 Training Loss: 0.16314441519858447 Test Loss: 0.007733210243962028\n",
      "Epoch: 8 Batch: 60 out of 93 Training Loss: 0.001827621125994084 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 61 out of 93 Training Loss: 0.00876381736379659 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 62 out of 93 Training Loss: 0.016584151649294256 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 63 out of 93 Training Loss: 0.02476526446771419 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 64 out of 93 Training Loss: 0.03203618222978508 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 65 out of 93 Training Loss: 0.039666955118445514 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 66 out of 93 Training Loss: 0.04876331588384306 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 67 out of 93 Training Loss: 0.0567707631191293 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 68 out of 93 Training Loss: 0.06474849185105955 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 69 out of 93 Training Loss: 0.07316165765878355 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 70 out of 93 Training Loss: 0.08219778793063556 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 71 out of 93 Training Loss: 0.09079069899526035 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 72 out of 93 Training Loss: 0.09780282968786155 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 73 out of 93 Training Loss: 0.10694253327455913 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 74 out of 93 Training Loss: 0.11478282487150823 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 75 out of 93 Training Loss: 0.12294287303593551 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 76 out of 93 Training Loss: 0.13232012162115966 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 77 out of 93 Training Loss: 0.1405366610189954 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 78 out of 93 Training Loss: 0.1496704786470214 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 79 out of 93 Training Loss: 0.1585955859547535 Test Loss: 0.0078070306388491936\n",
      "Epoch: 8 Batch: 80 out of 93 Training Loss: 0.0018118331883924404 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 81 out of 93 Training Loss: 0.009095549185283701 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 82 out of 93 Training Loss: 0.01611251670167617 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 83 out of 93 Training Loss: 0.023676182676561394 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 84 out of 93 Training Loss: 0.031210241906799593 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 85 out of 93 Training Loss: 0.03977673973188571 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 86 out of 93 Training Loss: 0.04724469554842285 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 87 out of 93 Training Loss: 0.05533094757468275 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 88 out of 93 Training Loss: 0.06348786183864645 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 89 out of 93 Training Loss: 0.0728782485914307 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 90 out of 93 Training Loss: 0.08113061690003447 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 91 out of 93 Training Loss: 0.09110564046771101 Test Loss: 0.00788965867832303\n",
      "Epoch: 8 Batch: 92 out of 93 Training Loss: 0.0983058629301267 Test Loss: 0.00788965867832303\n",
      "Epoch: 9 Batch: 0 out of 93 Training Loss: 9.3875883487604e-05 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 1 out of 93 Training Loss: 0.00932941535708084 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 2 out of 93 Training Loss: 0.016420798341152808 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 3 out of 93 Training Loss: 0.024140335118738553 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 4 out of 93 Training Loss: 0.031584459787575146 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 5 out of 93 Training Loss: 0.04016571535017862 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 6 out of 93 Training Loss: 0.04739257177558317 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 7 out of 93 Training Loss: 0.05538827037659063 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 8 out of 93 Training Loss: 0.06327210875448361 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 9 out of 93 Training Loss: 0.07163660457578078 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 10 out of 93 Training Loss: 0.08076556349393502 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 11 out of 93 Training Loss: 0.0878735184318997 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 12 out of 93 Training Loss: 0.09685095582377687 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 13 out of 93 Training Loss: 0.10525125075351968 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 14 out of 93 Training Loss: 0.11343376800638214 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 15 out of 93 Training Loss: 0.12082454651814475 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 16 out of 93 Training Loss: 0.12912389446270242 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 17 out of 93 Training Loss: 0.1380319604299642 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 18 out of 93 Training Loss: 0.1450513740650989 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 19 out of 93 Training Loss: 0.15312349699657932 Test Loss: 0.007752409712834792\n",
      "Epoch: 9 Batch: 20 out of 93 Training Loss: 0.0017350778200209542 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 21 out of 93 Training Loss: 0.009418017345714609 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 22 out of 93 Training Loss: 0.017806050482440036 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 23 out of 93 Training Loss: 0.026612899380970043 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 24 out of 93 Training Loss: 0.03465122554754023 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 25 out of 93 Training Loss: 0.04358863655780558 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 26 out of 93 Training Loss: 0.052415893527913135 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 27 out of 93 Training Loss: 0.060923406969058555 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 28 out of 93 Training Loss: 0.0690709493339539 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 29 out of 93 Training Loss: 0.07739822484855656 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 30 out of 93 Training Loss: 0.08678535010968451 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 31 out of 93 Training Loss: 0.0947024385661364 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 32 out of 93 Training Loss: 0.10313333675568108 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 33 out of 93 Training Loss: 0.11090344155510072 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 34 out of 93 Training Loss: 0.11958615633030538 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 35 out of 93 Training Loss: 0.12779881676961785 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 36 out of 93 Training Loss: 0.13571628706832534 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 37 out of 93 Training Loss: 0.14206515450333362 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 38 out of 93 Training Loss: 0.14990896463696962 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 39 out of 93 Training Loss: 0.15805673890118604 Test Loss: 0.007739159955897115\n",
      "Epoch: 9 Batch: 40 out of 93 Training Loss: 0.0017857594247632136 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 41 out of 93 Training Loss: 0.009407179506468069 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 42 out of 93 Training Loss: 0.016054159106659187 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 43 out of 93 Training Loss: 0.02420419927869965 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 44 out of 93 Training Loss: 0.031645457802312864 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 45 out of 93 Training Loss: 0.040538887103992 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 46 out of 93 Training Loss: 0.04991951732148458 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 47 out of 93 Training Loss: 0.05822698639889289 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 48 out of 93 Training Loss: 0.06624265534986784 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 49 out of 93 Training Loss: 0.07544350026358893 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 50 out of 93 Training Loss: 0.0844331770967417 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 51 out of 93 Training Loss: 0.09177304253165652 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 52 out of 93 Training Loss: 0.1008239966649287 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 53 out of 93 Training Loss: 0.10949300795977046 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 54 out of 93 Training Loss: 0.11858161080812146 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 55 out of 93 Training Loss: 0.12870863460187842 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 56 out of 93 Training Loss: 0.13673870925043752 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 57 out of 93 Training Loss: 0.14529198695157697 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 58 out of 93 Training Loss: 0.15324589617733886 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 59 out of 93 Training Loss: 0.16296710360978772 Test Loss: 0.007838717111471024\n",
      "Epoch: 9 Batch: 60 out of 93 Training Loss: 0.0018440699713722277 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 61 out of 93 Training Loss: 0.009780671498407464 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 62 out of 93 Training Loss: 0.018758478625346763 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 63 out of 93 Training Loss: 0.025464296857495648 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 64 out of 93 Training Loss: 0.034367916690786225 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 65 out of 93 Training Loss: 0.043060182178993565 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 66 out of 93 Training Loss: 0.05160669754691873 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 67 out of 93 Training Loss: 0.05982539388783727 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 68 out of 93 Training Loss: 0.0679498562166885 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 69 out of 93 Training Loss: 0.07726713694133316 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 70 out of 93 Training Loss: 0.08514915924020564 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 71 out of 93 Training Loss: 0.09431956760086571 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 72 out of 93 Training Loss: 0.10231866180398022 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 73 out of 93 Training Loss: 0.10960466885440504 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 74 out of 93 Training Loss: 0.11741371178500806 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 75 out of 93 Training Loss: 0.1257914528759614 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 76 out of 93 Training Loss: 0.13469329015784418 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 77 out of 93 Training Loss: 0.1435403414579526 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 78 out of 93 Training Loss: 0.15209193067126428 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 79 out of 93 Training Loss: 0.16037222189330017 Test Loss: 0.007944947701286186\n",
      "Epoch: 9 Batch: 80 out of 93 Training Loss: 0.0018025426930582177 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 81 out of 93 Training Loss: 0.009702587946394647 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 82 out of 93 Training Loss: 0.017719095087686744 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 83 out of 93 Training Loss: 0.027060939445236888 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 84 out of 93 Training Loss: 0.03532115321520921 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 85 out of 93 Training Loss: 0.04511756308291313 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 86 out of 93 Training Loss: 0.052987933605054584 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 87 out of 93 Training Loss: 0.0612226123726761 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 88 out of 93 Training Loss: 0.06903265880335327 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 89 out of 93 Training Loss: 0.07704394308764692 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 90 out of 93 Training Loss: 0.08643392005849834 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 91 out of 93 Training Loss: 0.09620720891196485 Test Loss: 0.007793581917543303\n",
      "Epoch: 9 Batch: 92 out of 93 Training Loss: 0.10265985967505928 Test Loss: 0.007793581917543303\n",
      "Epoch: 10 Batch: 0 out of 93 Training Loss: 7.551066797747407e-05 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 1 out of 93 Training Loss: 0.007613448856738946 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 2 out of 93 Training Loss: 0.014768823014912745 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 3 out of 93 Training Loss: 0.022399748640475413 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 4 out of 93 Training Loss: 0.0307786147690989 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 5 out of 93 Training Loss: 0.03894315097701325 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 6 out of 93 Training Loss: 0.04528467400219812 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 7 out of 93 Training Loss: 0.05406691456433906 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 8 out of 93 Training Loss: 0.060609463101593396 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 9 out of 93 Training Loss: 0.07025820711847916 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 10 out of 93 Training Loss: 0.07771620709669366 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 11 out of 93 Training Loss: 0.08595723469030633 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 12 out of 93 Training Loss: 0.09205039773356691 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 13 out of 93 Training Loss: 0.100511101039467 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 14 out of 93 Training Loss: 0.10732480460998192 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 15 out of 93 Training Loss: 0.11451857486960068 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 16 out of 93 Training Loss: 0.12347606985118761 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 17 out of 93 Training Loss: 0.1329495398908533 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 18 out of 93 Training Loss: 0.13887652285617366 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 19 out of 93 Training Loss: 0.14761122778278365 Test Loss: 0.0077139724638651715\n",
      "Epoch: 10 Batch: 20 out of 93 Training Loss: 0.0016620009082295962 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 21 out of 93 Training Loss: 0.010229144139859921 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 22 out of 93 Training Loss: 0.0181215637239554 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 23 out of 93 Training Loss: 0.02688958671567226 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 24 out of 93 Training Loss: 0.03554800049093748 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 25 out of 93 Training Loss: 0.042897032002631674 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 26 out of 93 Training Loss: 0.05086995883536721 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 27 out of 93 Training Loss: 0.059498348633471976 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 28 out of 93 Training Loss: 0.06736091001582528 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 29 out of 93 Training Loss: 0.07503642736626054 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 30 out of 93 Training Loss: 0.0837415842215457 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 31 out of 93 Training Loss: 0.09226381628466035 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 32 out of 93 Training Loss: 0.10038158985478068 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 33 out of 93 Training Loss: 0.10940895638120557 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 34 out of 93 Training Loss: 0.11856676581007625 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 35 out of 93 Training Loss: 0.12650960533571626 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 36 out of 93 Training Loss: 0.1356037616605797 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 37 out of 93 Training Loss: 0.14523035044473792 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 38 out of 93 Training Loss: 0.15369357949746038 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 39 out of 93 Training Loss: 0.16257080937427665 Test Loss: 0.007774403149431402\n",
      "Epoch: 10 Batch: 40 out of 93 Training Loss: 0.001828831078611756 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 41 out of 93 Training Loss: 0.011614489758514309 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 42 out of 93 Training Loss: 0.020953236537181282 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 43 out of 93 Training Loss: 0.030541118623815918 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 44 out of 93 Training Loss: 0.037525452597193865 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 45 out of 93 Training Loss: 0.045428351422898916 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 46 out of 93 Training Loss: 0.05458301856940284 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 47 out of 93 Training Loss: 0.06320380784397617 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 48 out of 93 Training Loss: 0.07041536263947978 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 49 out of 93 Training Loss: 0.07914381746029153 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 50 out of 93 Training Loss: 0.0876800342631866 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 51 out of 93 Training Loss: 0.09757620181714788 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 52 out of 93 Training Loss: 0.10653406467115178 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 53 out of 93 Training Loss: 0.11459918685083881 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 54 out of 93 Training Loss: 0.12376640356903568 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 55 out of 93 Training Loss: 0.13166824068700567 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 56 out of 93 Training Loss: 0.14146213598375812 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 57 out of 93 Training Loss: 0.15051228917961612 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 58 out of 93 Training Loss: 0.15976955268358484 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 59 out of 93 Training Loss: 0.16719151792874232 Test Loss: 0.007739067966626449\n",
      "Epoch: 10 Batch: 60 out of 93 Training Loss: 0.0018962299873645475 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 61 out of 93 Training Loss: 0.008487778664898412 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 62 out of 93 Training Loss: 0.01588318771250202 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 63 out of 93 Training Loss: 0.02291701868675305 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 64 out of 93 Training Loss: 0.0317198964258249 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 65 out of 93 Training Loss: 0.038978701406490346 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 66 out of 93 Training Loss: 0.04739983654148533 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 67 out of 93 Training Loss: 0.0571865364400263 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 68 out of 93 Training Loss: 0.06474792207158758 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 69 out of 93 Training Loss: 0.07230162475756957 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 70 out of 93 Training Loss: 0.08160649560890748 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 71 out of 93 Training Loss: 0.08968466375521972 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 72 out of 93 Training Loss: 0.09681281256056978 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 73 out of 93 Training Loss: 0.10395402642465784 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 74 out of 93 Training Loss: 0.1117885997009988 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 75 out of 93 Training Loss: 0.11899619311593129 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 76 out of 93 Training Loss: 0.1296265278433974 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 77 out of 93 Training Loss: 0.13758143172166898 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 78 out of 93 Training Loss: 0.14669535164288833 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 79 out of 93 Training Loss: 0.15490030166260554 Test Loss: 0.007853542877869173\n",
      "Epoch: 10 Batch: 80 out of 93 Training Loss: 0.0017514441066747662 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 81 out of 93 Training Loss: 0.010626714480794048 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 82 out of 93 Training Loss: 0.020402411734438512 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 83 out of 93 Training Loss: 0.0309538336375123 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 84 out of 93 Training Loss: 0.03981625641671476 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 85 out of 93 Training Loss: 0.046714110569781635 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 86 out of 93 Training Loss: 0.05436477138636169 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 87 out of 93 Training Loss: 0.06237299534467516 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 88 out of 93 Training Loss: 0.07034930287835656 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 89 out of 93 Training Loss: 0.07732648289737759 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 90 out of 93 Training Loss: 0.08760116881666241 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 91 out of 93 Training Loss: 0.09507284712953029 Test Loss: 0.007712569003078071\n",
      "Epoch: 10 Batch: 92 out of 93 Training Loss: 0.10428205438864885 Test Loss: 0.007712569003078071\n",
      "Epoch: 11 Batch: 0 out of 93 Training Loss: 9.316705688033052e-05 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 1 out of 93 Training Loss: 0.006619712212673759 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 2 out of 93 Training Loss: 0.017512632408730126 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 3 out of 93 Training Loss: 0.02605162082760725 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 4 out of 93 Training Loss: 0.03421030460446272 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 5 out of 93 Training Loss: 0.040638332130006886 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 6 out of 93 Training Loss: 0.04978754731415901 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 7 out of 93 Training Loss: 0.05877316823749933 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 8 out of 93 Training Loss: 0.06706907401382123 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 9 out of 93 Training Loss: 0.0749388643219987 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 10 out of 93 Training Loss: 0.08199284678845797 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 11 out of 93 Training Loss: 0.08925191160812174 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 12 out of 93 Training Loss: 0.09952567365541254 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 13 out of 93 Training Loss: 0.1079196309350351 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 14 out of 93 Training Loss: 0.11598754615112339 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 15 out of 93 Training Loss: 0.12323596874772702 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 16 out of 93 Training Loss: 0.13202786619364415 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 17 out of 93 Training Loss: 0.14042800000458155 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 18 out of 93 Training Loss: 0.14862771040349398 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 19 out of 93 Training Loss: 0.15620578719823752 Test Loss: 0.007746007843789729\n",
      "Epoch: 11 Batch: 20 out of 93 Training Loss: 0.0017752873507557798 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 21 out of 93 Training Loss: 0.010349351240438072 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 22 out of 93 Training Loss: 0.017693554775905458 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 23 out of 93 Training Loss: 0.026211920919012872 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 24 out of 93 Training Loss: 0.034362104514729826 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 25 out of 93 Training Loss: 0.042770645374548284 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 26 out of 93 Training Loss: 0.05038268051866874 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 27 out of 93 Training Loss: 0.057309530003112404 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 28 out of 93 Training Loss: 0.0661370198038636 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 29 out of 93 Training Loss: 0.0757933755201517 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 30 out of 93 Training Loss: 0.0848462397960244 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 31 out of 93 Training Loss: 0.09279095686797484 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 32 out of 93 Training Loss: 0.10215585145924672 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 33 out of 93 Training Loss: 0.11099057856772526 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 34 out of 93 Training Loss: 0.1188188058120428 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 35 out of 93 Training Loss: 0.12558834560040816 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 36 out of 93 Training Loss: 0.13335847042326315 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 37 out of 93 Training Loss: 0.14023834166873678 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 38 out of 93 Training Loss: 0.1469719645363508 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 39 out of 93 Training Loss: 0.15740950990293606 Test Loss: 0.007920295465737581\n",
      "Epoch: 11 Batch: 40 out of 93 Training Loss: 0.0017604573096852132 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 41 out of 93 Training Loss: 0.009970513771079046 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 42 out of 93 Training Loss: 0.018573145049415096 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 43 out of 93 Training Loss: 0.027914267631969913 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 44 out of 93 Training Loss: 0.03655043024828242 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 45 out of 93 Training Loss: 0.044744996952615246 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 46 out of 93 Training Loss: 0.05313914422204302 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 47 out of 93 Training Loss: 0.06118505507966088 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 48 out of 93 Training Loss: 0.06955252181997107 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 49 out of 93 Training Loss: 0.07768016740938233 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 50 out of 93 Training Loss: 0.08502497956415223 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 51 out of 93 Training Loss: 0.09482422810306357 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 52 out of 93 Training Loss: 0.10412066675981091 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 53 out of 93 Training Loss: 0.11184639031325982 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 54 out of 93 Training Loss: 0.12105463831012653 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 55 out of 93 Training Loss: 0.1283454975511651 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 56 out of 93 Training Loss: 0.13568665286784576 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 57 out of 93 Training Loss: 0.14487650258725093 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 58 out of 93 Training Loss: 0.1538501541528623 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 59 out of 93 Training Loss: 0.16162820220252083 Test Loss: 0.007747895422984253\n",
      "Epoch: 11 Batch: 60 out of 93 Training Loss: 0.0018284838430621083 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 61 out of 93 Training Loss: 0.0096310745708682 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 62 out of 93 Training Loss: 0.017772825971994585 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 63 out of 93 Training Loss: 0.024461282954368776 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 64 out of 93 Training Loss: 0.03306058102921028 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 65 out of 93 Training Loss: 0.0402624426193573 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 66 out of 93 Training Loss: 0.04884341319725294 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 67 out of 93 Training Loss: 0.058883598611507604 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 68 out of 93 Training Loss: 0.06672266645059366 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 69 out of 93 Training Loss: 0.07488385641649742 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 70 out of 93 Training Loss: 0.08248723864257712 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 71 out of 93 Training Loss: 0.09049429658502717 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 72 out of 93 Training Loss: 0.09837601992935319 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 73 out of 93 Training Loss: 0.10693195376247544 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 74 out of 93 Training Loss: 0.11408049899787087 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 75 out of 93 Training Loss: 0.12117683103010435 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 76 out of 93 Training Loss: 0.12944099360779304 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 77 out of 93 Training Loss: 0.1371485880360045 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 78 out of 93 Training Loss: 0.14357480367347975 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 79 out of 93 Training Loss: 0.15097002831921358 Test Loss: 0.007837514858692884\n",
      "Epoch: 11 Batch: 80 out of 93 Training Loss: 0.0017023913129363413 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 81 out of 93 Training Loss: 0.009907698029199635 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 82 out of 93 Training Loss: 0.018709435829440152 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 83 out of 93 Training Loss: 0.02727215434751848 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 84 out of 93 Training Loss: 0.03562158323220352 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 85 out of 93 Training Loss: 0.04413014999918083 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 86 out of 93 Training Loss: 0.0528997320248876 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 87 out of 93 Training Loss: 0.061254780509987866 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 88 out of 93 Training Loss: 0.0692474457933698 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 89 out of 93 Training Loss: 0.07700700150198678 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 90 out of 93 Training Loss: 0.08396869036696294 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 91 out of 93 Training Loss: 0.09235221124581436 Test Loss: 0.007728267926722765\n",
      "Epoch: 11 Batch: 92 out of 93 Training Loss: 0.09934448195017317 Test Loss: 0.007728267926722765\n",
      "Epoch: 12 Batch: 0 out of 93 Training Loss: 8.974929330169513e-05 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 1 out of 93 Training Loss: 0.007690757112477416 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 2 out of 93 Training Loss: 0.015093347122792596 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 3 out of 93 Training Loss: 0.02255713899609863 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 4 out of 93 Training Loss: 0.02908450557840287 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 5 out of 93 Training Loss: 0.037236979296330804 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 6 out of 93 Training Loss: 0.04601862008673369 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 7 out of 93 Training Loss: 0.055496394699339265 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 8 out of 93 Training Loss: 0.06502640626383244 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 9 out of 93 Training Loss: 0.07313419978124319 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 10 out of 93 Training Loss: 0.08104823722494064 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 11 out of 93 Training Loss: 0.0898237853030604 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 12 out of 93 Training Loss: 0.09762073594135462 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 13 out of 93 Training Loss: 0.1056543552036327 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 14 out of 93 Training Loss: 0.11275214941230832 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 15 out of 93 Training Loss: 0.12032674632025181 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 16 out of 93 Training Loss: 0.12896167214018522 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 17 out of 93 Training Loss: 0.1382636679987353 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 18 out of 93 Training Loss: 0.14578904171464263 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 19 out of 93 Training Loss: 0.15478530206707536 Test Loss: 0.007746859368952838\n",
      "Epoch: 12 Batch: 20 out of 93 Training Loss: 0.0017599493262136943 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 21 out of 93 Training Loss: 0.01164121478765855 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 22 out of 93 Training Loss: 0.01969492733448396 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 23 out of 93 Training Loss: 0.02662577778070817 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 24 out of 93 Training Loss: 0.034470957889851425 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 25 out of 93 Training Loss: 0.04334957636862645 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 26 out of 93 Training Loss: 0.05175366178124795 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 27 out of 93 Training Loss: 0.05962547212808976 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 28 out of 93 Training Loss: 0.06828915160119185 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 29 out of 93 Training Loss: 0.07542520454123268 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 30 out of 93 Training Loss: 0.08397981775894413 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 31 out of 93 Training Loss: 0.09308563517376432 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 32 out of 93 Training Loss: 0.10129062896564255 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 33 out of 93 Training Loss: 0.10980860302194605 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 34 out of 93 Training Loss: 0.11747680436789641 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 35 out of 93 Training Loss: 0.12612704392492424 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 36 out of 93 Training Loss: 0.13644234389036547 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 37 out of 93 Training Loss: 0.14352298453420054 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 38 out of 93 Training Loss: 0.15116207180514585 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 39 out of 93 Training Loss: 0.15886231949522744 Test Loss: 0.007852189454504034\n",
      "Epoch: 12 Batch: 40 out of 93 Training Loss: 0.00179136285297649 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 41 out of 93 Training Loss: 0.012030628712103534 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 42 out of 93 Training Loss: 0.019057552066818643 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 43 out of 93 Training Loss: 0.02718230589963572 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 44 out of 93 Training Loss: 0.03562065746374504 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 45 out of 93 Training Loss: 0.042869503812120124 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 46 out of 93 Training Loss: 0.051106883415208026 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 47 out of 93 Training Loss: 0.05971158595495002 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 48 out of 93 Training Loss: 0.06791881119571702 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 49 out of 93 Training Loss: 0.07636464593671338 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 50 out of 93 Training Loss: 0.08609615286551492 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 51 out of 93 Training Loss: 0.09592482467852609 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 52 out of 93 Training Loss: 0.10411445816718118 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 53 out of 93 Training Loss: 0.11227532403282897 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 54 out of 93 Training Loss: 0.11905198424793856 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 55 out of 93 Training Loss: 0.1272866332132377 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 56 out of 93 Training Loss: 0.13650656558312552 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 57 out of 93 Training Loss: 0.14353648220859544 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 58 out of 93 Training Loss: 0.15081501912064926 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 59 out of 93 Training Loss: 0.1593024122688927 Test Loss: 0.007813474121080204\n",
      "Epoch: 12 Batch: 60 out of 93 Training Loss: 0.0017891800483428423 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 61 out of 93 Training Loss: 0.01090288204007973 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 62 out of 93 Training Loss: 0.018830990836557048 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 63 out of 93 Training Loss: 0.027927815400596755 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 64 out of 93 Training Loss: 0.03714784369670977 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 65 out of 93 Training Loss: 0.04446572246828308 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 66 out of 93 Training Loss: 0.052602647755827805 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 67 out of 93 Training Loss: 0.06020013098471989 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 68 out of 93 Training Loss: 0.06680865581014385 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 69 out of 93 Training Loss: 0.07453716122412314 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 70 out of 93 Training Loss: 0.08452535276168217 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 71 out of 93 Training Loss: 0.09320288379543652 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 72 out of 93 Training Loss: 0.10090656480618944 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 73 out of 93 Training Loss: 0.10863253935107221 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 74 out of 93 Training Loss: 0.1158399418448769 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 75 out of 93 Training Loss: 0.12356780557760229 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 76 out of 93 Training Loss: 0.1302598127616309 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 77 out of 93 Training Loss: 0.13814214705356587 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 78 out of 93 Training Loss: 0.14793160273560513 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 79 out of 93 Training Loss: 0.15439105867275227 Test Loss: 0.007897504605352879\n",
      "Epoch: 12 Batch: 80 out of 93 Training Loss: 0.0017472944788051153 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 81 out of 93 Training Loss: 0.009255729299725344 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 82 out of 93 Training Loss: 0.018166335914970688 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 83 out of 93 Training Loss: 0.025724821873695902 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 84 out of 93 Training Loss: 0.03371061027118688 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 85 out of 93 Training Loss: 0.04071817515486603 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 86 out of 93 Training Loss: 0.04834186859497552 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 87 out of 93 Training Loss: 0.056703810729773096 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 88 out of 93 Training Loss: 0.06479130514034753 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 89 out of 93 Training Loss: 0.07260933440247302 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 90 out of 93 Training Loss: 0.08216654151746755 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 91 out of 93 Training Loss: 0.09252777803757911 Test Loss: 0.007713760630312291\n",
      "Epoch: 12 Batch: 92 out of 93 Training Loss: 0.09944162299925213 Test Loss: 0.007713760630312291\n",
      "Epoch: 13 Batch: 0 out of 93 Training Loss: 7.822374082220499e-05 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 1 out of 93 Training Loss: 0.007647622006154189 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 2 out of 93 Training Loss: 0.016305070931232103 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 3 out of 93 Training Loss: 0.024634082431113848 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 4 out of 93 Training Loss: 0.03275995850763334 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 5 out of 93 Training Loss: 0.04152471348843587 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 6 out of 93 Training Loss: 0.0503195166607858 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 7 out of 93 Training Loss: 0.057770829742914566 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 8 out of 93 Training Loss: 0.06572582861608875 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 9 out of 93 Training Loss: 0.07389439279413831 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 10 out of 93 Training Loss: 0.08129716441235554 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 11 out of 93 Training Loss: 0.09203680101982367 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 12 out of 93 Training Loss: 0.09955479074469817 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 13 out of 93 Training Loss: 0.10840840228102219 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 14 out of 93 Training Loss: 0.11778761502555621 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 15 out of 93 Training Loss: 0.12694170288941872 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 16 out of 93 Training Loss: 0.13589027580074084 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 17 out of 93 Training Loss: 0.14490187876126778 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 18 out of 93 Training Loss: 0.15204645982212436 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 19 out of 93 Training Loss: 0.15906002987376464 Test Loss: 0.007627581754191356\n",
      "Epoch: 13 Batch: 20 out of 93 Training Loss: 0.0017989259974505324 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 21 out of 93 Training Loss: 0.00974114153696555 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 22 out of 93 Training Loss: 0.01653665777458209 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 23 out of 93 Training Loss: 0.024537584820523442 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 24 out of 93 Training Loss: 0.03333310045464295 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 25 out of 93 Training Loss: 0.04137501500709313 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 26 out of 93 Training Loss: 0.048389444669469775 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 27 out of 93 Training Loss: 0.056059872807696046 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 28 out of 93 Training Loss: 0.06557818044139403 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 29 out of 93 Training Loss: 0.07397949516935605 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 30 out of 93 Training Loss: 0.08227742657227773 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 31 out of 93 Training Loss: 0.09141011357469815 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 32 out of 93 Training Loss: 0.0985446465962972 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 33 out of 93 Training Loss: 0.1061438574285652 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 34 out of 93 Training Loss: 0.11495923444791097 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 35 out of 93 Training Loss: 0.12357507467432279 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 36 out of 93 Training Loss: 0.13189493406726377 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 37 out of 93 Training Loss: 0.13942190313874142 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 38 out of 93 Training Loss: 0.14803773125795977 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 39 out of 93 Training Loss: 0.15715346167026417 Test Loss: 0.007838851983912966\n",
      "Epoch: 13 Batch: 40 out of 93 Training Loss: 0.0017771004627242799 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 41 out of 93 Training Loss: 0.009049542799849819 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 42 out of 93 Training Loss: 0.016634903002788137 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 43 out of 93 Training Loss: 0.025416603397835802 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 44 out of 93 Training Loss: 0.034276104618121694 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 45 out of 93 Training Loss: 0.0432765372956887 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 46 out of 93 Training Loss: 0.05033722668166907 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 47 out of 93 Training Loss: 0.059489826292057346 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 48 out of 93 Training Loss: 0.06783170866992028 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 49 out of 93 Training Loss: 0.07658642119940073 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 50 out of 93 Training Loss: 0.08410413385580928 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 51 out of 93 Training Loss: 0.09107922352131159 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 52 out of 93 Training Loss: 0.0985968239533499 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 53 out of 93 Training Loss: 0.1077955733912781 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 54 out of 93 Training Loss: 0.1163432249727443 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 55 out of 93 Training Loss: 0.12577370888765604 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 56 out of 93 Training Loss: 0.13393135051455052 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 57 out of 93 Training Loss: 0.1433639094161585 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 58 out of 93 Training Loss: 0.15012211361180694 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 59 out of 93 Training Loss: 0.15831872363786847 Test Loss: 0.007714907215400176\n",
      "Epoch: 13 Batch: 60 out of 93 Training Loss: 0.0017822954402362948 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 61 out of 93 Training Loss: 0.010999271792272294 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 62 out of 93 Training Loss: 0.01835096129859444 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 63 out of 93 Training Loss: 0.027324950934300388 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 64 out of 93 Training Loss: 0.03505687057683822 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 65 out of 93 Training Loss: 0.042067414761433566 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 66 out of 93 Training Loss: 0.050951790676364864 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 67 out of 93 Training Loss: 0.05872760774279591 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 68 out of 93 Training Loss: 0.06595004271427747 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 69 out of 93 Training Loss: 0.07408691212246772 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 70 out of 93 Training Loss: 0.08144978820989486 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 71 out of 93 Training Loss: 0.09059962924281713 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 72 out of 93 Training Loss: 0.09927585363099929 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 73 out of 93 Training Loss: 0.10813635136644002 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 74 out of 93 Training Loss: 0.11670696004937049 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 75 out of 93 Training Loss: 0.12480679448405858 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 76 out of 93 Training Loss: 0.1328474675835168 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 77 out of 93 Training Loss: 0.1416509851411974 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 78 out of 93 Training Loss: 0.15036201864192839 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 79 out of 93 Training Loss: 0.15844885460744257 Test Loss: 0.007636496416208419\n",
      "Epoch: 13 Batch: 80 out of 93 Training Loss: 0.0017883568236755165 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 81 out of 93 Training Loss: 0.008508306013249043 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 82 out of 93 Training Loss: 0.016643882745705725 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 83 out of 93 Training Loss: 0.025080328823827387 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 84 out of 93 Training Loss: 0.03451966775377858 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 85 out of 93 Training Loss: 0.04189611879905331 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 86 out of 93 Training Loss: 0.049757971444927336 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 87 out of 93 Training Loss: 0.05811625161936152 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 88 out of 93 Training Loss: 0.0655223597007023 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 89 out of 93 Training Loss: 0.07324496984278428 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 90 out of 93 Training Loss: 0.08040682181512582 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 91 out of 93 Training Loss: 0.08852263215725886 Test Loss: 0.0076138822531158275\n",
      "Epoch: 13 Batch: 92 out of 93 Training Loss: 0.10132873851602542 Test Loss: 0.0076138822531158275\n",
      "Epoch: 14 Batch: 0 out of 93 Training Loss: 9.327650230417969e-05 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 1 out of 93 Training Loss: 0.00972437754433642 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 2 out of 93 Training Loss: 0.017806297989301785 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 3 out of 93 Training Loss: 0.02497083822425495 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 4 out of 93 Training Loss: 0.03385746700849424 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 5 out of 93 Training Loss: 0.04176085943516384 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 6 out of 93 Training Loss: 0.05009626632716547 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 7 out of 93 Training Loss: 0.05792263092633377 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 8 out of 93 Training Loss: 0.0662825418465961 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 9 out of 93 Training Loss: 0.07462273853059898 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 10 out of 93 Training Loss: 0.08081833960648666 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 11 out of 93 Training Loss: 0.09085791779842267 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 12 out of 93 Training Loss: 0.09949427226718555 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 13 out of 93 Training Loss: 0.1079514551737536 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 14 out of 93 Training Loss: 0.1147424270228673 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 15 out of 93 Training Loss: 0.12348494108926544 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 16 out of 93 Training Loss: 0.13227388717394362 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 17 out of 93 Training Loss: 0.1395768382416297 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 18 out of 93 Training Loss: 0.1479399389417101 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 19 out of 93 Training Loss: 0.15743438795369158 Test Loss: 0.007852365220473572\n",
      "Epoch: 14 Batch: 20 out of 93 Training Loss: 0.0017801311496136532 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 21 out of 93 Training Loss: 0.010934910054826533 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 22 out of 93 Training Loss: 0.020447020668113026 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 23 out of 93 Training Loss: 0.0305932725477455 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 24 out of 93 Training Loss: 0.03879333964320401 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 25 out of 93 Training Loss: 0.04842222600015381 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 26 out of 93 Training Loss: 0.05615781039702276 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 27 out of 93 Training Loss: 0.06258292241724708 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 28 out of 93 Training Loss: 0.07089949796678999 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 29 out of 93 Training Loss: 0.07893352887364605 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 30 out of 93 Training Loss: 0.08632885643052438 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 31 out of 93 Training Loss: 0.095808441534215 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 32 out of 93 Training Loss: 0.104509752027267 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 33 out of 93 Training Loss: 0.11228145637320855 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 34 out of 93 Training Loss: 0.12117493596630671 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 35 out of 93 Training Loss: 0.1293469814281402 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 36 out of 93 Training Loss: 0.13807137381988147 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 37 out of 93 Training Loss: 0.14675216913925984 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 38 out of 93 Training Loss: 0.15620871518509963 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 39 out of 93 Training Loss: 0.16485189159172156 Test Loss: 0.007570363369516351\n",
      "Epoch: 14 Batch: 40 out of 93 Training Loss: 0.0018602925459658564 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 41 out of 93 Training Loss: 0.00881367494489903 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 42 out of 93 Training Loss: 0.017767146612963622 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 43 out of 93 Training Loss: 0.025344918399563973 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 44 out of 93 Training Loss: 0.03308233377661461 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 45 out of 93 Training Loss: 0.04128318783964867 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 46 out of 93 Training Loss: 0.04827404665898437 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 47 out of 93 Training Loss: 0.056719805962852185 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 48 out of 93 Training Loss: 0.06486854600857848 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 49 out of 93 Training Loss: 0.07340982454966659 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 50 out of 93 Training Loss: 0.08205837189357633 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 51 out of 93 Training Loss: 0.08995837058078403 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 52 out of 93 Training Loss: 0.09857968139599914 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 53 out of 93 Training Loss: 0.10801382168333883 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 54 out of 93 Training Loss: 0.11523136069204563 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 55 out of 93 Training Loss: 0.12338253521498198 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 56 out of 93 Training Loss: 0.13085343028556462 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 57 out of 93 Training Loss: 0.13789046780344005 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 58 out of 93 Training Loss: 0.14525901750799175 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 59 out of 93 Training Loss: 0.1567363835978126 Test Loss: 0.007753588343885812\n",
      "Epoch: 14 Batch: 60 out of 93 Training Loss: 0.0017826214413837458 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 61 out of 93 Training Loss: 0.010244269108374502 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 62 out of 93 Training Loss: 0.01984200831313887 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 63 out of 93 Training Loss: 0.02744510043223658 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 64 out of 93 Training Loss: 0.035013007650126604 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 65 out of 93 Training Loss: 0.041909900678296474 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 66 out of 93 Training Loss: 0.04860358191465297 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 67 out of 93 Training Loss: 0.056250896489222435 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 68 out of 93 Training Loss: 0.06468620076586762 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 69 out of 93 Training Loss: 0.07185532683317819 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 70 out of 93 Training Loss: 0.08042282489870944 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 71 out of 93 Training Loss: 0.08879434941147724 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 72 out of 93 Training Loss: 0.09668685840253988 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 73 out of 93 Training Loss: 0.10302871623819032 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 74 out of 93 Training Loss: 0.1098581568319098 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 75 out of 93 Training Loss: 0.1169027203675107 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 76 out of 93 Training Loss: 0.12530375197341004 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 77 out of 93 Training Loss: 0.13229753421430746 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 78 out of 93 Training Loss: 0.1399841287154452 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 79 out of 93 Training Loss: 0.14797123772834697 Test Loss: 0.007765262641689994\n",
      "Epoch: 14 Batch: 80 out of 93 Training Loss: 0.0016936954501212455 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 81 out of 93 Training Loss: 0.009209610653960787 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 82 out of 93 Training Loss: 0.01657640574678191 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 83 out of 93 Training Loss: 0.024765062278056228 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 84 out of 93 Training Loss: 0.03229796573697933 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 85 out of 93 Training Loss: 0.040118582213217104 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 86 out of 93 Training Loss: 0.04902826309382328 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 87 out of 93 Training Loss: 0.05641827792107471 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 88 out of 93 Training Loss: 0.06482827164410956 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 89 out of 93 Training Loss: 0.07234936686410912 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 90 out of 93 Training Loss: 0.08011073725476273 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 91 out of 93 Training Loss: 0.086678276436323 Test Loss: 0.0077025169422003355\n",
      "Epoch: 14 Batch: 92 out of 93 Training Loss: 0.09492661956875928 Test Loss: 0.0077025169422003355\n",
      "Epoch: 15 Batch: 0 out of 93 Training Loss: 8.733731804675954e-05 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 1 out of 93 Training Loss: 0.008114910434170435 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 2 out of 93 Training Loss: 0.016569787386043738 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 3 out of 93 Training Loss: 0.025325857783837984 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 4 out of 93 Training Loss: 0.03352507644443102 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 5 out of 93 Training Loss: 0.04046393859310336 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 6 out of 93 Training Loss: 0.04945973514689393 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 7 out of 93 Training Loss: 0.056328336457892134 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 8 out of 93 Training Loss: 0.06320513061357923 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 9 out of 93 Training Loss: 0.0713140826962728 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 10 out of 93 Training Loss: 0.07892896163387485 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 11 out of 93 Training Loss: 0.08655271374730654 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 12 out of 93 Training Loss: 0.09433070778025575 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 13 out of 93 Training Loss: 0.10370549394859262 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 14 out of 93 Training Loss: 0.11084090241341181 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 15 out of 93 Training Loss: 0.12078332224039622 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 16 out of 93 Training Loss: 0.12928798769710848 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 17 out of 93 Training Loss: 0.13840024893282243 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 18 out of 93 Training Loss: 0.1472535065665681 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 19 out of 93 Training Loss: 0.15344039558543154 Test Loss: 0.00774456552145156\n",
      "Epoch: 15 Batch: 20 out of 93 Training Loss: 0.0017284783607927903 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 21 out of 93 Training Loss: 0.010226566817483101 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 22 out of 93 Training Loss: 0.017008579343429003 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 23 out of 93 Training Loss: 0.026478355757763777 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 24 out of 93 Training Loss: 0.03685964365290681 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 25 out of 93 Training Loss: 0.04499866441624442 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 26 out of 93 Training Loss: 0.05366072525071183 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 27 out of 93 Training Loss: 0.06148370542364398 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 28 out of 93 Training Loss: 0.06796972035231272 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 29 out of 93 Training Loss: 0.07655732567222992 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 30 out of 93 Training Loss: 0.08342019841807166 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 31 out of 93 Training Loss: 0.09140823875354806 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 32 out of 93 Training Loss: 0.1006251033468751 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 33 out of 93 Training Loss: 0.10821703961851278 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 34 out of 93 Training Loss: 0.11490772432448546 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 35 out of 93 Training Loss: 0.12353617652120272 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 36 out of 93 Training Loss: 0.1301350547073988 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 37 out of 93 Training Loss: 0.13907464408116618 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 38 out of 93 Training Loss: 0.1469909316002039 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 39 out of 93 Training Loss: 0.15633116197573224 Test Loss: 0.007724705956537615\n",
      "Epoch: 15 Batch: 40 out of 93 Training Loss: 0.0017667960937131134 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 41 out of 93 Training Loss: 0.010106212329398461 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 42 out of 93 Training Loss: 0.018439136315416167 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 43 out of 93 Training Loss: 0.02664981210632212 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 44 out of 93 Training Loss: 0.035367977675329516 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 45 out of 93 Training Loss: 0.04293521946323998 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 46 out of 93 Training Loss: 0.05240053670508749 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 47 out of 93 Training Loss: 0.06016819305745847 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 48 out of 93 Training Loss: 0.06802232410786635 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 49 out of 93 Training Loss: 0.07639195244787222 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 50 out of 93 Training Loss: 0.08394660042626745 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 51 out of 93 Training Loss: 0.09134585071383006 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 52 out of 93 Training Loss: 0.09985625684289223 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 53 out of 93 Training Loss: 0.10667177928639537 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 54 out of 93 Training Loss: 0.11447515651611573 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 55 out of 93 Training Loss: 0.12230455163894183 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 56 out of 93 Training Loss: 0.12922374704880601 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 57 out of 93 Training Loss: 0.1374062223921526 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 58 out of 93 Training Loss: 0.14570590933812744 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 59 out of 93 Training Loss: 0.15332199201015717 Test Loss: 0.007840141230686144\n",
      "Epoch: 15 Batch: 60 out of 93 Training Loss: 0.001742253083618534 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 61 out of 93 Training Loss: 0.009968742731841458 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 62 out of 93 Training Loss: 0.018518349159391774 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 63 out of 93 Training Loss: 0.028219918724032296 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 64 out of 93 Training Loss: 0.035909269474329604 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 65 out of 93 Training Loss: 0.0436718622258624 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 66 out of 93 Training Loss: 0.05182090960803832 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 67 out of 93 Training Loss: 0.060743349607916726 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 68 out of 93 Training Loss: 0.06982818246248568 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 69 out of 93 Training Loss: 0.07788785733643377 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 70 out of 93 Training Loss: 0.08413263643521274 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 71 out of 93 Training Loss: 0.09198581474262679 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 72 out of 93 Training Loss: 0.10123931756603921 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 73 out of 93 Training Loss: 0.10924477270233834 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 74 out of 93 Training Loss: 0.11827142583567107 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 75 out of 93 Training Loss: 0.12664428307104314 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 76 out of 93 Training Loss: 0.13557883398968185 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 77 out of 93 Training Loss: 0.146205094180884 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 78 out of 93 Training Loss: 0.15556486463535035 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 79 out of 93 Training Loss: 0.16390035348373855 Test Loss: 0.007658375554125418\n",
      "Epoch: 15 Batch: 80 out of 93 Training Loss: 0.0018335552385805333 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 81 out of 93 Training Loss: 0.009274217250084706 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 82 out of 93 Training Loss: 0.01732855285171253 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 83 out of 93 Training Loss: 0.02697276319030506 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 84 out of 93 Training Loss: 0.03554759728375656 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 85 out of 93 Training Loss: 0.04401781870636923 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 86 out of 93 Training Loss: 0.0524326767272113 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 87 out of 93 Training Loss: 0.06116302399549467 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 88 out of 93 Training Loss: 0.0682580922229944 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 89 out of 93 Training Loss: 0.07812811010170562 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 90 out of 93 Training Loss: 0.08593514090988619 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 91 out of 93 Training Loss: 0.09252604138690812 Test Loss: 0.007608648347245021\n",
      "Epoch: 15 Batch: 92 out of 93 Training Loss: 0.10140110314477069 Test Loss: 0.007608648347245021\n",
      "Epoch: 16 Batch: 0 out of 93 Training Loss: 9.701243771981168e-05 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 1 out of 93 Training Loss: 0.00851292844100665 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 2 out of 93 Training Loss: 0.017887933470148554 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 3 out of 93 Training Loss: 0.026843027502138128 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 4 out of 93 Training Loss: 0.03464108967392515 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 5 out of 93 Training Loss: 0.04234588780610632 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 6 out of 93 Training Loss: 0.05015802784993123 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 7 out of 93 Training Loss: 0.05724196025340628 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 8 out of 93 Training Loss: 0.06554435548542809 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 9 out of 93 Training Loss: 0.07263184507547688 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 10 out of 93 Training Loss: 0.08066199609248709 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 11 out of 93 Training Loss: 0.08721398451535772 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 12 out of 93 Training Loss: 0.09504089486955475 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 13 out of 93 Training Loss: 0.10291022231041264 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 14 out of 93 Training Loss: 0.11081969720481705 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 15 out of 93 Training Loss: 0.12074352038263153 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 16 out of 93 Training Loss: 0.12776355008764934 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 17 out of 93 Training Loss: 0.135917449660439 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 18 out of 93 Training Loss: 0.14372333401053022 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 19 out of 93 Training Loss: 0.15318488591520857 Test Loss: 0.007749105803668499\n",
      "Epoch: 16 Batch: 20 out of 93 Training Loss: 0.0017384637168401107 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 21 out of 93 Training Loss: 0.00808440074536737 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 22 out of 93 Training Loss: 0.018085619623100124 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 23 out of 93 Training Loss: 0.026798297839497887 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 24 out of 93 Training Loss: 0.035410616735255085 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 25 out of 93 Training Loss: 0.04277752125847872 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 26 out of 93 Training Loss: 0.04990874994758304 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 27 out of 93 Training Loss: 0.05949036702725824 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 28 out of 93 Training Loss: 0.06641066512305077 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 29 out of 93 Training Loss: 0.07503156895030078 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 30 out of 93 Training Loss: 0.08283319482791957 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 31 out of 93 Training Loss: 0.08927175933707293 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 32 out of 93 Training Loss: 0.09710874846417245 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 33 out of 93 Training Loss: 0.10474199913654264 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 34 out of 93 Training Loss: 0.11380175706211265 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 35 out of 93 Training Loss: 0.12256374694619115 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 36 out of 93 Training Loss: 0.13066425007048782 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 37 out of 93 Training Loss: 0.13842636179391082 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 38 out of 93 Training Loss: 0.14637320600662168 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 39 out of 93 Training Loss: 0.1547690799481028 Test Loss: 0.007714711764658039\n",
      "Epoch: 16 Batch: 40 out of 93 Training Loss: 0.0017452867050953382 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 41 out of 93 Training Loss: 0.00991062571306901 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 42 out of 93 Training Loss: 0.018409701371688413 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 43 out of 93 Training Loss: 0.02667492565711217 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 44 out of 93 Training Loss: 0.03429472430621462 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 45 out of 93 Training Loss: 0.04221382535075026 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 46 out of 93 Training Loss: 0.05049359875743466 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 47 out of 93 Training Loss: 0.058312044320751 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 48 out of 93 Training Loss: 0.06495546114643293 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 49 out of 93 Training Loss: 0.07258404123609143 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 50 out of 93 Training Loss: 0.08056882939313727 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 51 out of 93 Training Loss: 0.08908513824467736 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 52 out of 93 Training Loss: 0.09643836536457019 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 53 out of 93 Training Loss: 0.10375476705854016 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 54 out of 93 Training Loss: 0.1132991107461341 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 55 out of 93 Training Loss: 0.12225657406275349 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 56 out of 93 Training Loss: 0.12976601284940914 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 57 out of 93 Training Loss: 0.13810066350748018 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 58 out of 93 Training Loss: 0.1466251083930321 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 59 out of 93 Training Loss: 0.15516162042518333 Test Loss: 0.007714884609661319\n",
      "Epoch: 16 Batch: 60 out of 93 Training Loss: 0.001750123710422947 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 61 out of 93 Training Loss: 0.01100926103541179 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 62 out of 93 Training Loss: 0.021438345336406664 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 63 out of 93 Training Loss: 0.03070104124495311 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 64 out of 93 Training Loss: 0.03698298383974953 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 65 out of 93 Training Loss: 0.04435439037629886 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 66 out of 93 Training Loss: 0.053124486492066816 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 67 out of 93 Training Loss: 0.06075794717767759 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 68 out of 93 Training Loss: 0.06931500697979255 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 69 out of 93 Training Loss: 0.07685453260788246 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 70 out of 93 Training Loss: 0.08542232776531501 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 71 out of 93 Training Loss: 0.09508231843361183 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 72 out of 93 Training Loss: 0.10248711660975142 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 73 out of 93 Training Loss: 0.11068011310422345 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 74 out of 93 Training Loss: 0.11921830989891215 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 75 out of 93 Training Loss: 0.12665625291415972 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 76 out of 93 Training Loss: 0.13557825239846033 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 77 out of 93 Training Loss: 0.14424370604106707 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 78 out of 93 Training Loss: 0.15224070756235641 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 79 out of 93 Training Loss: 0.1628559865290813 Test Loss: 0.0077415126460519705\n",
      "Epoch: 16 Batch: 80 out of 93 Training Loss: 0.0018374081403122711 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 81 out of 93 Training Loss: 0.00832111711428626 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 82 out of 93 Training Loss: 0.014934412639598684 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 83 out of 93 Training Loss: 0.020478849735121565 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 84 out of 93 Training Loss: 0.029160412464301424 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 85 out of 93 Training Loss: 0.03745226698742374 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 86 out of 93 Training Loss: 0.046160226192335924 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 87 out of 93 Training Loss: 0.05365319453925951 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 88 out of 93 Training Loss: 0.06215360664577349 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 89 out of 93 Training Loss: 0.07051797021926506 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 90 out of 93 Training Loss: 0.07897180129588707 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 91 out of 93 Training Loss: 0.08812941514701708 Test Loss: 0.007691358639435334\n",
      "Epoch: 16 Batch: 92 out of 93 Training Loss: 0.09554257846252187 Test Loss: 0.007691358639435334\n",
      "Epoch: 17 Batch: 0 out of 93 Training Loss: 7.486916197243558e-05 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 1 out of 93 Training Loss: 0.007386712963262232 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 2 out of 93 Training Loss: 0.016193477111437947 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 3 out of 93 Training Loss: 0.02540685855833593 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 4 out of 93 Training Loss: 0.035846363427618176 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 5 out of 93 Training Loss: 0.04384552375463548 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 6 out of 93 Training Loss: 0.05198532030967775 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 7 out of 93 Training Loss: 0.05968441502241197 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 8 out of 93 Training Loss: 0.06690351166812483 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 9 out of 93 Training Loss: 0.07369735711304251 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 10 out of 93 Training Loss: 0.08158871685204808 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 11 out of 93 Training Loss: 0.08879446019468609 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 12 out of 93 Training Loss: 0.09715989478913847 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 13 out of 93 Training Loss: 0.10481777056170408 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 14 out of 93 Training Loss: 0.11369269262118045 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 15 out of 93 Training Loss: 0.1219862300441951 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 16 out of 93 Training Loss: 0.12965905354408327 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 17 out of 93 Training Loss: 0.13790500504014794 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 18 out of 93 Training Loss: 0.1462642584466726 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 19 out of 93 Training Loss: 0.15539750331131522 Test Loss: 0.007787308486347849\n",
      "Epoch: 17 Batch: 20 out of 93 Training Loss: 0.001746385780378792 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 21 out of 93 Training Loss: 0.01088072588036916 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 22 out of 93 Training Loss: 0.018632059873297427 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 23 out of 93 Training Loss: 0.026520199098839972 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 24 out of 93 Training Loss: 0.035736600824847434 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 25 out of 93 Training Loss: 0.043670941100671504 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 26 out of 93 Training Loss: 0.05245361007119677 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 27 out of 93 Training Loss: 0.062247572423174594 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 28 out of 93 Training Loss: 0.07053557965274117 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 29 out of 93 Training Loss: 0.07943328372503779 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 30 out of 93 Training Loss: 0.08770699515308401 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 31 out of 93 Training Loss: 0.09540600278715751 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 32 out of 93 Training Loss: 0.10314929434399268 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 33 out of 93 Training Loss: 0.11218399969230554 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 34 out of 93 Training Loss: 0.1210738620285239 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 35 out of 93 Training Loss: 0.13044743840883632 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 36 out of 93 Training Loss: 0.13863319014500042 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 37 out of 93 Training Loss: 0.14788550739477535 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 38 out of 93 Training Loss: 0.15625815415124555 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 39 out of 93 Training Loss: 0.16389153334613105 Test Loss: 0.0077692821875891905\n",
      "Epoch: 17 Batch: 40 out of 93 Training Loss: 0.001850545095432345 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 41 out of 93 Training Loss: 0.00903609503634626 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 42 out of 93 Training Loss: 0.01802614905514175 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 43 out of 93 Training Loss: 0.025165668377805296 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 44 out of 93 Training Loss: 0.03303401513599211 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 45 out of 93 Training Loss: 0.03980282337047512 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 46 out of 93 Training Loss: 0.04809479524024183 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 47 out of 93 Training Loss: 0.05727651451357061 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 48 out of 93 Training Loss: 0.06728618607499057 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 49 out of 93 Training Loss: 0.0745913588126784 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 50 out of 93 Training Loss: 0.08223800935425216 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 51 out of 93 Training Loss: 0.09052566361256295 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 52 out of 93 Training Loss: 0.09795684895120912 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 53 out of 93 Training Loss: 0.10617344050310903 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 54 out of 93 Training Loss: 0.11488382859729582 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 55 out of 93 Training Loss: 0.12339198744498545 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 56 out of 93 Training Loss: 0.13206644511424356 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 57 out of 93 Training Loss: 0.13999033222549015 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 58 out of 93 Training Loss: 0.14789438775383287 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 59 out of 93 Training Loss: 0.15535357330568486 Test Loss: 0.00771566302600232\n",
      "Epoch: 17 Batch: 60 out of 93 Training Loss: 0.0017608774702371505 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 61 out of 93 Training Loss: 0.009257007191640272 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 62 out of 93 Training Loss: 0.016972918330443038 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 63 out of 93 Training Loss: 0.025364955222916735 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 64 out of 93 Training Loss: 0.03245090254278196 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 65 out of 93 Training Loss: 0.0398979166399063 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 66 out of 93 Training Loss: 0.04826753939197673 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 67 out of 93 Training Loss: 0.05694487484799041 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 68 out of 93 Training Loss: 0.06488956863687648 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 69 out of 93 Training Loss: 0.07384184634940041 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 70 out of 93 Training Loss: 0.08123950547234429 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 71 out of 93 Training Loss: 0.08869339086742414 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 72 out of 93 Training Loss: 0.09621863140420331 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 73 out of 93 Training Loss: 0.10529017521695508 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 74 out of 93 Training Loss: 0.11445761917786015 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 75 out of 93 Training Loss: 0.12139202960462583 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 76 out of 93 Training Loss: 0.12997897741080536 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 77 out of 93 Training Loss: 0.13790519469113363 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 78 out of 93 Training Loss: 0.14605330423147692 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 79 out of 93 Training Loss: 0.15399814610065235 Test Loss: 0.007667169863866134\n",
      "Epoch: 17 Batch: 80 out of 93 Training Loss: 0.0017283811783391316 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 81 out of 93 Training Loss: 0.00997620649542392 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 82 out of 93 Training Loss: 0.018254623896439677 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 83 out of 93 Training Loss: 0.026346852219660884 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 84 out of 93 Training Loss: 0.034243370554407244 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 85 out of 93 Training Loss: 0.043247837721367484 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 86 out of 93 Training Loss: 0.051609085089524394 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 87 out of 93 Training Loss: 0.058637167009522324 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 88 out of 93 Training Loss: 0.06605251613910973 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 89 out of 93 Training Loss: 0.07322814749460399 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 90 out of 93 Training Loss: 0.0802232983140248 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 91 out of 93 Training Loss: 0.08746536649670184 Test Loss: 0.007799894252622669\n",
      "Epoch: 17 Batch: 92 out of 93 Training Loss: 0.09514253415073931 Test Loss: 0.007799894252622669\n",
      "Epoch: 18 Batch: 0 out of 93 Training Loss: 9.634699271891707e-05 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 1 out of 93 Training Loss: 0.008478191490936022 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 2 out of 93 Training Loss: 0.014306026320624094 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 3 out of 93 Training Loss: 0.022878999631571514 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 4 out of 93 Training Loss: 0.03166575524555419 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 5 out of 93 Training Loss: 0.038608650952297194 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 6 out of 93 Training Loss: 0.04707747352840279 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 7 out of 93 Training Loss: 0.05529656195612524 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 8 out of 93 Training Loss: 0.06220509791346167 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 9 out of 93 Training Loss: 0.07071227980377053 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 10 out of 93 Training Loss: 0.07730414792494748 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 11 out of 93 Training Loss: 0.08628405797826026 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 12 out of 93 Training Loss: 0.09502696692543004 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 13 out of 93 Training Loss: 0.10378327387915824 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 14 out of 93 Training Loss: 0.11192367858784173 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 15 out of 93 Training Loss: 0.12065368312941764 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 16 out of 93 Training Loss: 0.12822659853445267 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 17 out of 93 Training Loss: 0.13622110411124205 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 18 out of 93 Training Loss: 0.1460971197073338 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 19 out of 93 Training Loss: 0.15397317781643843 Test Loss: 0.007728409403088418\n",
      "Epoch: 18 Batch: 20 out of 93 Training Loss: 0.0017396417596217098 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 21 out of 93 Training Loss: 0.010788663398551553 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 22 out of 93 Training Loss: 0.01697812659301123 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 23 out of 93 Training Loss: 0.024600970666932672 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 24 out of 93 Training Loss: 0.033422647152173135 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 25 out of 93 Training Loss: 0.04180715523295364 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 26 out of 93 Training Loss: 0.050363860201167676 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 27 out of 93 Training Loss: 0.05797140357651076 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 28 out of 93 Training Loss: 0.06700267048187813 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 29 out of 93 Training Loss: 0.07475299305849037 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 30 out of 93 Training Loss: 0.08265736709915361 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 31 out of 93 Training Loss: 0.090244001351374 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 32 out of 93 Training Loss: 0.09803478803120813 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 33 out of 93 Training Loss: 0.10409043227888903 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 34 out of 93 Training Loss: 0.11233216022707782 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 35 out of 93 Training Loss: 0.12056910490252337 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 36 out of 93 Training Loss: 0.129107363164502 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 37 out of 93 Training Loss: 0.13826337141551337 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 38 out of 93 Training Loss: 0.14727893622227273 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 39 out of 93 Training Loss: 0.154986893400627 Test Loss: 0.007738287052647634\n",
      "Epoch: 18 Batch: 40 out of 93 Training Loss: 0.001760644934511501 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 41 out of 93 Training Loss: 0.009602916523254235 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 42 out of 93 Training Loss: 0.01663241591621073 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 43 out of 93 Training Loss: 0.023824142388439495 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 44 out of 93 Training Loss: 0.03193118712250026 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 45 out of 93 Training Loss: 0.04183403784725698 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 46 out of 93 Training Loss: 0.049696629963255246 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 47 out of 93 Training Loss: 0.057038255597836096 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 48 out of 93 Training Loss: 0.06512256581652792 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 49 out of 93 Training Loss: 0.07328506894726189 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 50 out of 93 Training Loss: 0.08127933983076246 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 51 out of 93 Training Loss: 0.09027993548978003 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 52 out of 93 Training Loss: 0.09814327732045086 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 53 out of 93 Training Loss: 0.10672232751119765 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 54 out of 93 Training Loss: 0.11494328975994023 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 55 out of 93 Training Loss: 0.12134666191730531 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 56 out of 93 Training Loss: 0.12823131364155443 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 57 out of 93 Training Loss: 0.13599322559628518 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 58 out of 93 Training Loss: 0.14485799344573053 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 59 out of 93 Training Loss: 0.15226888805944355 Test Loss: 0.007708753332157026\n",
      "Epoch: 18 Batch: 60 out of 93 Training Loss: 0.0017234340610863734 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 61 out of 93 Training Loss: 0.010701546661830144 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 62 out of 93 Training Loss: 0.018883528404212195 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 63 out of 93 Training Loss: 0.026892559819197852 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 64 out of 93 Training Loss: 0.03709362282631417 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 65 out of 93 Training Loss: 0.04470923192081471 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 66 out of 93 Training Loss: 0.052602681547677714 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 67 out of 93 Training Loss: 0.06062714963433762 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 68 out of 93 Training Loss: 0.06831255623353144 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 69 out of 93 Training Loss: 0.07548586300787946 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 70 out of 93 Training Loss: 0.0834011409732463 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 71 out of 93 Training Loss: 0.09084999163848778 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 72 out of 93 Training Loss: 0.09792317425114533 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 73 out of 93 Training Loss: 0.10703010832173249 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 74 out of 93 Training Loss: 0.11522955981296917 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 75 out of 93 Training Loss: 0.12238673980561515 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 76 out of 93 Training Loss: 0.12985727538568398 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 77 out of 93 Training Loss: 0.13774226115298888 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 78 out of 93 Training Loss: 0.14521442375359556 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 79 out of 93 Training Loss: 0.15318738411394378 Test Loss: 0.0077820714055137205\n",
      "Epoch: 18 Batch: 80 out of 93 Training Loss: 0.0017239597539136509 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 81 out of 93 Training Loss: 0.010727765681845913 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 82 out of 93 Training Loss: 0.01797744137297369 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 83 out of 93 Training Loss: 0.027259756522876988 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 84 out of 93 Training Loss: 0.03448167274768449 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 85 out of 93 Training Loss: 0.04145246435950495 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 86 out of 93 Training Loss: 0.04868962540353633 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 87 out of 93 Training Loss: 0.0573977466585765 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 88 out of 93 Training Loss: 0.06519740418593384 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 89 out of 93 Training Loss: 0.073735107379658 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 90 out of 93 Training Loss: 0.08290024352322317 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 91 out of 93 Training Loss: 0.09147001867244936 Test Loss: 0.007678742986172438\n",
      "Epoch: 18 Batch: 92 out of 93 Training Loss: 0.10551194910953737 Test Loss: 0.007678742986172438\n",
      "Epoch: 19 Batch: 0 out of 93 Training Loss: 0.00010588125235611392 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 1 out of 93 Training Loss: 0.008045919359691681 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 2 out of 93 Training Loss: 0.01541881880632812 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 3 out of 93 Training Loss: 0.022418435783155503 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 4 out of 93 Training Loss: 0.030147932124354185 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 5 out of 93 Training Loss: 0.037911297873623906 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 6 out of 93 Training Loss: 0.04593844910062128 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 7 out of 93 Training Loss: 0.05405258585608774 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 8 out of 93 Training Loss: 0.06252184335983568 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 9 out of 93 Training Loss: 0.07158738759256178 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 10 out of 93 Training Loss: 0.08083866176105314 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 11 out of 93 Training Loss: 0.08888344199306542 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 12 out of 93 Training Loss: 0.0966822138326543 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 13 out of 93 Training Loss: 0.10483029636464292 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 14 out of 93 Training Loss: 0.11184892210087949 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 15 out of 93 Training Loss: 0.12025379261843139 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 16 out of 93 Training Loss: 0.12967350671909028 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 17 out of 93 Training Loss: 0.13795979752115184 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 18 out of 93 Training Loss: 0.14555997273794585 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 19 out of 93 Training Loss: 0.1550513232397216 Test Loss: 0.007807105229320851\n",
      "Epoch: 19 Batch: 20 out of 93 Training Loss: 0.0017607598278013795 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 21 out of 93 Training Loss: 0.00959301038564125 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 22 out of 93 Training Loss: 0.017305003916033897 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 23 out of 93 Training Loss: 0.024572867871382627 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 24 out of 93 Training Loss: 0.03273162615404407 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 25 out of 93 Training Loss: 0.040417203185269035 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 26 out of 93 Training Loss: 0.04763754081831256 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 27 out of 93 Training Loss: 0.0556391728754429 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 28 out of 93 Training Loss: 0.06441130858764926 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 29 out of 93 Training Loss: 0.07143396089703719 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 30 out of 93 Training Loss: 0.07929121746600548 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 31 out of 93 Training Loss: 0.08779244820506255 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 32 out of 93 Training Loss: 0.0969723448801128 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 33 out of 93 Training Loss: 0.10520208577544371 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 34 out of 93 Training Loss: 0.11415441005409638 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 35 out of 93 Training Loss: 0.1212147194604663 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 36 out of 93 Training Loss: 0.13004878532991687 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 37 out of 93 Training Loss: 0.13988213797018806 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 38 out of 93 Training Loss: 0.14864193738357107 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 39 out of 93 Training Loss: 0.15720813066617528 Test Loss: 0.0077577012760395355\n",
      "Epoch: 19 Batch: 40 out of 93 Training Loss: 0.0017775757515575594 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 41 out of 93 Training Loss: 0.009903447171464175 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 42 out of 93 Training Loss: 0.01769314466954729 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 43 out of 93 Training Loss: 0.02541836903409621 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 44 out of 93 Training Loss: 0.033734406279190984 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 45 out of 93 Training Loss: 0.04221618541495463 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 46 out of 93 Training Loss: 0.04997804335297605 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 47 out of 93 Training Loss: 0.057302259774669614 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 48 out of 93 Training Loss: 0.06598673195557257 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 49 out of 93 Training Loss: 0.07430508912520072 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 50 out of 93 Training Loss: 0.0821545132713165 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 51 out of 93 Training Loss: 0.09069412660764119 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 52 out of 93 Training Loss: 0.09868434810058734 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 53 out of 93 Training Loss: 0.10641860955373904 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 54 out of 93 Training Loss: 0.11673925903604886 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 55 out of 93 Training Loss: 0.12481043481247088 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 56 out of 93 Training Loss: 0.13320398603545092 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 57 out of 93 Training Loss: 0.13993117625804208 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 58 out of 93 Training Loss: 0.147375064836606 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 59 out of 93 Training Loss: 0.15504558983238004 Test Loss: 0.007798164278607477\n",
      "Epoch: 19 Batch: 60 out of 93 Training Loss: 0.0017397947382079297 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 61 out of 93 Training Loss: 0.010406284942840212 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 62 out of 93 Training Loss: 0.019119327306364647 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 63 out of 93 Training Loss: 0.028401606134687535 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 64 out of 93 Training Loss: 0.03643439093193876 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 65 out of 93 Training Loss: 0.04385350912398564 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 66 out of 93 Training Loss: 0.0514997452270137 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 67 out of 93 Training Loss: 0.060793773620937935 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 68 out of 93 Training Loss: 0.06884554063490497 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 69 out of 93 Training Loss: 0.07779052050373422 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 70 out of 93 Training Loss: 0.08573694375864135 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 71 out of 93 Training Loss: 0.09552694798967706 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 72 out of 93 Training Loss: 0.10414816567263471 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 73 out of 93 Training Loss: 0.11153219721800553 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 74 out of 93 Training Loss: 0.11875109223238098 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 75 out of 93 Training Loss: 0.12633548898211586 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 76 out of 93 Training Loss: 0.1350754381861435 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 77 out of 93 Training Loss: 0.1421657641190933 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 78 out of 93 Training Loss: 0.15005673083997237 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 79 out of 93 Training Loss: 0.15928966447300183 Test Loss: 0.007828791498799215\n",
      "Epoch: 19 Batch: 80 out of 93 Training Loss: 0.0018005162585205147 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 81 out of 93 Training Loss: 0.009911375689846904 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 82 out of 93 Training Loss: 0.015918449602616458 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 83 out of 93 Training Loss: 0.022676524810088305 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 84 out of 93 Training Loss: 0.030657345532966285 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 85 out of 93 Training Loss: 0.038115274730665116 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 86 out of 93 Training Loss: 0.04661659580801192 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 87 out of 93 Training Loss: 0.054157904453170924 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 88 out of 93 Training Loss: 0.06144896672253075 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 89 out of 93 Training Loss: 0.07026042659912768 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 90 out of 93 Training Loss: 0.07822756246809426 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 91 out of 93 Training Loss: 0.08581993878964844 Test Loss: 0.007703209592198784\n",
      "Epoch: 19 Batch: 92 out of 93 Training Loss: 0.09124762714986268 Test Loss: 0.007703209592198784\n",
      "Epoch: 20 Batch: 0 out of 93 Training Loss: 9.462605881434615e-05 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 1 out of 93 Training Loss: 0.008405524265942394 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 2 out of 93 Training Loss: 0.016688737978217422 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 3 out of 93 Training Loss: 0.02474278300720197 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 4 out of 93 Training Loss: 0.032086335548189696 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 5 out of 93 Training Loss: 0.03957716231885296 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 6 out of 93 Training Loss: 0.04741432746591907 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 7 out of 93 Training Loss: 0.05618677889647823 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 8 out of 93 Training Loss: 0.06286403905319911 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 9 out of 93 Training Loss: 0.07084150328951817 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 10 out of 93 Training Loss: 0.07836623625549417 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 11 out of 93 Training Loss: 0.08583183861988526 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 12 out of 93 Training Loss: 0.09591727606671792 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 13 out of 93 Training Loss: 0.10508348416256648 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 14 out of 93 Training Loss: 0.11412901799845439 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 15 out of 93 Training Loss: 0.12231522142606717 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 16 out of 93 Training Loss: 0.1306034205080841 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 17 out of 93 Training Loss: 0.13854990895557145 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 18 out of 93 Training Loss: 0.14670611913966874 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 19 out of 93 Training Loss: 0.1564279729099844 Test Loss: 0.007647200783883984\n",
      "Epoch: 20 Batch: 20 out of 93 Training Loss: 0.0017691453894479023 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 21 out of 93 Training Loss: 0.009330668391088938 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 22 out of 93 Training Loss: 0.017439892419974304 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 23 out of 93 Training Loss: 0.02559680575261543 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 24 out of 93 Training Loss: 0.033246110563526846 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 25 out of 93 Training Loss: 0.04170138798410723 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 26 out of 93 Training Loss: 0.050367513110290266 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 27 out of 93 Training Loss: 0.05824837125594447 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 28 out of 93 Training Loss: 0.06585526643077562 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 29 out of 93 Training Loss: 0.07330145863572786 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 30 out of 93 Training Loss: 0.08116451361904571 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 31 out of 93 Training Loss: 0.08871862981225083 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 32 out of 93 Training Loss: 0.09623465258384774 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 33 out of 93 Training Loss: 0.10348821235577295 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 34 out of 93 Training Loss: 0.11173452520499656 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 35 out of 93 Training Loss: 0.1200569550482435 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 36 out of 93 Training Loss: 0.12813524799178072 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 37 out of 93 Training Loss: 0.13815376152972408 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 38 out of 93 Training Loss: 0.14625767236332127 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 39 out of 93 Training Loss: 0.15581825738529392 Test Loss: 0.0076708253049714995\n",
      "Epoch: 20 Batch: 40 out of 93 Training Loss: 0.0017658335156477084 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 41 out of 93 Training Loss: 0.00896586645391947 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 42 out of 93 Training Loss: 0.016238782834235486 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 43 out of 93 Training Loss: 0.025276870746139345 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 44 out of 93 Training Loss: 0.031784216687057075 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 45 out of 93 Training Loss: 0.039145028963658866 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 46 out of 93 Training Loss: 0.04771380480376249 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 47 out of 93 Training Loss: 0.05673360712823635 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 48 out of 93 Training Loss: 0.06317295907062656 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 49 out of 93 Training Loss: 0.07237529037517673 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 50 out of 93 Training Loss: 0.07938010785726077 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 51 out of 93 Training Loss: 0.08719752617177731 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 52 out of 93 Training Loss: 0.09509396813807255 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 53 out of 93 Training Loss: 0.10361715946343905 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 54 out of 93 Training Loss: 0.1130442971363402 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 55 out of 93 Training Loss: 0.12308395169672734 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 56 out of 93 Training Loss: 0.13107005394992596 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 57 out of 93 Training Loss: 0.13723978288469083 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 58 out of 93 Training Loss: 0.1439121116883791 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 59 out of 93 Training Loss: 0.15051752217111355 Test Loss: 0.007688769139349461\n",
      "Epoch: 20 Batch: 60 out of 93 Training Loss: 0.001712392362695587 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 61 out of 93 Training Loss: 0.009347723609697474 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 62 out of 93 Training Loss: 0.016587469539534702 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 63 out of 93 Training Loss: 0.02471485435862507 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 64 out of 93 Training Loss: 0.0333719689388987 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 65 out of 93 Training Loss: 0.04311231318761076 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 66 out of 93 Training Loss: 0.05095346998323168 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 67 out of 93 Training Loss: 0.05956979356605972 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 68 out of 93 Training Loss: 0.06683401511881913 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 69 out of 93 Training Loss: 0.07418888229776587 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 70 out of 93 Training Loss: 0.08057449366797174 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 71 out of 93 Training Loss: 0.08920955173571075 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 72 out of 93 Training Loss: 0.09851736836035217 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 73 out of 93 Training Loss: 0.10586248552326526 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 74 out of 93 Training Loss: 0.11409169929001893 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 75 out of 93 Training Loss: 0.12137355264176096 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 76 out of 93 Training Loss: 0.12884910525653925 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 77 out of 93 Training Loss: 0.13763511208717194 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 78 out of 93 Training Loss: 0.1454975905818161 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 79 out of 93 Training Loss: 0.1535500272167143 Test Loss: 0.007635176181793213\n",
      "Epoch: 20 Batch: 80 out of 93 Training Loss: 0.0017252245256869987 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 81 out of 93 Training Loss: 0.009905690074917574 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 82 out of 93 Training Loss: 0.018824664623495836 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 83 out of 93 Training Loss: 0.026298508261349697 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 84 out of 93 Training Loss: 0.0323472476834624 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 85 out of 93 Training Loss: 0.04036807988679101 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 86 out of 93 Training Loss: 0.04853475999927213 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 87 out of 93 Training Loss: 0.05714759298509051 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 88 out of 93 Training Loss: 0.0641126564974158 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 89 out of 93 Training Loss: 0.07274844937866617 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 90 out of 93 Training Loss: 0.0796587134050398 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 91 out of 93 Training Loss: 0.08781773618331243 Test Loss: 0.007654172918674621\n",
      "Epoch: 20 Batch: 92 out of 93 Training Loss: 0.09477182778438617 Test Loss: 0.007654172918674621\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import SmoothL1Loss\n",
    "\n",
    "trainlosses, testlosses = model.fit(trainloader = trainloader,\n",
    "                                    validationloader = valloader,\n",
    "                                    loss = SmoothL1Loss,\n",
    "                                    optim = Adam,\n",
    "                                    lr=0.001,\n",
    "                                    epochs = 20, \n",
    "                                    val_per_batch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(trainlosses, label = 'Training Losses')\n",
    "plt.plot(testlosses, label = 'Testing Losses')\n",
    "plt.ylabel('Losses')\n",
    "plt.xlabel('Batch * Epoch')\n",
    "plt.legend()\n",
    "plt.title('Losses Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, expected = model.test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_from = 40\n",
    "val_to = 60\n",
    "\n",
    "plt.plot(result[val_from:val_to], label = 'Result')\n",
    "plt.plot(expected[val_from:val_to], label = 'Actual')\n",
    "plt.ylabel('Wind Speed (Normalized)')\n",
    "#plt.xticks(range(val_from, val_to))\n",
    "plt.xlabel('Time Range')\n",
    "plt.legend()\n",
    "plt.title('Model Output Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm-v2  - dir Created\n",
      "Model saved in lstm-v2\n",
      "Optimizer saved in lstm-v2\n"
     ]
    }
   ],
   "source": [
    "model.save('lstm-v2', save_optim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
